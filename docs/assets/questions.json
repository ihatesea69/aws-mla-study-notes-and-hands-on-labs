[
  {
    "id": 1,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is tuning an image classification model that shows poor performance on one of two available classes during prediction. Analysis reveals that the images whose class the model performed poorly on represent an extremely small fraction of the whole training dataset. The ML engineer must improve the model's performance. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Optimize for accuracy. Use image augmentation on the less common images to generate new samples."
      },
      {
        "id": "B",
        "text": "Optimize for F1 score. Use image augmentation on the less common images to generate new samples."
      },
      {
        "id": "C",
        "text": "Optimize for accuracy. Use Synthetic Minority Oversampling Technique (SMOTE) on the less common images to generate new samples."
      },
      {
        "id": "D",
        "text": "Optimize for F1 score. Use Synthetic Minority Oversampling Technique (SMOTE) on the less common images to generate new samples."
      }
    ],
    "answer": "D",
    "explanation": "D. Optimize for F1 score. Use Synthetic Minority Oversampling Technique (SMOTE) on the less common images to generate new samples. 1"
  },
  {
    "id": 2,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to deploy ML models to get inferences from large datasets in an asynchronous manner. The ML engineer also needs to implement scheduled monitoring of the data quality of the models. The ML engineer must receive alerts when changes in data quality occur. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the models by using scheduled AWS Glue jobs. Use Amazon CloudWatch alarms to monitor the data quality and to send alerts."
      },
      {
        "id": "B",
        "text": "Deploy the models by using scheduled AWS Batch jobs. Use AWS CloudTrail to monitor the data quality and to send alerts."
      },
      {
        "id": "C",
        "text": "Deploy the models by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to monitor the data quality and to send alerts."
      },
      {
        "id": "D",
        "text": "Deploy the models by using Amazon SageMaker AI batch transform. Use SageMaker Model Monitor to monitor the data quality and to send alerts."
      }
    ],
    "answer": "D",
    "explanation": "D. Deploy the models by using Amazon SageMaker AI batch transform. Use SageMaker Model Monitor to monitor the data quality and to send alerts. 2"
  },
  {
    "id": 3,
    "type": "ordering",
    "required_answers": 3,
    "question": "An ML engineer needs to use Amazon SageMaker to develop an ML solution for a company. The solution will use streaming video from cameras to count the number of people who walk past the company's store every day. Select and order the steps to implement the first version of the algorithm. (Select and order THREE)",
    "options": [
      {
        "id": "1",
        "text": "Determine if the challenge is a classification, detection, or segmentation problem."
      },
      {
        "id": "2",
        "text": "Decide the data input format and apply data augmentation if necessary."
      },
      {
        "id": "3",
        "text": "Choose a built-in algorithm or pre-trained model."
      }
    ],
    "answer": "1,2,3",
    "explanation": "1.\tStep 1: Determine if the challenge is a classification, detection, or segmentation problem. 2.\tStep 2: Decide the data input format and apply data augmentation if necessary. 3.\tStep 3: Choose a built-in algorithm or pre-trained model. 3"
  },
  {
    "id": 4,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to reduce the cost of its containerized ML applications. The applications use ML models that run on Amazon EC2 instances, AWS Lambda functions, and an Amazon Elastic Container Service (Amazon ECS) cluster. The EC2 workloads and ECS workloads use Amazon Elastic Block Store (Amazon EBS) volumes to save predictions and artifacts. An ML engineer must identify resources that are being used inefficiently. The ML engineer also must generate recommendations to reduce the cost of these resources. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create code to evaluate each instance's memory and compute usage."
      },
      {
        "id": "B",
        "text": "Add cost allocation tags to the resources. Activate the tags in AWS Billing and Cost Management."
      },
      {
        "id": "C",
        "text": "Check AWS CloudTrail event history for the creation of the resources."
      },
      {
        "id": "D",
        "text": "Run AWS Compute Optimizer."
      }
    ],
    "answer": "D",
    "explanation": "D. Run AWS Compute Optimizer. 4"
  },
  {
    "id": 5,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is using Amazon SageMaker AI to train an ML model. The ML engineer needs to use SageMaker AI automatic model tuning (AMT) features to tune the model hyperparameters over a large parameter space. The model has 20 categorical hyperparameters and 7 continuous hyperparameters that can be tuned. The ML engineer needs to run the tuning job a maximum of 1,000 times. The ML engineer must ensure that each parameter trial is built based on the performance of the previous trial. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Define the search space as categorical parameters of 1,000 possible combinations. Use grid search."
      },
      {
        "id": "B",
        "text": "Define the search space as continuous parameters. Use random search. Set the maximum number of tuning jobs to 1,000."
      },
      {
        "id": "C",
        "text": "Define the search space as categorical parameters and continuous parameters. Use Bayesian optimization. Set the maximum number of training jobs to 1,000."
      },
      {
        "id": "D",
        "text": "Define the search space as categorical parameters and continuous parameters. Use grid search. Set the maximum number of tuning jobs to 1,000."
      }
    ],
    "answer": "C",
    "explanation": "C. Define the search space as categorical parameters and continuous parameters. Use Bayesian optimization. Set the maximum number of training jobs to 1,000. 5"
  },
  {
    "id": 6,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a large, unstructured dataset. The dataset includes many duplicate records across several key attributes. Which solution on AWS will detect duplicates in the dataset with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Mechanical Turk jobs to detect duplicates."
      },
      {
        "id": "B",
        "text": "Use Amazon QuickSight ML Insights to build a custom deduplication model."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Data Wrangler to pre-process and detect duplicates."
      },
      {
        "id": "D",
        "text": "Use the AWS Glue FindMatches transform to detect duplicates."
      }
    ],
    "answer": "D",
    "explanation": "D. Use the AWS Glue FindMatches transform to detect duplicates. 6"
  },
  {
    "id": 7,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A hospital is using an ML model to validate x-ray results. The hospital runs a nightly batch inference job. The hospital needs to produce a daily report about model data quality and model performance. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Schedule a monitoring job in Amazon SageMaker Model Monitor. Generate the monitoring results for the model and data."
      },
      {
        "id": "B",
        "text": "Create an Amazon CloudWatch dashboard that includes the metrics for processing steps in the nightly batch inference job. Compare the baseline resource metrics. Share the dashboard link."
      },
      {
        "id": "C",
        "text": "Use AWS Glue DataBrew to create a custom recipe job that uses the Numerical Statistics data quality check for the model file. Generate the results."
      },
      {
        "id": "D",
        "text": "Create a SageMaker AI pipeline that includes a Quality Check step to run monitoring jobs. Generate the monitoring results for the model and the data."
      }
    ],
    "answer": "A",
    "explanation": "A. Schedule a monitoring job in Amazon SageMaker Model Monitor. Generate the monitoring results for the model and data. 7"
  },
  {
    "id": 8,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A healthcare analytics company is developing an ML model to predict whether a patient is at risk of developing diabetes based on historical health data. The dataset the company is using is highly imbalanced and includes patient demographics, medical history, and lifestyle changes. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use the Amazon SageMaker AI XGBoost algorithm. Set scale_pos_weight to adjust for the imbalance in the positive class."
      },
      {
        "id": "B",
        "text": "Use a k-means clustering algorithm. Set k to specify the number of imbalanced clusters."
      },
      {
        "id": "C",
        "text": "Use the Amazon SageMaker AI DeepAR algorithm. Set epochs for the number of training iterations in the imbalanced class."
      },
      {
        "id": "D",
        "text": "Use the Amazon SageMaker AI Random Cut Forest (RCF) algorithm. Set a contamination hyperparameter for the anomaly imbalance in the positive class."
      }
    ],
    "answer": "A",
    "explanation": "A. Use the Amazon SageMaker AI XGBoost algorithm. Set scale_pos_weight to adjust for the imbalance in the positive class. 8"
  },
  {
    "id": 9,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is setting up a continuous integration and continuous delivery (CI/CD) pipeline for an ML workflow in Amazon SageMaker AI. The pipeline needs to automate model re-training, testing, and deployment whenever new data is uploaded to an Amazon S3 bucket. New data files are approximately 10 GB in size. The ML engineer wants to track model versions for auditing. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS CodePipeline, Amazon S3, and AWS CodeBuild to retrain and deploy the model automatically and to track model versions."
      },
      {
        "id": "B",
        "text": "Use SageMaker Pipelines with the SageMaker Model Registry to orchestrate model training and version tracking."
      },
      {
        "id": "C",
        "text": "Create an AWS Lambda function to re-train and deploy the model. Use Amazon EventBridge to invoke the Lambda function. Reference the Lambda logs to track model versions."
      },
      {
        "id": "D",
        "text": "Use SageMaker AI notebook instances to manually re-train and deploy the model when needed. Reference AWS CloudTrail logs to track model versions."
      }
    ],
    "answer": "B",
    "explanation": "B. Use SageMaker Pipelines with the SageMaker Model Registry to orchestrate model training and version tracking. 9"
  },
  {
    "id": 10,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use AWS CloudFormation to create an ML model that an Amazon SageMaker AI endpoint will host. Which resource should the ML engineer declare in the CloudFormation template to meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "AWS::SageMaker::Model"
      },
      {
        "id": "B",
        "text": "AWS::SageMaker::Endpoint"
      },
      {
        "id": "C",
        "text": "AWS::SageMaker::NotebookInstance"
      },
      {
        "id": "D",
        "text": "AWS::SageMaker::Pipeline"
      }
    ],
    "answer": "A",
    "explanation": "A. AWS::SageMaker::Model 10"
  },
  {
    "id": 11,
    "type": "ordering",
    "required_answers": 3,
    "question": "An ML engineer needs to automate the rebuild and redeployment of an ML model. Updates will occur when changes are made to the model's code base. The ML engineer must use AWS services to configure a CI/CD pipeline. Select and order the steps to configure the CI/CD pipeline. (Select and order THREE)",
    "options": [
      {
        "id": "1",
        "text": "Create a Git source code repository."
      },
      {
        "id": "2",
        "text": "Create a pipeline in AWS CodePipeline. Build and test containers in AWS CodeBuild."
      },
      {
        "id": "3",
        "text": "Invoke Amazon SageMaker Pipelines to run all steps required for model training and deployment."
      }
    ],
    "answer": "1,2,3",
    "explanation": "1.\tStep 1: Create a Git source code repository. 2.\tStep 2: Create a pipeline in AWS CodePipeline. Build and test containers in AWS CodeBuild. 3.\tStep 3: Invoke Amazon SageMaker Pipelines to run all steps required for model training and deployment. 11"
  },
  {
    "id": 12,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has significantly increased the amount of data that is stored as .csv files in an Amazon S3 bucket. Data transformation scripts and queries are now taking much longer than they used to take. An ML engineer must implement a solution to optimize the data for query performance. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Configure an AWS Lambda function to split the .csv files into smaller objects in the S3 bucket."
      },
      {
        "id": "B",
        "text": "Configure an AWS Glue job to drop columns that have string type values and to save the results to the S3 bucket."
      },
      {
        "id": "C",
        "text": "Configure an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Apache Parquet format."
      },
      {
        "id": "D",
        "text": "Configure an Amazon EMR cluster to process the data that is in the S3 bucket."
      }
    ],
    "answer": "C",
    "explanation": "C. Configure an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Apache Parquet format. 12"
  },
  {
    "id": 13,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is using a training job to fine-tune a deep learning model in Amazon SageMaker Studio. The ML engineer previously trained the same pre-trained model with a similar dataset. The ML engineer expects vanishing gradient, underutilized GPU, and overfitting problems. The ML engineer needs to implement a solution to detect these issues and to react in predefined ways when the issues occur. The solution also must provide comprehensive real-time metrics during the training. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use TensorBoard to monitor the training job. Publish the findings to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function to consume the findings and to initiate the predefined actions."
      },
      {
        "id": "B",
        "text": "Use Amazon CloudWatch default metrics to gain insights about the training job. Use the metrics to invoke an AWS Lambda function to initiate the predefined actions."
      },
      {
        "id": "C",
        "text": "Expand the metrics in Amazon CloudWatch to include the gradients in each training step. Use the metrics to invoke an AWS Lambda function to initiate the predefined actions."
      },
      {
        "id": "D",
        "text": "Use SageMaker Debugger built-in rules to monitor the training job. Configure the rules to initiate the predefined actions."
      }
    ],
    "answer": "D",
    "explanation": "D. Use SageMaker Debugger built-in rules to monitor the training job. Configure the rules to initiate the predefined actions. 13"
  },
  {
    "id": 14,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using an Amazon Redshift database as its single data source. Some of the data is sensitive. A data scientist needs to use some of the sensitive data from the database. An ML engineer must give the data scientist access to the data without transforming the source data and without storing anonymized data in the database. Which solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": "A",
        "text": "Configure dynamic data masking policies to control how sensitive data is shared with the data scientist at query time."
      },
      {
        "id": "B",
        "text": "Create a materialized view with masking logic on top of the database. Grant the necessary read permissions to the data scientist."
      },
      {
        "id": "C",
        "text": "Unload the Amazon Redshift data to Amazon S3. Use Amazon Athena to create schema-on-read with masking logic. Share the view with the data scientist."
      },
      {
        "id": "D",
        "text": "Unload the Amazon Redshift data to Amazon S3. Create an AWS Glue job to anonymize the data. Share the dataset with the data scientist."
      }
    ],
    "answer": "A",
    "explanation": "A. Configure dynamic data masking policies to control how sensitive data is shared with the data scientist at query time. 14"
  },
  {
    "id": 15,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses Amazon Athena to query a dataset in Amazon S3. The dataset has a target variable that the company wants to predict. The company needs to use the dataset in a solution to determine if a model can predict the target variable. Which solution will provide this information with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create a new model by using Amazon SageMaker Autopilot. Report the model's achieved performance."
      },
      {
        "id": "B",
        "text": "Implement custom scripts to perform data pre-processing, multiple linear regression, and performance evaluation. Run the scripts on Amazon EC2 instances."
      },
      {
        "id": "C",
        "text": "Configure Amazon Macie to analyze the dataset and to create a model. Report the model's achieved performance."
      },
      {
        "id": "D",
        "text": "Select a model from Amazon Bedrock. Tune the model with the data. Report the model's achieved performance."
      }
    ],
    "answer": "A",
    "explanation": "A. Create a new model by using Amazon SageMaker Autopilot. Report the model's achieved performance. 15"
  },
  {
    "id": 16,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company deployed an Amazon SageMaker AI ML model to an endpoint by calling the CreateModel API operation. The network that was established with the API call includes two private subnets and one security group. The model must download data from an Amazon S3 bucket and must upload data to the S3 bucket. The traffic to the S3 bucket must not travel across the internet. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a NAT gateway. Configure the security group to allow outbound connections. Configure route tables to redirect any traffic to the S3 bucket through the NAT gateway."
      },
      {
        "id": "B",
        "text": "Create a gateway VPC endpoint. Configure an endpoint policy that restricts access to the S3 bucket. Configure route tables to redirect any traffic to the S3 bucket through the endpoint."
      },
      {
        "id": "C",
        "text": "Create an interface VPC endpoint. Verify that the security group allows only outbound connections. Configure route tables to redirect any traffic to the S3 bucket through the endpoint."
      },
      {
        "id": "D",
        "text": "Create a Gateway Load Balancer VPC endpoint. Configure an IAM policy that restricts access to the S3 bucket. Configure route tables to redirect any traffic to the S3 bucket through the endpoint."
      }
    ],
    "answer": "B",
    "explanation": "B. Create a gateway VPC endpoint. Configure an endpoint policy that restricts access to the S3 bucket. Configure route tables to redirect any traffic to the S3 bucket through the endpoint. 16"
  },
  {
    "id": 17,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer has an Amazon Comprehend custom model in Account A in the us-east-1 Region. The ML engineer needs to copy the model to Account B in the same Region. Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon S3 to make a copy of the model. Transfer the copy to Account B."
      },
      {
        "id": "B",
        "text": "Create a resource-based IAM policy. Use the Amazon Comprehend ImportModel API operation to copy the model to Account B."
      },
      {
        "id": "C",
        "text": "Use AWS DataSync to replicate the model from Account A to Account B."
      },
      {
        "id": "D",
        "text": "Create an AWS Site-to-Site VPN connection between Account A and Account B to transfer the model."
      }
    ],
    "answer": "B",
    "explanation": "B. Create a resource-based IAM policy. Use the Amazon Comprehend ImportModel API operation to copy the model to Account B. 17"
  },
  {
    "id": 18,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is building an enterprise AI platform. The company must catalog models for production, manage model versions, and associate metadata such as training metrics with models. The company needs to eliminate the burden of managing different versions of models. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use the Amazon SageMaker Model Registry to catalog the models. Create unique tags for each model version. Create key-value pairs to maintain associated metadata."
      },
      {
        "id": "B",
        "text": "Use the Amazon SageMaker Model Registry to catalog the models. Create model groups for each model to manage the model versions and to maintain associated metadata."
      },
      {
        "id": "C",
        "text": "Create a separate Amazon Elastic Container Registry (Amazon ECR) repository for each model. Use the repositories to catalog the models and to manage model versions and associated metadata."
      },
      {
        "id": "D",
        "text": "Create a separate Amazon Elastic Container Registry (Amazon ECR) repository for each model. Create unique tags for each model version. Create key-value pairs to maintain associated metadata."
      }
    ],
    "answer": "B",
    "explanation": "B. Use the Amazon SageMaker Model Registry to catalog the models. Create model groups for each model to manage the model versions and to maintain associated metadata. 18"
  },
  {
    "id": 19,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to run intensive model training jobs each month that can take 48 to 72 hours to run. The training jobs can be interrupted and resumed without major issues. The ML engineer has a fixed budget and needs to optimize computing resources. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": "A",
        "text": "Purchase Reserved Instances with a partial upfront payment."
      },
      {
        "id": "B",
        "text": "Purchase On-Demand Instances with no commitment."
      },
      {
        "id": "C",
        "text": "Purchase Amazon SageMaker AI Savings Plans."
      },
      {
        "id": "D",
        "text": "Purchase Spot Instances that use automated checkpoints."
      }
    ],
    "answer": "D",
    "explanation": "D. Purchase Spot Instances that use automated checkpoints. 19"
  },
  {
    "id": 20,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is running ML models on premises by using custom Python scripts and proprietary datasets. The company is using PyTorch. The model building requires unique domain knowledge. The company needs to move the models to AWS. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker AI built-in algorithms to train the proprietary datasets."
      },
      {
        "id": "B",
        "text": "Use SageMaker AI script mode and premade images for ML frameworks."
      },
      {
        "id": "C",
        "text": "Build a container on AWS that includes custom packages and a choice of ML frameworks."
      },
      {
        "id": "D",
        "text": "Purchase similar production models through AWS Marketplace."
      }
    ],
    "answer": "B",
    "explanation": "B. Use SageMaker AI script mode and premade images for ML frameworks. 20"
  },
  {
    "id": 21,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a conversational AI assistant that sends requests through Amazon Bedrock to an Anthropic Claude large language model (LLM). Users report that when they ask similar questions multiple times, they sometimes receive different answers. An ML engineer needs to improve the responses to be more consistent and less random. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Increase the temperature parameter and the top k parameter."
      },
      {
        "id": "B",
        "text": "Increase the temperature parameter. Decrease the top k parameter."
      },
      {
        "id": "C",
        "text": "Decrease the temperature parameter. Increase the top k parameter."
      },
      {
        "id": "D",
        "text": "Decrease the temperature parameter and the top k parameter."
      }
    ],
    "answer": "D",
    "explanation": "D. Decrease the temperature parameter and the top k parameter. 21"
  },
  {
    "id": 22,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is evaluating several ML models and must choose one model to use in production. The cost of false negative predictions by the models is much higher than the cost of false positive predictions. Which metric finding should the ML engineer prioritize the MOST when choosing the model?",
    "options": [
      {
        "id": "A",
        "text": "Low precision"
      },
      {
        "id": "B",
        "text": "High precision"
      },
      {
        "id": "C",
        "text": "Low recall"
      },
      {
        "id": "D",
        "text": "High recall"
      }
    ],
    "answer": "D",
    "explanation": "D. High recall 22"
  },
  {
    "id": 23,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses an Amazon SageMaker AI ML model to make real-time inferences. The company has configured auto scaling for the Amazon EC2 instances that SageMaker AI uses for the inferences. During times of peak usage, new instances launch before existing instances are fully ready. As a result, the model experiences inefficiencies and delays. Which solution will optimize the scaling process without affecting response times?",
    "options": [
      {
        "id": "A",
        "text": "Change to a multi-model endpoint configuration in SageMaker AI."
      },
      {
        "id": "B",
        "text": "Integrate Amazon API Gateway and AWS Lambda to manage invocations of the SageMaker AI inference endpoint."
      },
      {
        "id": "C",
        "text": "Decrease the cooldown period for scale-in activities. Increase the maximum number of instances."
      },
      {
        "id": "D",
        "text": "Increase the cooldown period after scale-out activities."
      }
    ],
    "answer": "C",
    "explanation": "C. Decrease the cooldown period for scale-in activities. Increase the maximum number of instances. 23"
  },
  {
    "id": 24,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to use Amazon SageMaker AI to make classification ratings that are based on images. The company has 6 TB of training data that is stored on an Amazon FSx for NetApp ONTAP system virtual machine (SVM). The SVM is in the same VPC as SageMaker AI. An ML engineer must make the training data accessible for ML models that are in the SageMaker AI environment. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Mount the FSx for ONTAP file system as a volume to the SageMaker AI instance."
      },
      {
        "id": "B",
        "text": "Create an Amazon S3 bucket. Use Mountpoint for Amazon S3 to link the S3 bucket to the FSx for ONTAP file system."
      },
      {
        "id": "C",
        "text": "Create a catalog connection from SageMaker Data Wrangler to the FSx for ONTAP file system."
      },
      {
        "id": "D",
        "text": "Create a direct connection from SageMaker Data Wrangler to the FSx for ONTAP file system."
      }
    ],
    "answer": "A",
    "explanation": "A. Mount the FSx for ONTAP file system as a volume to the SageMaker AI instance. 24"
  },
  {
    "id": 25,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using Amazon SageMaker AI to deploy a new recommendation model for its e-commerce website. The model must use data from all client website interactions as input. Traffic is variable throughout the day. The company needs to create an inference endpoint for the model. Which type of inference endpoint will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": "A",
        "text": "Batch transform inference endpoint"
      },
      {
        "id": "B",
        "text": "Asynchronous inference endpoint"
      },
      {
        "id": "C",
        "text": "Real-time inference endpoint"
      },
      {
        "id": "D",
        "text": "Serverless inference endpoint"
      }
    ],
    "answer": "D",
    "explanation": "D. Serverless inference endpoint 25"
  },
  {
    "id": 26,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A logistics company has installed in-vehicle cameras for basic monitoring of its drivers. The company wants to improve driver safety by identifying distractions that could lead to accidents. Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Rekognition eye gaze direction detection to monitor driver behavior and identify distractions."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker AI to customize an AI model to monitor driver behavior and identify distractions."
      },
      {
        "id": "C",
        "text": "Integrate a third-party driver monitoring system with Amazon Rekognition to monitor driver behavior and identify distractions."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend to analyze text-based driver feedback and identify distractions."
      }
    ],
    "answer": "A",
    "explanation": "A. Use Amazon Rekognition eye gaze direction detection to monitor driver behavior and identify distractions. 26"
  },
  {
    "id": 27,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A music streaming company constantly streams song ratings from an application to an Amazon S3 bucket. The company wants to use the ratings as an input for training and inference of an Amazon SageMaker AI model. The company has an AWS Glue Data Catalog that is configured with the S3 bucket as the source. An ML engineer needs to implement a solution to create a repository for this data. The solution must ensure that the data stays synchronized during batch training and real-time inference. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Ingest data into SageMaker Feature Store from the S3 bucket. Apply tags and indexes."
      },
      {
        "id": "B",
        "text": "Use Amazon Athena. Create tables by using CREATE TABLE AS SELECT (CTAS) queries to group data."
      },
      {
        "id": "C",
        "text": "Use AWS Lake Formation. Apply tag-based control on the data."
      },
      {
        "id": "D",
        "text": "Use the Generate Data Insights function in SageMaker Data Wrangler."
      }
    ],
    "answer": "A",
    "explanation": "A. Ingest data into SageMaker Feature Store from the S3 bucket. Apply tags and indexes. 27"
  },
  {
    "id": 28,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has implemented a data ingestion pipeline for sales transactions from its e-commerce website. The company uses Amazon Data Firehose to ingest data into Amazon OpenSearch Service. The buffer interval of the Firehose stream is set for 60 seconds. An OpenSearch linear model generates real-time sales forecasts based on the data and presents the data in an OpenSearch dashboard. The company needs to optimize the data ingestion pipeline to support sub-second latency for the real-time dashboard. Which change to the architecture will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use zero buffering in the Firehose stream. Tune the batch size that is used in the PutRecordBatch operation."
      },
      {
        "id": "B",
        "text": "Replace the Firehose stream with an AWS DataSync task. Configure the task with enhanced fan-out consumers."
      },
      {
        "id": "C",
        "text": "Increase the buffer interval of the Firehose stream from 60 seconds to 120 seconds."
      },
      {
        "id": "D",
        "text": "Replace the Firehose stream with an Amazon Simple Queue Service (Amazon SQS) queue."
      }
    ],
    "answer": "A",
    "explanation": "A. Use zero buffering in the Firehose stream. Tune the batch size that is used in the PutRecordBatch operation. 28"
  },
  {
    "id": 29,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is creating an ML model to identify defects in a product. The company has gathered a dataset and has stored the dataset in TIFF format in Amazon S3. The dataset contains 200 images in which the most common defects are visible. The dataset also contains 1,800 images in which there is no defect visible. An ML engineer trains the model and notices poor performance in some classes. The ML engineer identifies a class imbalance problem in the dataset. What should the ML engineer do to solve this problem?",
    "options": [
      {
        "id": "A",
        "text": "Use a few hundred images and Amazon Rekognition Custom Labels to train a new model."
      },
      {
        "id": "B",
        "text": "Undersample the 200 images in which the most common defects are visible."
      },
      {
        "id": "C",
        "text": "Oversample the 200 images in which the most common defects are visible."
      },
      {
        "id": "D",
        "text": "Use all 2,000 images and Amazon Rekognition Custom Labels to train a new model."
      }
    ],
    "answer": "C",
    "explanation": "C. Oversample the 200 images in which the most common defects are visible. 29"
  },
  {
    "id": 30,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an ML model in Amazon SageMaker AI. An ML engineer needs to implement a monitoring solution to automatically detect changes in the input data distribution of model features. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Configure SageMaker Model Monitor. Establish a data quality baseline. Ensure that the emit metrics option is enabled in the baseline constraints file. Configure an Amazon CloudWatch alarm to notify the company about changes in specific metrics that are related to data quality."
      },
      {
        "id": "B",
        "text": "Configure SageMaker Model Monitor. Establish a model quality baseline. Ensure that the comparison_method option is set to Robust in the baseline constraints file. Configure an Amazon CloudWatch alarm to notify the company about changes in model quality metrics."
      },
      {
        "id": "C",
        "text": "Use SageMaker Debugger with custom rules to track shifts in feature distributions. Configure Amazon CloudWatch alarms to notify the company when the rules detect significant changes."
      },
      {
        "id": "D",
        "text": "Use Amazon CloudWatch to directly observe the SageMaker AI endpoint's performance metrics. Manually analyze the CloudWatch logs for indicators of data drift or shifts in feature distribution."
      }
    ],
    "answer": "A",
    "explanation": "A. Configure SageMaker Model Monitor. Establish a data quality baseline. Ensure that the emit metrics option is enabled in the baseline constraints file. Configure an Amazon CloudWatch alarm to notify the company about changes in specific metrics that are related to data quality. 30"
  },
  {
    "id": 31,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company runs an Amazon SageMaker AI domain in a public subnet of a newly created VPC. The network is configured properly, and ML engineers can access the SageMaker AI domain. Recently, the company discovered suspicious traffic to the domain from a specific IP address. The company needs to block traffic from the specific IP address. Which update to the network configuration will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Create a security group inbound rule to deny traffic from the specific IP address. Assign the security group to the domain."
      },
      {
        "id": "B",
        "text": "Create a network ACL inbound rule to deny traffic from the specific IP address. Assign the rule to the default network ACL for the subnet where the domain is located."
      },
      {
        "id": "C",
        "text": "Create a shadow variant for the domain. Configure SageMaker Inference Recommender to send traffic from the specific IP address to the shadow endpoint."
      },
      {
        "id": "D",
        "text": "Create a VPC route table to deny inbound traffic from the specific IP address. Assign the route table to the domain."
      }
    ],
    "answer": "B",
    "explanation": "B. Create a network ACL inbound rule to deny traffic from the specific IP address. Assign the rule to the default network ACL for the subnet where the domain is located. 31"
  },
  {
    "id": 32,
    "type": "multiple_response",
    "required_answers": 2,
    "question": "An ML engineer is training a simple neural network model. The ML engineer tracks the performance of the model over time on a validation dataset. The model's performance improves substantially at first and then degrades after a specific number of epochs. Which solutions will mitigate this problem? (Select TWO)",
    "options": [
      {
        "id": "A",
        "text": "Enable early stopping on the model."
      },
      {
        "id": "B",
        "text": "Increase dropout in the layers."
      },
      {
        "id": "C",
        "text": "Increase the number of layers."
      },
      {
        "id": "D",
        "text": "Increase the number of neurons."
      },
      {
        "id": "E",
        "text": "Investigate and reduce the sources of model bias."
      }
    ],
    "answer": "A,B",
    "explanation": "A. Enable early stopping on the model. AND B. Increase dropout in the layers. 32"
  },
  {
    "id": 33,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a Retrieval Augmented Generation (RAG) application that uses a vector database to store embeddings of documents. The company must migrate the application to AWS and must implement a solution that provides semantic search of text files. The company has already migrated the text repository to an Amazon S3 bucket. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use an AWS Batch job to process the files and generate embeddings. Use AWS Glue to store the embeddings. Use SQL queries to perform the semantic searches."
      },
      {
        "id": "B",
        "text": "Use a custom Amazon SageMaker AI notebook to run a custom script to generate embeddings. Use SageMaker Feature Store to store the embeddings. Use SQL queries to perform the semantic searches."
      },
      {
        "id": "C",
        "text": "Use the Amazon Kendra S3 connector to ingest the documents from the S3 bucket into Amazon Kendra. Query Amazon Kendra to perform the semantic searches."
      },
      {
        "id": "D",
        "text": "Use an Amazon Textract asynchronous job to ingest the documents from the S3 bucket. Query Amazon Textract to perform the semantic searches."
      }
    ],
    "answer": "C",
    "explanation": "C. Use the Amazon Kendra S3 connector to ingest the documents from the S3 bucket into Amazon Kendra. Query Amazon Kendra to perform the semantic searches. 33"
  },
  {
    "id": 34,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is building an Amazon SageMaker AI pipeline for an ML model. The pipeline uses distributed processing and training. An ML engineer needs to encrypt network communication between instances that run distributed jobs. The ML engineer configures the distributed jobs to run in a private VPC. What should the ML engineer do to meet the encryption requirement?",
    "options": [
      {
        "id": "A",
        "text": "Enable network isolation."
      },
      {
        "id": "B",
        "text": "Configure traffic encryption by using security groups."
      },
      {
        "id": "C",
        "text": "Enable inter-container traffic encryption."
      },
      {
        "id": "D",
        "text": "Enable VPC flow logs."
      }
    ],
    "answer": "C",
    "explanation": "C. Enable inter-container traffic encryption. 34"
  },
  {
    "id": 35,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has trained an ML model that is packaged in a container. The company will integrate the model with an existing Python web application. The company needs to host the model on AWS by using Kubernetes. The company does not want to manage the control plane and must provision the resources in a repeatable manner. The infrastructure must be provisioned by using Python. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS CloudFormation to provision Amazon EC2 instances in multiple Availability Zones. Set up a Kubernetes cluster. Host the model container on the Kubernetes cluster."
      },
      {
        "id": "B",
        "text": "Use the AWS CLI to provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Store the image in an Amazon Elastic Container Registry (Amazon ECR) repository. Host the model container on the EKS cluster."
      },
      {
        "id": "C",
        "text": "Use the AWS Cloud Development Kit (AWS CDK) to provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Store the image in an Amazon Elastic Container Registry (Amazon ECR) repository. Host the model container on the EKS cluster."
      },
      {
        "id": "D",
        "text": "Use AWS CloudFormation to provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Store the image in an Amazon Elastic Container Registry (Amazon ECR) repository. Host the model container on the EKS cluster."
      }
    ],
    "answer": "C",
    "explanation": "C. Use the AWS Cloud Development Kit (AWS CDK) to provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Store the image in an Amazon Elastic Container Registry (Amazon ECR) repository. Host the model container on the EKS cluster. 35"
  },
  {
    "id": 36,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A utility company is using a deep learning model in an Amazon SageMaker AI notebook to predict energy demand. During the hyperparameter selection process of model training, the company is concerned that the learning rate might be too high for the model to converge. The company needs to assess the learning rate during model training. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Configure the training job to use a SageMaker Debugger hook. Monitor the built- in confusion rule."
      },
      {
        "id": "B",
        "text": "Configure the training job to use a SageMaker Debugger hook. Monitor the built- in WeightUpdateRatio rule."
      },
      {
        "id": "C",
        "text": "Configure the training job to use SageMaker Clarify to monitor feature attributions that are based on Shapley Additive explanations (SHAP) values."
      },
      {
        "id": "D",
        "text": "Configure the training job to use SageMaker Clarify to monitor feature attributions that are based on asymmetric Shapley values."
      }
    ],
    "answer": "B",
    "explanation": "B. Configure the training job to use a SageMaker Debugger hook. Monitor the built-in WeightUpdateRatio rule. 36"
  },
  {
    "id": 37,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer trained an ML model on Amazon SageMaker AI to detect automobile accidents from closed-circuit TV footage. The ML engineer used SageMaker Data Wrangler to create a training dataset of images of accidents and non-accidents. The model performed well during training and validation. However, the model is underperforming in production because of variations in the quality of the images from various cameras. Which solution will improve the model's accuracy in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Collect more images from all the cameras. Use Data Wrangler to prepare a new training dataset."
      },
      {
        "id": "B",
        "text": "Recreate the training dataset by using the Data Wrangler corrupt image transform. Specify the impulse noise option."
      },
      {
        "id": "C",
        "text": "Recreate the training dataset by using the Data Wrangler enhance image contrast transform. Specify the Gamma contrast option."
      },
      {
        "id": "D",
        "text": "Recreate the training dataset by using the Data Wrangler resize image transform. Crop all images to the same size."
      }
    ],
    "answer": "D",
    "explanation": "D. Recreate the training dataset by using the Data Wrangler resize image transform. Crop all images to the same size. 37"
  },
  {
    "id": 38,
    "type": "matching",
    "required_answers": 3,
    "question": "An ML engineer is building a generative AI application on Amazon Bedrock by using large language models (LLMs). Select the correct generative AI term from the list for each description.",
    "options": [
      {
        "id": "1",
        "text": "Text representation of basic units of data processed by LLMs"
      },
      {
        "id": "2",
        "text": "High-dimensional vectors that contain the semantic meaning of text"
      },
      {
        "id": "3",
        "text": "Enrichment of information from additional data sources to improve a generated response"
      }
    ],
    "answer": "1-A,2-B,3-C",
    "explanation": "1. Text representation of basic units of data processed by LLMs: Token. 2. High-dimensional vectors that contain the semantic meaning of text: Embedding. 3. Enrichment of information from additional data sources to improve a generated response: Retrieval Augmented Generation (RAG).",
    "matching_choices": [
      {
        "id": "A",
        "text": "Token"
      },
      {
        "id": "B",
        "text": "Embedding"
      },
      {
        "id": "C",
        "text": "Retrieval Augmented Generation (RAG)"
      }
    ]
  },
  {
    "id": 39,
    "type": "multiple_response",
    "required_answers": 2,
    "question": "An ML engineer has developed a binary classification model outside of Amazon SageMaker AI. The ML engineer needs to make the model accessible to a SageMaker Canvas user for additional tuning. The model artifacts are stored in an Amazon S3 bucket. The ML engineer and the Canvas user are part of the same SageMaker AI domain. Which combination of requirements must be met so that the ML engineer can share the model with the Canvas user? (Select TWO)",
    "options": [
      {
        "id": "A",
        "text": "The ML engineer and the Canvas user must be in separate SageMaker AI domains."
      },
      {
        "id": "B",
        "text": "The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored."
      },
      {
        "id": "C",
        "text": "The model must be registered in the SageMaker Model Registry."
      },
      {
        "id": "D",
        "text": "The ML engineer must host the model on AWS Marketplace."
      },
      {
        "id": "E",
        "text": "The ML engineer must deploy the model to a SageMaker AI endpoint."
      }
    ],
    "answer": "B,C",
    "explanation": "B. The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored. AND C. The model must be registered in the SageMaker Model Registry. 41"
  },
  {
    "id": 40,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is deploying a generative AI model-based customer support agent that uses Amazon SageMaker AI for inference. The customer support agent must respond to customer questions about topics such as shipping policies, refund processes, and account management. The generative AI model generates one token at a time. Customers report dissatisfaction with how long the customer support agent takes to generate lengthy responses to questions. The ML engineer must apply an inference optimization technique to improve the performance of the customer support agent. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Compilation"
      },
      {
        "id": "B",
        "text": "Speculative decoding"
      },
      {
        "id": "C",
        "text": "Quantization"
      },
      {
        "id": "D",
        "text": "Fast model loading"
      }
    ],
    "answer": "B",
    "explanation": "B. Speculative decoding 42"
  },
  {
    "id": 41,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has developed a computer vision model. The company needs to deploy the model into production on Amazon SageMaker AI. The company has not hosted a model on SageMaker AI previously. An ML engineer needs to implement a solution to track model versions. The solution also must provide recommendations about which Amazon EC2 instance types to use to host the model. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Register the model in Amazon Elastic Container Registry (Amazon ECR). Use AWS Compute Optimizer for recommendations about instance types."
      },
      {
        "id": "B",
        "text": "Register the model in the SageMaker Model Registry. Use SageMaker Autopilot for recommendations about instance types."
      },
      {
        "id": "C",
        "text": "Register the model in the SageMaker Model Registry. Use SageMaker Inference Recommender for recommendations about instance types."
      },
      {
        "id": "D",
        "text": "Register the model in Amazon Elastic Container Registry (Amazon ECR). Use SageMaker Experiments for recommendations about instance types."
      }
    ],
    "answer": "C",
    "explanation": "C. Register the model in the SageMaker Model Registry. Use SageMaker Inference Recommender for recommendations about instance types. 43"
  },
  {
    "id": 42,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A financial company receives a high volume of real-time market data streams from an external provider. The streams consist of thousands of JSON records every second. The company needs to implement a scalable solution on AWS to identify anomalous data points. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Ingest real-time data into Amazon Kinesis data streams. Use the built-in RANDOM_CUT_FOREST function in Amazon Managed Service for Apache Flink to process the data streams and to detect data anomalies."
      },
      {
        "id": "B",
        "text": "Ingest real-time data into Amazon Kinesis data streams. Deploy an Amazon SageMaker AI endpoint for real-time outlier detection. Create an AWS Lambda function to detect anomalies. Use the data streams to invoke the Lambda function."
      },
      {
        "id": "C",
        "text": "Ingest real-time data into Apache Kafka on Amazon EC2 instances. Deploy an Amazon SageMaker AI endpoint for real-time outlier detection. Create an AWS Lambda function to detect anomalies. Use the data streams to invoke the Lambda function."
      },
      {
        "id": "D",
        "text": "Send real-time data to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create an AWS Lambda function to consume the queue messages. Program the Lambda function to start an AWS Glue extract, transform, and load (ETL) job for batch processing and anomaly detection."
      }
    ],
    "answer": "A",
    "explanation": "A. Ingest real-time data into Amazon Kinesis data streams. Use the built-in RANDOM_CUT_FOREST function in Amazon Managed Service for Apache Flink to process the data streams and to detect data anomalies. 44"
  },
  {
    "id": 43,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to ingest data from data sources into Amazon SageMaker Data Wrangler. The data sources are Amazon S3, Amazon Redshift, and Snowflake. The ingested data must always be up to date with the latest changes in the source systems. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use direct connections to import data from the data sources into Data Wrangler."
      },
      {
        "id": "B",
        "text": "Use cataloged connections to import data from the data sources into Data Wrangler."
      },
      {
        "id": "C",
        "text": "Use AWS Glue to extract data from the data sources. Use AWS Glue also to import the data directly into Data Wrangler."
      },
      {
        "id": "D",
        "text": "Use AWS Lambda to extract data from the data sources. Use Lambda also to import the data directly into Data Wrangler."
      }
    ],
    "answer": "A",
    "explanation": "A. Use direct connections to import data from the data sources into Data Wrangler. 45"
  },
  {
    "id": 44,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is developing a generative AI conversational interface to assist customers with payments. The company wants to use an ML solution to detect customer intent. The company does not have training data to train a model. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Fine-tune a sequence-to-sequence (seq2seq) algorithm in Amazon SageMaker JumpStart."
      },
      {
        "id": "B",
        "text": "Use an LLM from Amazon Bedrock with zero-shot learning."
      },
      {
        "id": "C",
        "text": "Use the Amazon Comprehend DetectEntities API."
      },
      {
        "id": "D",
        "text": "Run an LLM from Amazon Bedrock on Amazon EC2 instances."
      }
    ],
    "answer": "B",
    "explanation": "B. Use an LLM from Amazon Bedrock with zero-shot learning. 46"
  },
  {
    "id": 45,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses a hybrid cloud environment. A model that is deployed on premises uses data in Amazon S3 to provide customers with a live conversational engine. The model is using sensitive data. An ML engineer needs to implement a solution to identify and remove the sensitive data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the model on Amazon SageMaker AI. Create a set of AWS Lambda functions to identify and remove the sensitive data."
      },
      {
        "id": "B",
        "text": "Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. Create an AWS Batch job to identify and remove the sensitive data."
      },
      {
        "id": "C",
        "text": "Use Amazon Macie to identify the sensitive data. Create a set of AWS Lambda functions to remove the sensitive data."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend to identify the sensitive data. Launch Amazon EC2 instances to remove the sensitive data."
      }
    ],
    "answer": "C",
    "explanation": "C. Use Amazon Macie to identify the sensitive data. Create a set of AWS Lambda functions to remove the sensitive data. 47"
  },
  {
    "id": 46,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use an Amazon EMR cluster to process large volumes of data in batches. Any data loss is unacceptable. Which instance purchasing option will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": "A",
        "text": "Run the primary node, core nodes, and task nodes on On-Demand Instances."
      },
      {
        "id": "B",
        "text": "Run the primary node, core nodes, and task nodes on Spot Instances."
      },
      {
        "id": "C",
        "text": "Run the primary node on an On-Demand Instance. Run the core nodes and task nodes on Spot Instances."
      },
      {
        "id": "D",
        "text": "Run the primary node and core nodes on On-Demand Instances. Run the task nodes on Spot Instances."
      }
    ],
    "answer": "D",
    "explanation": "D. Run the primary node and core nodes on On-Demand Instances. Run the task nodes on Spot Instances. 48"
  },
  {
    "id": 47,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer normalized training data by using min-max normalization in AWS Glue DataBrew. The ML engineer must normalize the production inference data in the same way as the training data before passing the production inference data to the model for predictions. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Apply statistics from a well-known dataset to normalize the production samples."
      },
      {
        "id": "B",
        "text": "Keep the min-max normalization statistics from the training set. Use these values to normalize the production samples."
      },
      {
        "id": "C",
        "text": "Calculate a new set of min-max normalization statistics from a batch of production samples. Use these values to normalize all the production samples."
      },
      {
        "id": "D",
        "text": "Calculate a new set of min-max normalization statistics from each production sample. Use these values to normalize all the production samples."
      }
    ],
    "answer": "B",
    "explanation": "B. Keep the min-max normalization statistics from the training set. Use these values to normalize the production samples. 49"
  },
  {
    "id": 48,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company collects customer data every day. The company stores the data as compressed files in an Amazon S3 bucket that is partitioned by date. Every month, analysts download the data, process the data to check the data quality, and then upload the data to Amazon QuickSight dashboards. An ML engineer needs to implement a solution to automatically check the data quality before the data is sent to QuickSight. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Run an AWS Glue crawler every month to update the AWS Glue Data Catalog. Use AWS Glue Data Quality rules to check the data quality."
      },
      {
        "id": "B",
        "text": "Use an AWS Glue trigger to run an AWS Glue crawler every month to update the AWS Glue Data Catalog. Create an AWS Glue job that loads the data into a PySpark DataFrame. Configure the job to apply custom functions and to evaluate the data quality."
      },
      {
        "id": "C",
        "text": "Run Python scripts on an AWS Lambda function every month to evaluate data quality. Configure the S3 bucket to invoke the Lambda function when objects are added to the S3 bucket."
      },
      {
        "id": "D",
        "text": "Configure the S3 bucket to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue when objects are uploaded. Use Amazon CloudWatch insights every month for the SQS queue to evaluate the data quality."
      }
    ],
    "answer": "A",
    "explanation": "A. Run an AWS Glue crawler every month to update the AWS Glue Data Catalog. Use AWS Glue Data Quality rules to check the data quality. 50"
  },
  {
    "id": 49,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is building a deep learning model on Amazon SageMaker AI. The company uses a large amount of data as the training dataset. The company needs to optimize the model's hyperparameters to minimize the loss function on the validation dataset. Which hyperparameter tuning strategy will accomplish this goal with the LEAST computation time?",
    "options": [
      {
        "id": "A",
        "text": "Hyperband"
      },
      {
        "id": "B",
        "text": "Grid search"
      },
      {
        "id": "C",
        "text": "Bayesian optimization"
      },
      {
        "id": "D",
        "text": "Random search"
      }
    ],
    "answer": "A",
    "explanation": "A. Hyperband 51"
  },
  {
    "id": 50,
    "type": "matching",
    "required_answers": 3,
    "question": "A company is using Amazon SageMaker to deploy a new version of its ML model. Select the correct SageMaker traffic shifting strategy from the following list for each use case.",
    "options": [
      {
        "id": "1",
        "text": "Shift traffic in two steps to validate the new model version"
      },
      {
        "id": "2",
        "text": "Shift traffic to the new model version in the shortest amount of time"
      },
      {
        "id": "3",
        "text": "Incrementally shift traffic to the new model version over time"
      }
    ],
    "answer": "1-A,2-B,3-C",
    "explanation": "1. Shift traffic in two steps to validate the new model version: Canary traffic shifting. 2. Shift traffic to the new model version in the shortest amount of time: All at once traffic shifting. 3. Incrementally shift traffic to the new model version over time: Linear traffic shifting.",
    "matching_choices": [
      {
        "id": "A",
        "text": "Canary traffic shifting"
      },
      {
        "id": "B",
        "text": "All at once traffic shifting"
      },
      {
        "id": "C",
        "text": "Linear traffic shifting"
      }
    ]
  },
  {
    "id": 51,
    "type": "multiple_response",
    "required_answers": 2,
    "question": "An ML engineer is designing an AI-powered traffic management system to adjust traffic lights during predicted congestion. The system must use near real-time inference to generate predictions to help prevent traffic collisions. The system must use a batch processing pipeline to perform historical analysis of the predictions to continuously refine and improve the model. The historical analysis will take several hours to evaluate how well the predictions correlate with actual outcomes. The system must be able to scale inference endpoints appropriately to meet demand. Which combination of solutions will meet these requirements? (Select TWO)",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker real-time inference endpoints. Configure the endpoints to scale automatically based on a target tracking scaling policy that uses the metric ConcurrentInvocationsPerInstance."
      },
      {
        "id": "B",
        "text": "Configure reserved concurrency for AWS Lambda functions to process streaming data. Use Lambda SnapStart to connect the Lambda functions to Amazon SageMaker real-time endpoints to support near real-time traffic predictions."
      },
      {
        "id": "C",
        "text": "Configure an Amazon SageMaker Processing job for batch analysis of historical prediction data. Use Amazon EventBridge to schedule the job to run daily. Allow several hours for in-depth analysis to refine and improve the traffic management model."
      },
      {
        "id": "D",
        "text": "Use an Amazon EC2 Auto Scaling group to host containers to support the batch analysis of historical prediction data. Configure scaling based on Amazon CloudWatch metrics to analyze historical traffic patterns and model performance over multiple hours."
      },
      {
        "id": "E",
        "text": "Use an AWS Lambda function to perform the historical analysis. Use Amazon EventBridge to invoke the Lambda function."
      }
    ],
    "answer": "A,C",
    "explanation": "A. Use Amazon SageMaker real-time inference endpoints. Configure the endpoints to scale automatically based on a target tracking scaling policy that uses the metric ConcurrentInvocationsPerInstance. AND C. Configure an Amazon SageMaker Processing job for batch analysis of historical prediction data. Use Amazon EventBridge to schedule the job to run daily. 55"
  },
  {
    "id": 52,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to analyze a large dataset that is stored in Amazon S3 in Apache Parquet format. The company wants to use one-hot encoding for some of the columns. The company needs a no-code solution to transform the data. The solution must store the transformed data back to the same S3 bucket for model training. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Configure an AWS Glue DataBrew project that connects to the data. Use the DataBrew interactive interface to create a recipe that performs the one-hot encoding transformation. Create a job to apply the transformation and to write the output back to an S3 bucket."
      },
      {
        "id": "B",
        "text": "Configure an AWS Glue Data Catalog table that points to the data. Use Amazon Athena to write SQL commands to perform the one-hot encoding transformation. Configure Athena to write the query results back to an S3 bucket."
      },
      {
        "id": "C",
        "text": "Configure an AWS Glue Data Catalog table that points to the data. Create an AWS Glue ETL interactive notebook. Use the notebook to perform the one-hot encoding transformation. Run the configured cells and write the results back to an S3 bucket."
      },
      {
        "id": "D",
        "text": "Configure an Amazon Redshift cluster to access the data by using Redshift Spectrum. Use SQL commands to perform the one-hot encoding transformation within Amazon Redshift. Configure Amazon Redshift to write the results back to an S3 bucket."
      }
    ],
    "answer": "A",
    "explanation": "A. Configure an AWS Glue DataBrew project that connects to the data. Use the DataBrew interactive interface to create a recipe that performs the one-hot encoding transformation. Create a job to apply the transformation and to write the output back to an S3 bucket. 56"
  },
  {
    "id": 53,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using an ML model to classify motion in videos. The data is stored in MP4 format in Amazon S3. When the company created the model, the company needed 4 months to label all the video frames. The company needs to retrain the model with an existing training workflow in Amazon SageMaker AI. An ML engineer must implement a solution that decreases the labeling time. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker Ground Truth to annotate the video frames."
      },
      {
        "id": "B",
        "text": "Use SageMaker JumpStart to use pre-trained computer vision models to develop a labeling model."
      },
      {
        "id": "C",
        "text": "Use SageMaker Data Wrangler to create a data workflow. Use the workflow to optimize the labeling process."
      },
      {
        "id": "D",
        "text": "Use the labeling interface of Amazon Augmented AI (Amazon A2I) with Amazon Rekognition to label the video frames."
      }
    ],
    "answer": "B",
    "explanation": "B. Use SageMaker JumpStart to use pre-trained computer vision models to develop a labeling model. 57"
  },
  {
    "id": 54,
    "type": "matching",
    "required_answers": 4,
    "question": "A company uses Amazon SageMaker AI to support ML workflows such as model training and deployment. Select the correct registry from the following list to meet the requirements for each use case with the LEAST operational overhead. Each registry should be selected one or more times.",
    "options": [
      {
        "id": "1",
        "text": "Tag model packages and use model package groups that include container images for training and deployment."
      },
      {
        "id": "2",
        "text": "Store predefined language packages, kernels, and relevant dependencies."
      },
      {
        "id": "3",
        "text": "Organize models and their images into model groups for better discoverability."
      },
      {
        "id": "4",
        "text": "Pull built-in SageMaker AI images for model training."
      }
    ],
    "answer": "1-B,2-A,3-B,4-A",
    "explanation": "1. Tag model packages and use model package groups: SageMaker Model Registry. 2. Store predefined language packages, kernels, dependencies: Amazon ECR. 3. Organize models into model groups for discoverability: SageMaker Model Registry. 4. Pull built-in SageMaker AI images: Amazon ECR.",
    "matching_choices": [
      {
        "id": "A",
        "text": "Amazon Elastic Container Registry (ECR)"
      },
      {
        "id": "B",
        "text": "SageMaker Model Registry"
      }
    ]
  },
  {
    "id": 55,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company regularly receives new training data from the vendor of an ML model. The vendor delivers cleaned and prepared data to the company's Amazon S3 bucket every 3-4 days. The company has an Amazon SageMaker AI pipeline to retrain the model. An ML engineer needs to implement a solution to run the pipeline when new data is uploaded to the S3 bucket. Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": "A",
        "text": "Create an S3 Lifecycle rule to transfer the data to the SageMaker AI training instance and to initiate training."
      },
      {
        "id": "B",
        "text": "Create an AWS Lambda function that scans the S3 bucket. Program the Lambda function to initiate the pipeline when new data is uploaded."
      },
      {
        "id": "C",
        "text": "Create an Amazon EventBridge rule that has an event pattern that matches the S3 upload. Configure the pipeline as the target of the rule."
      },
      {
        "id": "D",
        "text": "Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the pipeline when new data is uploaded."
      }
    ],
    "answer": "C",
    "explanation": "C. Create an Amazon EventBridge rule that has an event pattern that matches the S3 upload. Configure the pipeline as the target of the rule. 62"
  },
  {
    "id": 56,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is training a new ML model to replace a model that is deployed on an Amazon SageMaker AI real-time endpoint. An ML engineer needs to determine the latency and the accuracy of the new model. The ML engineer must evaluate the new model in a production scenario without affecting the users of the existing model. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Perform a blue/green deployment with linear traffic shifting."
      },
      {
        "id": "B",
        "text": "Perform a blue/green deployment with canary traffic shifting."
      },
      {
        "id": "C",
        "text": "Perform a rolling deployment with a rolling batch size of 50% of the current fleet."
      },
      {
        "id": "D",
        "text": "Perform shadow testing with a traffic sampling percentage of 100%."
      }
    ],
    "answer": "D",
    "explanation": "D. Perform shadow testing with a traffic sampling percentage of 100%. 63"
  },
  {
    "id": 57,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A customer call center uses Amazon Transcribe to convert hundreds of audio recordings of conversations between customers and support agents to text files. The call center wants to use the text files to train an ML model. To comply with industry regulations, the call center must remove customer names, addresses, and phone numbers from the training text files. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Bedrock Guardrails to process and redact personal information from the text files."
      },
      {
        "id": "B",
        "text": "Use the AWS Glue Detect PII transform to remove personal information from the text files."
      },
      {
        "id": "C",
        "text": "Store the text files in Amazon S3 buckets. Use S3 Object Lambda functions to redact personal information."
      },
      {
        "id": "D",
        "text": "Configure an Amazon SageMaker Data Wrangler custom transformation to remove personal information from the text files."
      }
    ],
    "answer": "B",
    "explanation": "B. Use the AWS Glue Detect PII transform to remove personal information from the text files. 64"
  },
  {
    "id": 58,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is developing a linear regression ML model. The model shows high accuracy on the training dataset but performs poorly on unseen new data. Which action should the ML engineer take to address this issue?",
    "options": [
      {
        "id": "A",
        "text": "Increase the complexity of the model to capture more patterns in the training data."
      },
      {
        "id": "B",
        "text": "Apply ML techniques such as cross-validation and regularization. Use Amazon SageMaker Experiments to track and compare different model versions and their performance metrics."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Debugger to monitor for convergence issues. Directly deploy the model into production. Use Amazon SageMaker Clarify to interpret model outputs on new data. Adjust the model based on these insights."
      },
      {
        "id": "D",
        "text": "Increase the size of the training dataset without adjusting the size of the model. Retrain the model on the new data. Generate a confusion matrix to analyze the results."
      }
    ],
    "answer": "B",
    "explanation": "B. Apply ML techniques such as cross-validation and regularization. Use Amazon SageMaker Experiments to track and compare different model versions and their performance metrics. 65"
  },
  {
    "id": 59,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is training a text generation model on Amazon SageMaker AI. After several epochs, the loss function does not converge, and the model's accuracy on the validation dataset starts to show oscillating results. The ML engineer needs to ensure that the model achieves generalization. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Increase the learning rate and decrease the mini-batch size."
      },
      {
        "id": "B",
        "text": "Increase the learning rate as the number of epochs increases."
      },
      {
        "id": "C",
        "text": "Decrease the learning rate and increase the mini-batch size."
      },
      {
        "id": "D",
        "text": "Decrease the learning rate and decrease the mini-batch size."
      }
    ],
    "answer": "C",
    "explanation": "C. Decrease the learning rate and increase the mini-batch size. 66"
  },
  {
    "id": 60,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use an ML model to predict the price of apartments in a specific location. Which metric should the ML engineer use to evaluate the model's performance?",
    "options": [
      {
        "id": "A",
        "text": "Accuracy"
      },
      {
        "id": "B",
        "text": "Area Under the ROC Curve (AUC)"
      },
      {
        "id": "C",
        "text": "F1 score"
      },
      {
        "id": "D",
        "text": "Mean absolute error (MAE)"
      }
    ],
    "answer": "D",
    "explanation": "D. Mean absolute error (MAE) 67"
  },
  {
    "id": 61,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is setting up an Amazon SageMaker AI pipeline for an ML model. The pipeline must automatically initiate a re-training job if any data drift is detected. How should the ML engineer set up the pipeline to meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Use an AWS Glue crawler and an AWS Glue extract, transform, and load (ETL) job to detect data drift. Use AWS Glue triggers to automate the re-training job."
      },
      {
        "id": "B",
        "text": "Use Amazon Managed Service for Apache Flink to detect data drift. Use an AWS Lambda function to automate the re-training job."
      },
      {
        "id": "C",
        "text": "Use SageMaker Model Monitor to detect data drift. Use an AWS Lambda function to automate the re-training job."
      },
      {
        "id": "D",
        "text": "Use Amazon QuickSight anomaly detection to detect data drift. Use an AWS Step Functions workflow to automate the re-training job."
      }
    ],
    "answer": "C",
    "explanation": "C. Use SageMaker Model Monitor to detect data drift. Use an AWS Lambda function to automate the re-training job. 68"
  },
  {
    "id": 62,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to process thousands of existing CSV documents and new CSV documents that are uploaded. The CSV documents are stored in a central Amazon S3 bucket and have the same number of columns. One of the columns is a transaction date. The ML engineer must query the data based on the transaction date. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use an Amazon Athena CREATE TABLE AS SELECT (CTAS) statement to create a table based on the transaction date from data in the central S3 bucket. Query the objects from the table."
      },
      {
        "id": "B",
        "text": "Create a new S3 bucket for processed data. Set up S3 replication from the central S3 bucket to the new S3 bucket. Use S3 Object Lambda to query the objects based on transaction date."
      },
      {
        "id": "C",
        "text": "Create a new S3 bucket for processed data. Use AWS Glue for Apache Spark to create a job to query the CSV objects based on transaction date. Configure the job to store the results in the new S3 bucket."
      },
      {
        "id": "D",
        "text": "Create a new S3 bucket for processed data. Use Amazon Data Firehose to transfer the data from the central S3 bucket to the new S3 bucket. Configure Firehose to run an AWS Lambda function to query the data based on transaction date."
      }
    ],
    "answer": "A",
    "explanation": "A. Use an Amazon Athena CREATE TABLE AS SELECT (CTAS) statement to create a table based on the transaction date from data in the central S3 bucket. Query the objects from the table. 69"
  },
  {
    "id": 63,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use AWS services to identify and extract meaningful unique keywords from documents. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use the Natural Language Toolkit (NLTK) library on Amazon EC2 instances for text pre-processing. Use the Latent Dirichlet Allocation (LDA) algorithm to identify and extract relevant keywords."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker AI and the BlazingText algorithm. Apply custom pre- processing steps for stemming and removal of stop words. Calculate term frequency-inverse document frequency (TF-IDF) scores to identify and extract relevant keywords."
      },
      {
        "id": "C",
        "text": "Store the documents in an Amazon S3 bucket. Create AWS Lambda functions to process the documents and to run Python scripts for stemming and removal of stop words. Use bigram and trigram techniques to identify and extract relevant keywords."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords."
      }
    ],
    "answer": "D",
    "explanation": "D. Use Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords. 70"
  },
  {
    "id": 64,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using ML to predict the presence of a specific weed in a farmer's field. The company is using the Amazon SageMaker AI linear learner built-in algorithm with a value of multiclass_classifier for the predictor_type hyperparameter. What should the company do to MINIMIZE false positives?",
    "options": [
      {
        "id": "A",
        "text": "Set the value of the weight_decay hyperparameter to zero."
      },
      {
        "id": "B",
        "text": "Increase the number of training epochs."
      },
      {
        "id": "C",
        "text": "Increase the value of the target_precision hyperparameter."
      },
      {
        "id": "D",
        "text": "Change the value of the predictor_type hyperparameter to regressor."
      }
    ],
    "answer": "C",
    "explanation": "C. Increase the value of the target_precision hyperparameter. 71"
  },
  {
    "id": 65,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer wants to use, prepare, and load data from Amazon S3 for analytics. The ML engineer must run an extract, transform, and load (ETL) job to discover the schema of the data and to store the metadata. Which solution will meet these requirements with the LEAST manual effort?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS Glue to run the ETL job. Use the job to discover the schema and to store the associated metadata in the AWS Glue Data Catalog."
      },
      {
        "id": "B",
        "text": "Create an Amazon SageMaker Data Wrangler flow to run the ETL job. Use the job to discover the schema and to store the associated metadata in an S3 bucket."
      },
      {
        "id": "C",
        "text": "Create an ETL pipeline by using Amazon Athena integrated with AWS Step Functions. Use the pipeline to run the ETL job to discover the schema and to store the associated metadata in an S3 bucket."
      },
      {
        "id": "D",
        "text": "Launch an Amazon EC2 instance that includes the scikit-learn library to run the ETL job. Use the job to discover the schema and to store the associated metadata in Amazon Redshift."
      }
    ],
    "answer": "A",
    "explanation": "A. Use AWS Glue to run the ETL job. Use the job to discover the schema and to store the associated metadata in the AWS Glue Data Catalog. 72"
  },
  {
    "id": 66,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case Study - A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring. The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3. The company needs to use the central model registry to manage different versions of models in the application. Which action will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create a separate Amazon Elastic Container Registry (Amazon ECR) repository for each model."
      },
      {
        "id": "B",
        "text": "Use Amazon Elastic Container Registry (Amazon ECR) and unique tags for each model version."
      },
      {
        "id": "C",
        "text": "Use the SageMaker Model Registry and model groups to catalog the models."
      },
      {
        "id": "D",
        "text": "Use the SageMaker Model Registry and unique tags for each model version."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Use the SageMaker Model Registry and model groups): y l p n chnh xc nht v Amazon SageMaker Model Registry l tnh nng c thit k chuyn bit (purpose-built)  qun l vng i ca m hnh. Khi nim \"Model Group\" (Nhm m hnh) trong Model Registry cho php bn gp cc phin bn khc nhau ca cng mt m hnh li vi nhau. Khi bn ng k mt m hnh mi vo mt Model Group, SageMaker s t ng nh s phin bn (version 1, 2, 3...) m khng cn thao tc th cng phc tp. iu ny p ng hon ho yu cu \"LEAST operational overhead\" (t chi ph vn hnh nht). Ti sao khng chn A (Create a separate Amazon ECR repository for each model): Amazon ECR (Elastic Container Registry) l ni lu tr Docker container images, khng phi l ni chuyn dng  qun l cc artifact ca m hnh ML hay metadata (nh  chnh xc, tham s hun luyn). Vic to mt repo ring cho tng m hnh s gy ra s ln xn, kh qun l (management sprawl) v tn rt nhiu cng sc vn hnh  theo di phin bn, hon ton i ngc li yu cu \"t chi ph vn hnh\". Ti sao khng chn B (Use Amazon ECR and unique tags): Mc d ECR h tr tags  nh du phin bn image, nhng n vn ch l ni lu tr container image. N thiu cc tnh nng ct li ca mt Model Registry nh: quy trnh ph duyt (approval workflows), theo di ngun gc (lineage tracking), v lin kt metadata ca m hnh. S dng ECR  thay th Model Registry i hi phi xy dng thm cc cng c ty chnh, gy tn km chi ph vn hnh. Ti sao khng chn D (Use the SageMaker Model Registry and unique tags): Mc d s dng ng dch v l SageMaker Model Registry, nhng cch dng \"unique tags\"  qun l phin bn l khng chnh xc v mt k thut v quy trnh. Trong SageMaker Model Registry, c ch qun l phin bn chun l thng qua Model Groups. Tags thng c dng  thm metadata (v d: stage: production, project: alpha) ch khng phi l c ch chnh  catalog cc phin bn m h..."
  },
  {
    "id": 67,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case Study - A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring. The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3. The company is experimenting with consecutive training jobs. How can the company MINIMIZE infrastructure startup times for these jobs?",
    "options": [
      {
        "id": "A",
        "text": "Use Managed Spot Training."
      },
      {
        "id": "B",
        "text": "Use SageMaker managed warm pools."
      },
      {
        "id": "C",
        "text": "Use SageMaker Training Compiler."
      },
      {
        "id": "D",
        "text": "Use the SageMaker distributed data parallelism (SMDDP) library."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Use SageMaker managed warm pools): y l p n chnh xc v tnh nng SageMaker managed warm pools cho php gi li cc instance (my ch) v ti nguyn  c provision (cp pht)  trng thi \"sn sng\" (warm) sau khi mt training job kt thc. Khi chy cc job lin tip (consecutive training jobs), SageMaker c th ti s dng ngay cc instance ny m khng cn tn thi gian khi ng li, ti container image hay ci t th vin, gip gim ng k thi gian khi ng h tng (infrastructure startup times). Ti sao khng chn A (Use Managed Spot Training): Managed Spot Training s dng dung lng d tha ca AWS EC2  tit kim chi ph (cost optimization). Tuy nhin, n khng m bo v thi gian khi ng v thm ch c th lm tng thi gian ch i nu khng c sn dung lng Spot hoc b gin on gia chng. Mc tiu ca Spot l \"gi r\", khng phi \"khi ng nhanh\". Ti sao khng chn C (Use SageMaker Training Compiler): SageMaker Training Compiler l mt trnh bin dch gip ti u ha m ngun m hnh (model code)  tng tc  hun luyn (training throughput) bng cch s dng hiu qu hn phn cng GPU. N tp trung vo vic lm cho qu trnh hun luyn nhanh hn, ch khng gip gim thi gian khi ng h tng (provisioning phase). Ti sao khng chn D (Use the SageMaker distributed data parallelism library): Th vin SMDDP c dng  phn tn d liu hun luyn trn nhiu GPU/instance  hun luyn nhanh hn (parallel processing). Tng t nh p n C, n gii quyt bi ton v tc  x l d liu trong khi train, ch khng gii quyt vn   tr khi khi to ti nguyn ban u."
  },
  {
    "id": 68,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case Study - A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring. The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3. The company must implement a manual approval-based workflow to ensure that only approved models can be deployed to production endpoints. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker Experiments to facilitate the approval process during model registration."
      },
      {
        "id": "B",
        "text": "Use SageMaker ML Lineage Tracking on the central model registry. Create tracking entities for the approval process."
      },
      {
        "id": "C",
        "text": "Use SageMaker Model Monitor to evaluate the performance of the model and to manage the approval."
      },
      {
        "id": "D",
        "text": "Use SageMaker Pipelines. When a model version is registered, use the AWS SDK to change the approval status to \"Approved.\""
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Pipelines & Model Registry Approval): o  bi yu cu mt quy trnh ph duyt th cng (manual approval-based workflow)  kim sot vic deploy model. Trong kin trc MLOps ca AWS, SageMaker Model Registry l thnh phn trung tm lu tr cc phin bn model (Model Versions). o Mi phin bn model trong Registry c thuc tnh ModelApprovalStatus. Gi tr mc nh thng l PendingManualApproval.  deploy model ra production, trng thi ny phi c chuyn sang Approved. o p n D chnh xc v mt k thut v vic thay i trng thi ny c th thc hin thng qua AWS SDK (s dng API UpdateModelPackage) hoc qua giao din SageMaker Studio. y l \"trigger\" chun  cc pipeline CI/CD (nh EventBridge + CodePipeline) bt s kin v thc hin deploy. Ti sao khng chn A (SageMaker Experiments): o SageMaker Experiments c thit k  theo di cc ln chy hun luyn (training runs), so snh metrics v hyperparameters gia cc th nghim. N gip Data Scientist chn ra model tt nht nhng khng c tnh nng qun l trng thi ph duyt (approval status) hay c ch Gatekeeper  chn deploy nh Model Registry. Ti sao khng chn B (SageMaker ML Lineage Tracking): o SageMaker ML Lineage Tracking dng  truy vt ngun gc (lineage) ca d liu v model (v d: Model A c train t Dataset B bi User C). N l cng c  audit (kim ton) lch s, khng phi l cng c  thc thi workflow ph duyt (operational workflow). Vic to \"tracking entities\" khng thay i c trng thi logic  cho php deploy. Ti sao khng chn C (SageMaker Model Monitor): o SageMaker Model Monitor l dch v gim st model sau khi  deploy (theo di Data Drift, Model Quality Drift). N hot ng  giai on vn hnh (Operation), khng tham gia vo quy trnh ph duyt model (Registration & Approval) trc khi deploy."
  },
  {
    "id": 69,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case Study - A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring. The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3. The company needs to run an on-demand workflow to monitor bias drift for models that are deployed to real-time endpoints from the application. Which action will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Configure the application to invoke an AWS Lambda function that runs a SageMaker Clarify job."
      },
      {
        "id": "B",
        "text": "Invoke an AWS Lambda function to pull the sagemaker-model-monitor-analyzer built-in SageMaker image."
      },
      {
        "id": "C",
        "text": "Use AWS Glue Data Quality to monitor bias."
      },
      {
        "id": "D",
        "text": "Use SageMaker notebooks to compare the bias."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (SageMaker Clarify & Lambda): o Gii quyt vn  \"Bias Drift\": Trong h sinh thi AWS, SageMaker Clarify l cng c chuyn bit  pht hin cc loi thin kin (bias) trong d liu v m hnh (c trc v sau khi hun luyn). N c th tnh ton cc ch s bias (nh Class Imbalance, Difference in Proportions, v.v.) da trn d liu thc t thu thp t endpoint. o Gii quyt vn  \"On-demand Workflow\":  bi yu cu quy trnh chy \"theo yu cu\" (on-demand) t ng dng web, ch khng phi chy theo lch trnh c nh (scheduled). AWS Lambda l gii php hon ho  lm cu ni: ng dng gi Lambda -> Lambda gi API CreateProcessingJob  khi chy SageMaker Clarify job. Ti sao khng chn B (sagemaker-model-monitor-analyzer image): o Hnh nh container sagemaker-model-monitor-analyzer ch yu c s dng bi SageMaker Model Monitor  gim st Data Quality (cht lng d liu) v Model Quality ( chnh xc, v d: RMSE, Accuracy). o i vi Bias Drift, SageMaker Model Monitor thc t s dng container ca Clarify bn di. Quan trng hn, hnh ng \"pull image\" (ti image v) khng gii quyt c vn ; bn cn phi chy mt job x l (Processing Job) vi image , ch khng ch n thun l ti n v. Ti sao khng chn C (AWS Glue Data Quality): o AWS Glue Data Quality c thit k  kim tra cht lng d liu trong cc pipeline ETL (v d: kim tra null, nh dng d liu trong S3 hoc Glue Data Catalog). N khng c kh nng phn tch cc c tnh chuyn su ca ML Model nh Bias (s thin v ca thut ton) hay Model Drift. Ti sao khng chn D (SageMaker notebooks): o SageMaker Notebooks l mi trng pht trin tng tc (Interactive Development) dnh cho Data Scientist  th nghim code. N khng phi l mt gii php vn hnh (Operational Solution)  tch hp vo ng dng web chy t ng. Bn khng th dng Notebook  p ng yu cu \"on- demand workflow\" cho production mt cch n nh v t ng ha."
  },
  {
    "id": 70,
    "type": "ordering",
    "required_answers": 3,
    "question": "A company stores historical data in .csv files in Amazon S3. Only some of the rows and columns in the .csv files are populated. The columns are not labeled. An ML engineer needs to prepare and store the data so that the company can use the data to train ML models. Task: Select and order the correct steps from the following list to perform this task. (Select three and place them in the correct order).",
    "options": [],
    "answer": "1,2,3",
    "explanation": "Bc 1: Use AWS Glue crawlers to infer the schemas and available columns. o L do chn: D liu u vo l CSV th, thiu nhn ct (columns are not labeled) v khng ng nht.  x l, h thng cn hiu cu trc d liu (metadata/schema). AWS Glue Crawler l dch v chuyn dng  qut S3, t ng nhn din nh dng, suy lun schema v to bng trong AWS Glue Data Catalog. y l bc tin   cc cng c khc (nh DataBrew) c th hiu d liu u vo. o Ti sao khng dng Amazon Athena: Athena dng  truy vn d liu bng SQL da trn schema  c (thng do Glue Crawler to ra). Athena khng phi l cng c ti u  \"infer schema\" v lu tr metadata cho quy trnh ETL. Bc 2: Use AWS Glue DataBrew for data cleaning and feature engineering. o L do chn:  bi m t d liu rt \"bn\" (missing rows, missing columns, unlabelled). AWS Glue DataBrew l cng c visual data preparation (no-code), cho php ngi dng nhn thy trc quan d liu, d dng thc hin cc thao tc nh lm sch (cleaning), in gi tr thiu (imputation), v gn nhn ct m khng cn vit code phc tp. o Ti sao khng dng SageMaker Batch Transform: Batch Transform dng  chy suy lun (inference) trn hng lot d liu bng mt model  c hun luyn. N khng dng  lm sch d liu th ban u. Bc 3: Store the resulting data back in Amazon S3. o L do chn: Sau khi d liu  c lm sch v chun ha (Prepared Data), n cn c lu tr li vo mt v tr bn vng  SageMaker c th truy cp cho vic Training. Amazon S3 l ni lu tr tiu chun cho d liu hun luyn ML."
  },
  {
    "id": 71,
    "type": "ordering",
    "required_answers": 3,
    "question": "An ML engineer needs to use Amazon SageMaker Feature Store to create and manage features to train a model. Task: Select and order the steps from the following list to create and use the features in Feature Store. (Select three and place them in the correct order).",
    "options": [],
    "answer": "1,2,3",
    "explanation": "Bc 1: Create a feature group. o L do: Trc khi lu tr bt k d liu no, bn bt buc phi nh ngha \"Feature Group\". y l bc thit lp schema (tn ct, kiu d liu) v cu hnh h tng lu tr (Online/Offline store). Khng c Feature Group th khng c ch  cha d liu. Bc 2: Ingest the records. o L do: Sau khi  c \"thng cha\" (Feature Group), bc tip theo l np d liu (Ingestion) vo . D liu c a vo thng qua API PutRecord hoc streaming. Bc 3: Access the store to build datasets for training. o L do: Khi d liu  nm trong Store, bn mi c th truy xut (Retrieve/Query) n  to thnh cc tp d liu hun luyn (Training Datasets) hoc phc v cho suy lun (Inference)."
  },
  {
    "id": 72,
    "type": "ordering",
    "required_answers": 3,
    "question": "A company wants to host an ML model on Amazon SageMaker. An ML engineer is configuring a continuous integration and continuous delivery (CI/CD) pipeline in AWS CodePipeline to deploy the model. The pipeline must run automatically when new training data for the model is uploaded to an Amazon S3 bucket. Task: Select and order the pipeline's correct steps from the following list. Each step should be selected one time or not at all. (Select and order three.)",
    "options": [],
    "answer": "1,2,3",
    "explanation": "Bc 1: An S3 event notification invokes the pipeline when new data is uploaded. o Ti sao chn:  t ng ha quy trnh khi c \"new training data\", chng ta cn mt c ch Trigger (kch hot). AWS CodePipeline h tr trc tip S3 Source Action. Khi c object mi c upload (PutObject), S3 s gi event notification (thng qua EventBridge hoc trc tip)  khi ng Pipeline. o Ti sao khng chn S3 Lifecycle rule: S3 Lifecycle ch dng  qun l vng i lu tr (v d: chuyn sang Glacier sau 30 ngy, xa sau 1 nm), hon ton khng c chc nng trigger CI/CD pipeline. Bc 2: SageMaker retrains the model by using the data in the S3 bucket. o Ti sao chn: Mc ch ca vic upload d liu mi l  cp nht model. Do , bc logic tip theo sau khi pipeline chy l phi gi SageMaker Training Job  hun luyn li (Retrain) model trn d liu mi . Bc 3: The pipeline deploys the model to a SageMaker endpoint. o Ti sao chn:  bi nu r mc tiu l \"host an ML model\" v \"deploy the model\".  model c th phc v suy lun (inference), n phi c deploy ra mt SageMaker Endpoint (Real-time hoc Serverless). y l ch n cui cng ca quy trnh CD (Continuous Delivery). o Ti sao khng chn Model Registry: Mc d Model Registry l mt bc quan trng trong MLOps ( qun l phin bn), nhng vic a model vo Registry ch l \"lu tr/ng k\" (Register), cha phi l \"trin khai  chy\" (Deploy/Host). V  bi yu cu ch chn 3 bc  hon thnh vic \"host\", nn Endpoint l p n chnh xc hn  tha mn yu cu kinh doanh."
  },
  {
    "id": 73,
    "type": "matching",
    "required_answers": 3,
    "question": "An ML engineer is building a generative AI application on Amazon Bedrock by using large language models (LLMs). Task: Select the correct generative AI term from the following list for each description. Each term should be selected one time or not at all. (Select three.)",
    "options": [],
    "answer": "?",
    "explanation": "Token: o nh ngha: L n v vn bn nh nht m LLM c th x l. Mt token c th l mt t, mt phn ca t, hoc mt du cu (v d: t \"Amazon\" c th l 1 token, nhng \"antidisestablishmentarianism\" c th b tch thnh nhiu token). o Ti sao chn: Bt c khi no  cp n \"unit of text\", \"processing unit\", hoc chi ph tnh tin (billing based on input/output count),  chnh l Token. Embedding: o nh ngha: L k thut chuyn i vn bn (text) thnh cc dy s thc (vectors) trong khng gian nhiu chiu. Cc vector ny gip my tnh hiu c \" ngha ng ngha\" (semantic meaning). Cc t c ngha gn nhau s c vector nm gn nhau. o Ti sao chn: T kha nhn din l \"Vector\", \"High-dimensional\", \"Semantic meaning\", hoc \"Numerical representation\". Retrieval Augmented Generation (RAG): o nh ngha: L kin trc kt hp sc mnh ca LLM vi d liu bn ngoi (External Knowledge Base). Trc khi gi cu hi cho LLM, h thng s tm kim thng tin lin quan t c s d liu ring, kp thng tin  vo prompt  LLM tr li chnh xc hn. o Ti sao chn: T kha l \"External data\", \"Knowledge base\", \"Augment response\", hoc \"Context\". Ti sao khng chn Temperature: o nh ngha: y l mt siu tham s (hyperparameter) cu hnh  \"sng to\" hoc \"ngu nhin\" ca model (gi tr t 0 n 1). Temperature thp (gn 0) cho kt qu nht qun, chnh xc; Temperature cao (gn 1) cho kt qu a dng, sng to hn. o  bi ch yu cu chn 3 thut ng tng ng vi 3 m t v cu trc/k thut, khng hi v tham s cu hnh."
  },
  {
    "id": 74,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is working on an ML model to predict the prices of similarly sized homes. The model will base predictions on several features. The ML engineer will use the following feature engineering techniques to estimate the prices of the homes. Task: Select the correct feature engineering techniques for the following list of features. Each feature engineering technique should be selected one time or not at all (Select three). Techniques List: * Feature splitting * Logarithmic transformation * One-hot encoding * Standardized distribution Features to Map (Reconstructed): 1. City (Name): (e.g., \"Seattle\", \"Austin\", \"Boston\") 2. Type_Year: (e.g., \"Apartment_2015\", \"House_1998\") 3. Size of the building: (e.g., Square feet or Square Meters)",
    "options": [],
    "answer": "?",
    "explanation": "1. City (Name)  One-hot encoding: o Ti sao: \"City\" l d liu nh danh (Categorical/Nominal data) khng c th t t nhin (v d: H Ni khng ln hn hay nh hn TP.HCM v mt s hc).  a vo m hnh ML, ta khng th dng Label Encoding (gn 1, 2, 3...) v s to ra th t gi to. One-hot encoding l k thut chun  bin mi thnh ph thnh mt vector nh phn (0/1), gip m hnh i x bnh ng vi cc thnh ph. 2. Type_Year  Feature splitting: o Ti sao: y l mt thuc tnh ghp (Composite Feature) cha hai thng tin hon ton khc nhau: \"Loi nh\" (Categorical) v \"Nm xy dng\" (Numerical/Time).  m hnh hc hiu qu, chng ta cn tch (Split) chui ny ra thnh hai ct ring bit. Sau  mi x l tip (v d: Type th One-hot, Year th tnh tui nh). 3. Size of the building  Standardized distribution: o Ti sao: Din tch nh l bin s lin tc (Numerical Continuous). Trong cc thut ton hi quy (Regression)  d on gi nh, vic cc bin s c n v khc nhau (v d: din tch l hng ngn m2, trong khi s phng ng ch l 1-5) s lm m hnh kh hi t. Standardized distribution (chun ha Z- score v Mean=0, Std=1) gip a d liu v cng mt t l. o Ti sao khng chn Logarithmic transformation: Mc d Log transform thng dng cho din tch (do phn phi lch phi), nhng  bi c t kha quan trng: \"similarly sized homes\" (cc ngi nh c kch thc tng ng). iu ny m ch d liu din tch khng b lch qu nhiu (skewness thp), nn vic chun ha (Standardization) l u tin hp l hn  ci thin tc  hun luyn."
  },
  {
    "id": 75,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case study - An ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data. Which AWS service or feature can aggregate the data from the various data sources?",
    "options": [
      {
        "id": "A",
        "text": "Amazon EMR Spark jobs"
      },
      {
        "id": "B",
        "text": "Amazon Kinesis Data Streams"
      },
      {
        "id": "C",
        "text": "Amazon DynamoDB"
      },
      {
        "id": "D",
        "text": "AWS Lake Formation"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (EMR Spark jobs):  bi  cp n vic d liu c vn  v \"mt cn bng lp\" (class imbalance) v cc c trng c \"s ph thuc ln nhau\" (interdependencies).  gii quyt vn  ny nhm gip thut ton nm bt c pattern, k s ML cn thc hin Feature Engineering (K thut c trng) v Data Transformation phc tp (nh SMOTE  cn bng lp, hoc Join cc bng  to c trng tng hp). Apache Spark trn Amazon EMR l cng c x l d liu phn tn (Distributed Processing) tiu chun, h tr cc connector  c d liu t nhiu ngun khc nhau (S3, JDBC cho MySQL) v thc hin cc php tnh ton JOIN, GROUP BY quy m ln  \"aggregate\" (tng hp) d liu thnh mt tp training set sch. Ti sao khng chn B (Amazon Kinesis Data Streams): Kinesis l dch v dnh cho vic thu thp v x l d liu Streaming (thi gian thc). Trong khi , d liu trong  bi (transaction logs, tables, profiles) l d liu tnh/lch s (batch data), nn Kinesis khng ph hp. Ti sao khng chn C (Amazon DynamoDB): DynamoDB l c s d liu NoSQL (Key-Value) c ti u cho cc truy vn nhanh (low latency) ca ng dng, khng phi l mt cng c Analytics hay ETL  thc hin vic tng hp d liu (aggregation) t nhiu ngun khc nhau. Ti sao khng chn D (AWS Lake Formation): Lake Formation l dch v gip thit lp, bo mt v qun tr (Governance) Data Lake. Mc d n c th gip \"gom\" d liu v S3 (thng qua Glue Blueprints), nhng bn thn n l mt lp qun l quyn truy cp v catalog, khng phi l compute engine  x l logic Feature Engineering phc tp (nh x l class imbalance hay interdependencies) m  bi yu cu."
  },
  {
    "id": 76,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case study - An ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data. After the data is aggregated, the ML engineer must implement a solution to automatically detect anomalies in the data and to visualize the result. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Athena to automatically detect the anomalies and to visualize the result."
      },
      {
        "id": "B",
        "text": "Use Amazon Redshift Spectrum to automatically detect the anomalies. Use Amazon QuickSight to visualize the result."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Data Wrangler to automatically detect the anomalies and to visualize the result."
      },
      {
        "id": "D",
        "text": "Use AWS Batch to automatically detect the anomalies. Use Amazon QuickSight to visualize the result."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Data Wrangler): Trong quy trnh Machine Learning, SageMaker Data Wrangler l dch v chuyn bit gip cc k s ML chun b d liu.2 N cung cp tnh nng Data Quality and Insights Report (Bo co cht lng v thng tin d liu) v Analyses tch hp sn.3 Tnh nng ny c th t ng pht hin cc bt thng (anomalies), trng lp, target leakage (r r d liu mc tiu) v mt cn bng lp (class imbalance) ngay khi nhp d liu. Ngoi ra, Data Wrangler cn tch hp sn kh nng trc quan ha (visualization) nh histogram, scatter plots m khng cn vit m hay tch hp thm cng c bn ngoi nh QuickSight.4 y l gii php \"t thao tc nht\" (low-code) p ng trn vn yu cu ca  bi. Ti sao khng chn A (Amazon Athena): Athena l dch v truy vn tng tc (Interactive Query Service) s dng SQL  truy vn d liu trn S3.5 Mc d bn c th vit cu lnh SQL  tm cc gi tr ngoi lai, nhng n khng c tnh nng \"t ng pht hin bt thng\" dnh cho ML (nh Isolation Forest hay thng k phn phi t ng). Hn na, Athena khng c kh nng trc quan ha d liu (visualization) tch hp sn m bt buc phi kt ni vi Amazon QuickSight, lm tng  phc tp so vi yu cu. Ti sao khng chn B (Amazon Redshift Spectrum): Redshift Spectrum cho php truy vn d liu trn S3 thng qua Redshift, nhng bn cht y l gii php Kho d liu (Data Warehousing). Vic thit lp mt cm Redshift ch  pht hin bt thng v trc quan ha trong giai on chun b d liu ML l qu cng knh (overkill) v tn km. Tng t Athena, Redshift cn QuickSight  trc quan ha ch khng c sn cng c visual mnh m trong console cho quy trnh ML prep. Ti sao khng chn D (AWS Batch): AWS Batch l dch v iu phi v chy cc tc v tnh ton hng lot (batch computing) bng container.6 N hon ton khng phi l cng c phn tch d liu hay trc quan ha.  dng Batch, bn phi t vit code pht hin bt thng, ng gi vo Docker ..."
  },
  {
    "id": 77,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case study - An ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data. The training dataset includes categorical data and numerical data. The ML engineer must prepare the training dataset to maximize the accuracy of the model. Which action will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS Glue to transform the categorical data into numerical data."
      },
      {
        "id": "B",
        "text": "Use AWS Glue to transform the numerical data into categorical data."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Data Wrangler to transform the categorical data into numerical data."
      },
      {
        "id": "D",
        "text": "Use Amazon SageMaker Data Wrangler to transform the numerical data into categorical data."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Data Wrangler + Categorical to Numerical): o Nguyn l ML: Hu ht cc thut ton Machine Learning (nh trong bi ton pht hin gian ln) hot ng da trn ton hc (ma trn, khong cch vect), do  bt buc phi chuyn i d liu phn loi (categorical data - v d: \"High\", \"Medium\", \"Low\") thnh d liu s (numerical data - v d: 2, 1, 0 hoc One-hot encoding). o Ti u vn hnh (Least Operational Overhead): SageMaker Data Wrangler cung cp cc tnh nng chuyn i tch hp sn (built-in transforms) nh \"Encode Categorical\" ngay trn giao din ngi dng (GUI).1 K s ML ch cn chn ct v loi encoding m khng cn vit code ETL phc tp hay qun l h tng, p ng tt nht yu cu v gim thiu vn hnh. Ti sao khng chn A (AWS Glue + Categorical to Numerical): Mc d AWS Glue c th thc hin vic ny, nhng Glue c thit k thin v cc tc v ETL k thut d liu quy m ln (Data Engineering). Vic thit lp Glue Job, vit script (PySpark/Python), v debug thng tn nhiu cng sc vn hnh hn so vi tri nghim \"click-and-config\" chuyn bit cho ML ca Data Wrangler. Ti sao khng chn B (AWS Glue + Numerical to Categorical): y l hng x l sai v mt khoa hc d liu. Thng thng, chng ta cn bin d liu ch thnh s  my hc, ch khng phi bin d liu s (ang c gi tr nh lng) thnh d liu phn loi (tr cc trng hp binning rt c th). Vic ny s lm mt mt thng tin quan trng. Ti sao khng chn D (SageMaker Data Wrangler + Numerical to Categorical): Tng t p n B, hng chuyn i ny (Numerical -> Categorical) l khng hp l cho mc tiu \"ti a ha  chnh xc m hnh\" trong bi cnh chung. D cng c ng (Data Wrangler) nhng phng php sai."
  },
  {
    "id": 78,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case study - An ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data. Before the ML engineer trains the model, the ML engineer must resolve the issue of the imbalanced data. Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Athena to identify patterns that contribute to the imbalance. Adjust the dataset accordingly."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker Studio Classic built-in algorithms to process the imbalanced dataset."
      },
      {
        "id": "C",
        "text": "Use AWS Glue DataBrew built-in features to oversample the minority class."
      },
      {
        "id": "D",
        "text": "Use the Amazon SageMaker Data Wrangler balance data operation to oversample the minority class."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Data Wrangler):  bi yu cu gii quyt vn  mt cn bng d liu (class imbalance) vi \"n lc vn hnh t nht\" (LEAST operational effort). SageMaker Data Wrangler cung cp sn mt php bin i (transform) tn l \"Balance Data\". Tnh nng ny cho php ngi dng thc hin Oversample (nhn bn lp thiu s), Undersample (gim lp a s) hoc s dng thut ton SMOTE (Synthetic Minority Over-sampling Technique) ch bng vi c nhp chut trn giao din  ha. y l gii php chuyn bit cho ML Prep v khng yu cu vit code. Ti sao khng chn A (Amazon Athena): Athena dng  truy vn v phn tch d liu bng SQL. Mc d bn c th dng SQL  lc v trch xut d liu nhm cn bng th cng, nhng n khng c chc nng \"t ng cn bng\" (nh SMOTE). Vic phi vit query phc tp  iu chnh dataset i hi n lc vn hnh cao hn nhiu so vi cng c GUI c sn ca Data Wrangler. Ti sao khng chn B (Amazon SageMaker Studio Classic built-in algorithms): Cc thut ton tch hp sn (built-in algorithms) ca SageMaker (nh XGBoost, Linear Learner) thng c tham s  x l trng s lp (class weights), nhng  l cu hnh trong qu trnh hun luyn (training phase).  bi ang hi v vic x l chnh b d liu trc khi train (\"Before the ML engineer trains the model\"). Ngoi ra, vic tinh chnh hyperparameter khng gii quyt trit  vn   mc dataset tt bng vic oversampling. Ti sao khng chn C (AWS Glue DataBrew): Glue DataBrew cng l cng c chun b d liu trc quan (visual data prep). Tuy nhin, DataBrew tp trung vo lm sch v chun ha d liu chung (ETL/Data Cleaning). DataBrew khng cung cp sn cc tnh nng chuyn su  cn bng d liu cho ML nh SMOTE hay cc k thut oversampling phc tp mt cch t ng (built-in recipe step for balancing) mnh m v trc din nh Data Wrangler."
  },
  {
    "id": 79,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Case study - An ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data. The ML engineer needs to use an Amazon SageMaker built-in algorithm to train the model. Which algorithm should the ML engineer use to meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "LightGBM"
      },
      {
        "id": "B",
        "text": "Linear learner"
      },
      {
        "id": "C",
        "text": "-means clustering"
      },
      {
        "id": "D",
        "text": "Neural Topic Model (NTM)"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (LightGBM): . Tnh nng Built-in: Hin ti, LightGBM  chnh thc l mt Amazon SageMaker built-in algorithm. (Trc y n ch c th dng qua script mode, nhng AWS  cp nht h tr native container cho LightGBM). . Gii quyt \"Class Imbalance\": LightGBM cc k mnh m trong vic x l d liu mt cn bng (class imbalance) thng qua tham s scale_pos_weight hoc tnh nng weighted loss t ng, iu m bi ton pht hin gian ln (fraud detection) lun gp phi. . Gii quyt \"Interdependencies\":  bi nhn mnh d liu c s ph thuc ln nhau gia cc tnh nng (feature interdependencies) v thut ton hin ti khng nm bt c cc mu (patterns). LightGBM l thut ton da trn cy quyt nh (Decision Tree-based Gradient Boosting), n t ng nm bt c cc mi quan h phi tuyn tnh (non-linear) v tng tc phc tp gia cc tnh nng tt hn nhiu so vi cc m hnh tuyn tnh. Ti sao khng chn B (Linear learner): Mc d Linear Learner l mt thut ton built-in v c th x l binary classification, nhng n hot ng da trn gi nh v mi quan h tuyn tnh (linear relationship) gia u vo v u ra.  bi ni r rng thut ton hin ti khng nm bt c cc mu tim n v cc tnh nng c s ph thuc ln nhau (iu thng to ra ranh gii quyt nh phi tuyn tnh). Linear Learner s gp kh khn ln (underfitting) vi loi d liu phc tp ny so vi cc thut ton Tree-based nh LightGBM hay XGBoost. Ti sao khng chn C (K-means clustering): K-means l thut ton hc khng gim st (Unsupervised Learning) dng  gom nhm d liu. Trong bi ton ny, chng ta c \"class imbalance\" (ng  d liu  c gn nhn l gian ln hoc khng), do  y l bi ton hc c gim st (Supervised Learning). K-means khng ph hp  hun luyn m hnh phn lp gian ln chnh xc trong ng cnh ny. Ti sao khng chn D (Neural Topic Model - NTM): NTM l thut ton chuyn dng cho x l ngn ng t nhin (NLP)  tm cc ch  (to..."
  },
  {
    "id": 80,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has deployed an XGBoost prediction model in production to predict if a customer is likely to cancel a subscription. The company uses Amazon SageMaker Model Monitor to detect deviations in the F1 score. During a baseline analysis of model quality, the company recorded a threshold for the F1 score. After several months of no change, the model's F1 score decreases significantly. What could be the reason for the reduced F1 score?",
    "options": [
      {
        "id": "A",
        "text": "Concept drift occurred in the underlying customer data that was used for predictions."
      },
      {
        "id": "B",
        "text": "The model was not sufficiently complex to capture all the patterns in the original baseline data."
      },
      {
        "id": "C",
        "text": "The original baseline data had a data quality issue of missing values."
      },
      {
        "id": "D",
        "text": "Incorrect ground truth labels were provided to Model Monitor during the calculation of the baseline."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Concept drift): \"Concept drift\" (Tri dt khi nim) xy ra khi mi quan h thng k gia cc tnh nng u vo (features) v bin mc tiu (target label) thay i theo thi gian. Trong bi ton d on hy ng k (churn), hnh vi khch hng thay i theo thi gian (v d: do xu hng th trng mi, i th cnh tranh mi) khin cc mu (patterns) m m hnh  hc trc y khng cn ng na. c im nhn dng chnh trong  bi l m hnh hot ng n nh trong vi thng (\"after several months of no change\") ri mi b suy gim ch s F1. y l du hiu kinh in ca vic m hnh b li thi do Concept Drift. Ti sao khng chn B (Model complexity): Nu m hnh khng  phc tp (underfitting), hiu sut (F1 score) s thp ngay t giai on hun luyn v trin khai ban u (Day 1). N s khng th duy tr n nh trong vi thng ri mi gim t ngt nh m t. Ti sao khng chn C (Data quality issue in baseline): Cc vn  v cht lng d liu trong tp baseline (nh thiu gi tr) s nh hng n cht lng m hnh hoc ch s baseline ngay t u. N khng gii thch c nguyn nhn ti sao hiu sut li suy gim sau mt khong thi gian di vn hnh n nh. Ti sao khng chn D (Incorrect ground truth labels in baseline): Nu nhn thc t (ground truth) b sai trong qu trnh tnh ton baseline, th ch s F1 ca baseline s b tnh sai ngay t thi im . iu ny dn n vic thit lp ngng (threshold) sai, nhng n l mt li tnh (static error). N khng gii thch c hin tng \"suy gim\" (degradation) din ra theo thi gian thc ca m hnh hin ti."
  },
  {
    "id": 81,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a team of data scientists who use Amazon SageMaker notebook instances to test ML models. When the data scientists need new permissions, the company attaches the permissions to each individual role that was created during the creation of the SageMaker notebook instance. The company needs to centralize management of the team's permissions. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Create a single IAM role that has the necessary permissions. Attach the role to each notebook instance that the team uses."
      },
      {
        "id": "B",
        "text": "Create a single IAM group. Add the data scientists to the group. Associate the group with each notebook instance that the team uses."
      },
      {
        "id": "C",
        "text": "Create a single IAM user. Attach the AdministratorAccess AWS managed IAM policy to the user. Configure each notebook instance to use the IAM user."
      },
      {
        "id": "D",
        "text": "Create a single IAM group. Add the data scientists to the group. Create an IAM role. Attach the AdministratorAccess AWS managed IAM policy to the role. Associate the role with the group. Associate the group with each notebook instance that the team uses."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Single IAM Role): SageMaker Notebook Instance hot ng da trn c ch IAM Execution Role  tng tc vi cc dch v khc (nh S3, Glue, Athena). Bng cch to mt IAM Role chung cha tt c quyn cn thit v gn Role ny cho tt c cc Notebook Instance ca nhm, bn t c mc tiu \"centralize management\" (qun l tp trung). Khi cn thm hoc bt quyn, bn ch cn sa Policy ca Role duy nht ny mt ln, thay v phi sa tng Role ring l ca tng Notebook. Ti sao khng chn B (IAM Group): IAM Group c thit k  qun l quyn cho IAM Users (con ngi), KHNG phi cho ti nguyn AWS (AWS Resources). Bn khng th gn mt IAM Group trc tip cho mt SageMaker Notebook Instance. Ti nguyn AWS phi s dng IAM Role. Ti sao khng chn C (IAM User & AdministratorAccess): Th nht, v mt k thut, Notebook Instance s dng IAM Role, khng s dng IAM User  chy (service assumption). Th hai, vic gn quyn AdministratorAccess (ton quyn qun tr) vi phm nghim trng nguyn tc bo mt \"Least Privilege\" (Quyn ti thiu), gy ri ro an ninh ln nu Notebook b xm nhp. Ti sao khng chn D (Complex setup & IAM Group): Tng t nh p n B, bn khng th gn IAM Group cho Notebook Instance. Ngoi ra, gii php ny qu phc tp v cng vi phm nguyn tc \"Least Privilege\" khi s dng AdministratorAccess."
  },
  {
    "id": 82,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use an ML model to predict the price of apartments in a specific location. Which metric should the ML engineer use to evaluate the model's performance?",
    "options": [
      {
        "id": "A",
        "text": "Accuracy"
      },
      {
        "id": "B",
        "text": "Area Under the ROC Curve (AUC)"
      },
      {
        "id": "C",
        "text": "F1 score"
      },
      {
        "id": "D",
        "text": "Mean absolute error (MAE)"
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Mean absolute error - MAE): Bi ton yu cu d on gi cn h (price of apartments). Gi tin l mt bin s lin tc (continuous variable), do  y l bi ton Hi quy (Regression). Trong Machine Learning, cc ch s dng  nh gi m hnh Hi quy bao gm MAE (Sai s tuyt i trung bnh), MSE (Sai s bnh phng trung bnh), v RMSE (Cn bc hai ca MSE). MAE o lng mc  sai lch trung bnh gia gi tr d on v gi tr thc t, rt ph hp  nh gi  chnh xc ca d on gi. Ti sao khng chn A (Accuracy): Accuracy ( chnh xc) l t l s ln d on ng trn tng s ln d on. Ch s ny ch dng cho bi ton Phn loi (Classification) (v d: Cn h ny c bn c khng? - C/Khng). N khng c  ngha i vi d liu lin tc nh gi tin. Ti sao khng chn B (Area Under the ROC Curve - AUC): AUC l din tch di ng cong ROC, c s dng  nh gi hiu sut ca m hnh Phn loi nh phn (Binary Classification) ti cc ngng phn loi khc nhau. N hon ton khng p dng cho bi ton Hi quy. Ti sao khng chn C (F1 score): F1 score l trung bnh iu ha (harmonic mean) gia Precision ( chnh xc) v Recall ( nhy). y l ch s quan trng cho cc bi ton Phn loi (Classification), c bit l khi d liu b mt cn bng (imbalanced data). N khng dng  o lng sai s ca mt gi tr lin tc."
  },
  {
    "id": 83,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer has trained a neural network by using stochastic gradient descent (SGD). The neural network performs poorly on the test set. The values for training loss and validation loss remain high and show an oscillating pattern. The values decrease for a few epochs and then increase for a few epochs before repeating the same cycle. What should the ML engineer do to improve the training process?",
    "options": [
      {
        "id": "A",
        "text": "Introduce early stopping."
      },
      {
        "id": "B",
        "text": "Increase the size of the test set."
      },
      {
        "id": "C",
        "text": "Increase the learning rate."
      },
      {
        "id": "D",
        "text": "Decrease the learning rate."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Decrease the learning rate): Hin tng gi tr hm mt mt (loss function) khng gim dn m c biu hin \"oscillating pattern\" (dao ng ln xung) v duy tr  mc cao l du hiu kinh in ca vic Learning Rate (tc  hc) qu cao. Trong thut ton Gradient Descent, nu bc nhy (learning rate) qu ln, thut ton s \"nhy qua\" (overshoot) im cc tiu (minima) thay v trt xung y ca n. N s lin tc nhy qua li gia hai sn dc ca hm mt mt, gy ra hin tng dao ng. Vic gim Learning Rate s thu nh bc nhy, gip thut ton i xung t t v hi t chnh xc vo im cc tiu. Ti sao khng chn A (Introduce early stopping): Early Stopping l k thut dng  chng Overfitting (khi Training Loss gim nhng Validation Loss bt u tng).1 Trong trng hp ny, c hai loss u cao v dao ng (ngha l m hnh cha hc c hoc khng hi t - underfitting/non-convergence), nn vic dng sm s ch khin bn nhn c mt m hnh km cht lng, khng gii quyt c nguyn nhn gc r l m hnh khng th hi t. Ti sao khng chn B (Increase the size of the test set): Tp Test (Test set) ch dng  nh gi hiu nng m hnh sau khi hun luyn xong. Kch thc ca tp Test hon ton khng tham gia vo qu trnh tnh ton Gradient hay cp nht trng s (weights) trong qu trnh hun luyn, do  khng th sa cha li dao ng ca hm loss. Ti sao khng chn C (Increase the learning rate): Nu Learning Rate hin ti  gy ra dao ng (do qu cao), vic tng n thm na s lm bc nhy cng ln hn. iu ny s khin dao ng mnh hn hoc lm m hnh phn k hon ton (Divergence - Loss tng vt ln v cc), lm tnh hnh ti t hn."
  },
  {
    "id": 84,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to process thousands of existing CSV objects and new CSV objects that are uploaded. The CSV objects are stored in a central Amazon S3 bucket and have the same number of columns. One of the columns is a transaction date. The ML engineer must query the data based on the transaction date. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use an Amazon Athena CREATE TABLE AS SELECT (CTAS) statement to create a table based on the transaction date from data in the central S3 bucket. Query the objects from the table."
      },
      {
        "id": "B",
        "text": "Create a new S3 bucket for processed data. Set up S3 replication from the central S3 bucket to the new S3 bucket. Use S3 Object Lambda to query the objects based on transaction date."
      },
      {
        "id": "C",
        "text": "Create a new S3 bucket for processed data. Use AWS Glue for Apache Spark to create a job to query the CSV objects based on transaction date. Configure the job to store the results in the new S3 bucket. Query the objects from the new S3 bucket."
      },
      {
        "id": "D",
        "text": "Create a new S3 bucket for processed data. Use Amazon Data Firehose to transfer the data from the central S3 bucket to the new S3 bucket. Configure Firehose to run an AWS Lambda function to query the data based on transaction date."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Amazon Athena CTAS): . Serverless & SQL: Amazon Athena l dch v truy vn tng tc khng my ch (Serverless), cho php dng chun SQL  truy vn trc tip d liu nm trn S3 m khng cn thit lp h tng (Least operational overhead). . Ti u ha vi CTAS: Cu lnh CREATE TABLE AS SELECT (CTAS) trong Athena khng ch to bng m cn c th dng  chuyn i nh dng d liu (v d: t CSV sang Parquet) v phn vng (partition) d liu theo ct transaction_date. Vic ny gip cc truy vn sau ny ch qut ng phn vng ngy thng cn thit, tng tc  v gim chi ph ng k so vi vic qut ton b file CSV th. Ti sao khng chn B (S3 Object Lambda): S3 Object Lambda c thit k  sa i ni dung ca mt i tng ring l trong khi n ang c truy xut (v d: lm m d liu nhy cm khi ngi dng ti file v). N khng phi l mt cng c truy vn phn tch (Analytical Query Engine)  lc v tng hp d liu t hng ngn file da trn mt ct c th. Ti sao khng chn C (AWS Glue for Apache Spark): Mc d Glue c th lm c vic ny, nhng Glue l mt dch v ETL (Trch xut, Chuyn i, Ti) yu cu vit code (Python/Scala) hoc thit lp Job phc tp hn. So vi vic ch vit mt cu lnh SQL trong Athena, Glue c \"operational overhead\" (gnh nng vn hnh) cao hn. Ti sao khng chn D (Amazon Data Firehose): Amazon Data Firehose l dch v Ingestion (np d liu) dng  nhn lung d liu v lu vo S3/Redshift. N khng h tr vic c d liu  tn ti (existing objects) trong S3 bucket lm ngun (Source). Do , gii php ny khng kh thi v mt k thut cho d liu c."
  },
  {
    "id": 85,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a large, unstructured dataset. The dataset includes many duplicate records across several key attributes. Which solution on AWS will detect duplicates in the dataset with the LEAST code development?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Mechanical Turk jobs to detect duplicates."
      },
      {
        "id": "B",
        "text": "Use Amazon QuickSight ML Insights to build a custom deduplication model."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Data Wrangler to pre-process and detect duplicates."
      },
      {
        "id": "D",
        "text": "Use the AWS Glue FindMatches transform to detect duplicates. Cho bn, y l phn tch chi tit cho cu hi v pht hin d liu trng lp (Deduplication/Entity Resolution) trn AWS vi n lc lp trnh t nht."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (AWS Glue FindMatches): . Tnh nng chuyn bit (Purpose-built): FindMatches l mt tnh nng Machine Learning chuyn i (ML Transform) tch hp sn trong AWS Glue. N c thit k c bit  gii quyt bi ton \"Record Linkage\" hoc \"Entity Resolution\" (lin kt bn ghi), cho php tm cc bn ghi trng lp ngay c khi chng khng khp chnh xc tng k t (fuzzy matching - v d: \"Jon Smith\" v \"John Smith\"). . Least code development (t code nht): Bn khng cn vit m thut ton ML. Thay vo , bn dy h thng bng cch gn nhn th cng mt tp nh cc cp bn ghi l \"trng\" hoc \"khng trng\". Glue s t ng hc v p dng logic  cho ton b tp d liu ln. y l gii php \"Zero-code\" v mt thut ton. Ti sao khng chn A (Amazon Mechanical Turk): Mechanical Turk l dch v s dng sc ngi (crowdsourcing)  thc hin cc tc v. Mc d con ngi rt gii pht hin trng lp trong d liu phi cu trc, nhng gii php ny khng th m rng (scale) cho \"large dataset\" mt cch hiu qu v thi gian v chi ph. N cng i hi quy trnh qun l (operational overhead) phc tp hn nhiu so vi vic chy mt Glue Job t ng. Ti sao khng chn B (Amazon QuickSight ML Insights): QuickSight l cng c Business Intelligence (BI) dng  trc quan ha d liu. Tnh nng ML Insights ca n h tr pht hin bt thng (Anomaly Detection) hoc d bo (Forecasting) trn biu , KHNG c kh nng xy dng m hnh chng trng lp d liu (Deduplication)  lm sch d liu ngun. Ti sao khng chn C (Amazon SageMaker Data Wrangler): SageMaker Data Wrangler c tnh nng \"Drop duplicates\", nhng tnh nng ny thng hot ng da trn nguyn tc khp chnh xc (exact match) trn cc ct c chn. i vi cc bi ton yu cu pht hin trng lp phc tp (v d: li chnh t, vit tt khc nhau trong cc thuc tnh chnh), Glue FindMatches (vi kh nng Fuzzy Matching) l gii php mnh m v chuyn dng hn. Hn na, Glue x l tt hn  quy m..."
  },
  {
    "id": 86,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to run a batch data-processing job on Amazon EC2 instances. The job will run during the weekend and will take 90 minutes to finish running. The processing can handle interruptions. The company will run the job every weekend for the next 6 months. Which EC2 instance purchasing option will meet these requirements MOST cost- effectively?",
    "options": [
      {
        "id": "A",
        "text": "Spot Instances"
      },
      {
        "id": "B",
        "text": "Reserved Instances"
      },
      {
        "id": "C",
        "text": "On-Demand Instances"
      },
      {
        "id": "D",
        "text": "Dedicated Instances"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Spot Instances): y l la chn ti u nht v chi ph (c th r hn ti 90% so vi gi On-Demand). Cha kha ca cu hi nm  cm t \"The processing can handle interruptions\" (Quy trnh c th x l vic b gin on). Spot Instances l phn dung lng d tha ca AWS, gi rt r nhng c th b AWS thu hi (interruption) bt c lc no vi cnh bo trc 2 pht. V ng dng chu c li ny, y l la chn hon ho. Ti sao khng chn B (Reserved Instances): Reserved Instances (RI) yu cu cam kt s dng lu di (1 hoc 3 nm) v thng dnh cho cc tc v chy lin tc 24/7 (steady-state usage). Vic mua RI cho mt job ch chy 90 pht mi tun l cc k lng ph v bn phi tr tin cho c thi gian khng s dng. Ti sao khng chn C (On-Demand Instances): On-Demand Instances l la chn tiu chun, tr tin theo gi/giy s dng v khng b gin on. Tuy nhin, mc gi ca n cao hn nhiu so vi Spot Instances. V  bi yu cu \"Most cost-effectively\" v job chu c gin on, nn vic tr gi cao cho s n nh ca On-Demand l khng cn thit. Ti sao khng chn D (Dedicated Instances): Dedicated Instances chy trn phn cng vt l dnh ring cho mt khch hng (thng dng cho yu cu tun th/compliance). y l phng n t  nht, hon ton tri ngc vi mc tiu tit kim chi ph."
  },
  {
    "id": 87,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer has an Amazon Comprehend custom model in Account A in the us-east-1 Region. The ML engineer needs to copy the model to Account  in the same Region. Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon S3 to make a copy of the model. Transfer the copy to Account B."
      },
      {
        "id": "B",
        "text": "Create a resource-based IAM policy. Use the Amazon Comprehend ImportModel API operation to copy the model to Account B."
      },
      {
        "id": "C",
        "text": "Use AWS DataSync to replicate the model from Account A to Account B."
      },
      {
        "id": "D",
        "text": "Create an AWS Site-to-Site VPN connection between Account A and Account  to transfer the model."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (ImportModel API): Amazon Comprehend cung cp tnh nng native (nguyn bn)  sao chp hoc chia s Custom Model gia cc ti khon AWS thng qua API ImportModel. Quy trnh chun bao gm: . Ti Account A (Source): Cu hnh Resource-based IAM policy trn Model (hoc KMS key nu model c m ha)  cho php Account B truy cp. . Ti Account B (Target): Gi lnh ImportModel v tr ti ARN ca model gc  Account A. y l gii php chnh thng, t tn cng sc lp trnh nht v AWS x l ton b quy trnh sao chp  backend. Ti sao khng chn A (Amazon S3 copy): Mt Amazon Comprehend Custom Model khng ch n thun l cc file artifacts nm trn S3. N l mt ti nguyn c qun l (managed resource) bao gm metadata, cu hnh hun luyn v endpoint. Vic ch sao chp file vt l t S3 bucket ca Account A sang Account B s khng to ra mt i tng \"Model\" hot ng c trong giao din iu khin (Console) hay API ca Account B. Bn vn s cn dng API  ng k n, do  cch ny th cng v rm r hn. Ti sao khng chn C (AWS DataSync): AWS DataSync l dch v chuyn dng  di chuyn lng d liu ln gia cc h thng lu tr (nh t On-premises NAS ln S3, hoc gia cc EFS). N hot ng  cp  file/object storage, khng hiu ng ngha ca mt \"Machine Learning Model\". N khng th dng  ng k hay import model vo dch v Comprehend. Ti sao khng chn D (AWS Site-to-Site VPN): Site-to-Site VPN c dng  kt ni mng gia trung tm d liu ti ch (On-premises) v AWS VPC. Vic kt ni gia hai ti khon AWS (Account A v Account B) nm trong cng mt Region hon ton khng cn n VPN; chng giao tip qua mng backbone ca AWS hoc VPC Peering/Transit Gateway. VPN l gii php sai hon ton v mt h tng mng cho ng cnh ny."
  },
  {
    "id": 88,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is training a simple neural network model. The ML engineer tracks the performance of the model over time on a validation dataset. The model's performance improves substantially at first and then degrades after a specific number of epochs. Which solutions will mitigate this problem? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Enable early stopping on the model."
      },
      {
        "id": "B",
        "text": "Increase dropout in the layers."
      },
      {
        "id": "C",
        "text": "Increase the number of layers."
      },
      {
        "id": "D",
        "text": "Increase the number of neurons."
      },
      {
        "id": "E",
        "text": "Investigate and reduce the sources of model bias."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Early stopping): Hin tng \"hiu sut ci thin ban u ri sau  suy gim trn tp validation\" l nh ngha sch gio khoa ca Overfitting (Qu khp). Khi model bt u hc thuc lng cc nhiu (noise) ca tp train thay v hc quy lut chung, li trn tp validation s tng ln. Early Stopping l k thut gim st ch s trn tp validation v t ng dng qu trnh hun luyn ngay khi hiu sut bt u c du hiu suy gim, ngn chn vic model b Overfitting. Ti sao chn B (Increase dropout): Dropout l mt k thut iu chun (Regularization) mnh m cho mng n-ron. Bng cch ngu nhin \"tt\" mt s n- ron trong qu trnh hun luyn, Dropout buc mng phi hc cc c trng mnh m hn (robust features) v khng ph thuc qu nhiu vo bt k n-ron n l no. Vic tng t l Dropout gip gim  phc tp thc t ca m hnh, t  gim thiu Overfitting. Ti sao khng chn C (Increase the number of layers): Tng s lng lp (lm model su hn) ng ngha vi vic tng  phc tp v kh nng ghi nh (capacity) ca m hnh. Mt m hnh cng phc tp th cng d b Overfitting. Hnh ng ny s lm tnh trng ti t hn. Ti sao khng chn D (Increase the number of neurons): Tng t nh p n C, vic tng s lng n-ron (lm model rng hn) cng lm tng kh nng ghi nh ca m hnh, dn n nguy c Overfitting cao hn. Ti sao khng chn E (Reduce model bias): Khi nim \"Bias\" cao thng lin quan n Underfitting (m hnh qu n gin, khng hc c g). Trong tnh hung ny, m hnh  hc c (hiu sut ci thin ban u) nhng sau  hc qu k dn n sai trn tp mi (High Variance). Do , vn  l Variance, khng phi Bias."
  },
  {
    "id": 89,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a Retrieval Augmented Generation (RAG) application that uses a vector database to store embeddings of documents. The company must migrate the application to AWS and must implement a solution that provides semantic search of text files. The company has already migrated the text repository to an Amazon S3 bucket. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use an AWS Batch job to process the files and generate embeddings. Use AWS Glue to store the embeddings. Use SQL queries to perform the semantic searches."
      },
      {
        "id": "B",
        "text": "Use a custom Amazon SageMaker notebook to run a custom script to generate embeddings. Use SageMaker Feature Store to store the embeddings. Use SQL queries to perform the semantic searches."
      },
      {
        "id": "C",
        "text": "Use the Amazon Kendra S3 connector to ingest the documents from the S3 bucket into Amazon Kendra. Query Amazon Kendra to perform the semantic searches."
      },
      {
        "id": "D",
        "text": "Use an Amazon Textract asynchronous job to ingest the documents from the S3 bucket. Query Amazon Textract to perform the semantic searches."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Amazon Kendra): . Dch v chuyn bit (Purpose-built): Amazon Kendra l dch v tm kim thng minh (Intelligent Search Service) c qun l hon ton bi AWS. Tnh nng ct li ca n l Semantic Search (Tm kim theo ng ngha) s dng Machine Learning  hiu  nh ngi dng thay v ch khp t kha (keyword matching). . Tch hp sn RAG: Kendra thng xuyn c s dng lm b truy xut (Retriever) trong kin trc RAG (Retrieval Augmented Generation) v kh nng tr v cc on vn bn chnh xc cao. . D dng tch hp S3: Kendra cung cp sn S3 Connector  t ng thu thp (crawl) v nh ch mc (index) cc ti liu vn bn t S3 m khng cn vit code x l phc tp. Ti sao khng chn A (AWS Glue & SQL): AWS Glue Data Catalog l mt kho cha metadata, v vic chy truy vn SQL tiu chun trn Glue (thng qua Athena) ch h tr khp chnh xc hoc khp mu (LIKE operator). SQL truyn thng khng h tr Semantic Search (so snh  tng ng gia cc vector embeddings).  lm c iu ny vi SQL, bn cn cc c s d liu h tr vector extension (nh PostgreSQL vi pgvector), iu m Glue khng cung cp. Ti sao khng chn B (SageMaker Feature Store & SQL): SageMaker Feature Store c thit k  lu tr v phc v cc tnh nng (features) cho m hnh ML vi  tr thp (cho inference) hoc dng li cho training. Mc d n c h tr lu vector, nhng n khng phi l mt cng c tm kim (Search Engine). Bn khng th thc hin truy vn tm kim ng ngha linh hot trn Feature Store bng cc cu lnh SQL n thun. Ti sao khng chn D (Amazon Textract): Amazon Textract l dch v OCR (Optical Character Recognition) dng  trch xut vn bn t hnh nh hoc file PDF scan. N l cng c \"tin x l\" (preprocessing). Textract khng c kh nng lu tr ch mc hay cung cp API  thc hin tm kim ng ngha trn tp d liu  trch xut."
  },
  {
    "id": 90,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses Amazon Athena to query a dataset in Amazon S3. The dataset has a target variable that the company wants to predict. The company needs to use the dataset in a solution to determine if a model can predict the target variable. Which solution will provide this information with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create a new model by using Amazon SageMaker Autopilot. Report the model's achieved performance."
      },
      {
        "id": "B",
        "text": "Implement custom scripts to perform data pre-processing, multiple linear regression, and performance evaluation. Run the scripts on Amazon EC2 instances."
      },
      {
        "id": "C",
        "text": "Configure Amazon Macie to analyze the dataset and to create a model. Report the model's achieved performance."
      },
      {
        "id": "D",
        "text": "Select a model from Amazon Bedrock. Tune the model with the data. Report the model's achieved performance."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Amazon SageMaker Autopilot): SageMaker Autopilot l gii php AutoML (T ng ha Machine Learning) ca AWS. N t ng thc hin mi bc t khm ph d liu (Data Exploration), tin x l (Preprocessing), chn thut ton, n tinh chnh siu tham s (Hyperparameter Tuning).  tr li cu hi \"Liu m hnh c th d on bin mc tiu khng?\", bn ch cn tr Autopilot vo d liu S3 v chn ct mc tiu. Autopilot s chy hng lot th nghim v tr v bng xp hng (Leaderboard) km cc ch s hiu sut (Accuracy, MSE, F1...). y l cch nhanh nht, tn t cng sc nht (Low-code/No-code)  nh gi tnh kh thi. Ti sao khng chn B (Custom scripts on EC2): Vic t vit script (code tay)  tin x l, chy hi quy v nh gi, cng vi vic phi qun l h tng my ch EC2, i hi n lc pht trin rt ln (High development effort). y l cch lm th cng tri ngc vi yu cu \"LEAST development effort\". Ti sao khng chn C (Amazon Macie): Amazon Macie l dch v bo mt dng  pht hin d liu nhy cm (PII) v phn loi d liu trong S3. N hon ton khng c chc nng xy dng m hnh Machine Learning  d on bin mc tiu (Classification/Regression). Ti sao khng chn D (Amazon Bedrock): Amazon Bedrock cung cp cc m hnh nn tng (Foundation Models) cho Generative AI (AI to sinh). Mc d c th dng LLM cho mt s tc v phn loi, nhng vic \"Tune\" (tinh chnh) mt m hnh ngn ng ln cho mt bi ton d liu dng bng (tabular data) n gin l \"dng dao m tru git g\", tn km v phc tp hn nhiu so vi vic dng AutoML chuyn dng nh Autopilot."
  },
  {
    "id": 91,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses Amazon Athena to query a dataset in Amazon S3. The dataset has a target variable that the company wants to predict. The company needs to use the dataset in a solution to determine if a model can predict the target variable. Which solution will provide this information with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create a new model by using Amazon SageMaker Autopilot. Report the model's achieved performance."
      },
      {
        "id": "B",
        "text": "Implement custom scripts to perform data pre-processing, multiple linear regression, and performance evaluation. Run the scripts on Amazon EC2 instances."
      },
      {
        "id": "C",
        "text": "Configure Amazon Macie to analyze the dataset and to create a model. Report the model's achieved performance."
      },
      {
        "id": "D",
        "text": "Select a model from Amazon Bedrock. Tune the model with the data. Report the model's achieved performance."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Amazon SageMaker Autopilot): SageMaker Autopilot l gii php AutoML (T ng ha Machine Learning) ca AWS. N t ng thc hin mi bc t khm ph d liu (Data Exploration), tin x l (Preprocessing), chn thut ton, n tinh chnh siu tham s (Hyperparameter Tuning).  tr li cu hi \"Liu m hnh c th d on bin mc tiu khng?\", bn ch cn tr Autopilot vo d liu S3 v chn ct mc tiu. Autopilot s chy hng lot th nghim v tr v bng xp hng (Leaderboard) km cc ch s hiu sut (Accuracy, MSE, F1...). y l cch nhanh nht, tn t cng sc nht (Low-code/No-code)  nh gi tnh kh thi. Ti sao khng chn B (Custom scripts on EC2): Vic t vit script (code tay)  tin x l, chy hi quy v nh gi, cng vi vic phi qun l h tng my ch EC2, i hi n lc pht trin rt ln (High development effort). y l cch lm th cng tri ngc vi yu cu \"LEAST development effort\". Ti sao khng chn C (Amazon Macie): Amazon Macie l dch v bo mt dng  pht hin d liu nhy cm (PII) v phn loi d liu trong S3. N hon ton khng c chc nng xy dng m hnh Machine Learning  d on bin mc tiu (Classification/Regression). Ti sao khng chn D (Amazon Bedrock): Amazon Bedrock cung cp cc m hnh nn tng (Foundation Models) cho Generative AI (AI to sinh). Mc d c th dng LLM cho mt s tc v phn loi, nhng vic \"Tune\" (tinh chnh) mt m hnh ngn ng ln cho mt bi ton d liu dng bng (tabular data) n gin l \"dng dao m tru git g\", tn km v phc tp hn nhiu so vi vic dng AutoML chuyn dng nh Autopilot."
  },
  {
    "id": 92,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to predict the success of advertising campaigns by considering the color scheme of each advertisement. An ML engineer is preparing data for a neural network model. The dataset includes color information as categorical data. Which technique for feature engineering should the ML engineer use for the model?",
    "options": [
      {
        "id": "A",
        "text": "Apply label encoding to the color categories. Automatically assign each color a unique integer."
      },
      {
        "id": "B",
        "text": "Implement padding to ensure that all color feature vectors have the same length."
      },
      {
        "id": "C",
        "text": "Perform dimensionality reduction on the color categories."
      },
      {
        "id": "D",
        "text": "One-hot encode the color categories to transform the color scheme feature into a binary matrix."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (One-hot encode): \"Color scheme\" (bng mu) l d liu danh mc nh danh (Nominal Categorical Data), ngha l cc mu sc nh , Xanh, Vng ngang hng nhau, khng c th t hn km ( khng \"ln hn\" hay \"nh hn\" Xanh). One-hot encoding bin mi gi tr mu thnh mt vector nh phn ring bit (v d:  = [1, 0, 0], Xanh = [0, 1, 0]). iu ny gip Mng N-ron (Neural Network) x l tng mu sc nh mt tn hiu c lp m khng v tnh hc sai cc mi quan h s hc (nh 1 < 2 < 3) khng tn ti. Ti sao khng chn A (Label encoding): Label encoding gn mi mu mt s nguyn (v d: =1, Xanh=2, Vng=3). Mng N-ron s hiu nhm rng Vng (3) c gi tr \"ln hn\"  (1) hoc trung bnh cng ca  v Vng l Xanh. iu ny sai hon ton v mt bn cht d liu mu sc v s lm m hnh hc sai lch. Label encoding ch nn dng cho d liu danh mc c th t (Ordinal Data - v d: Km, Trung bnh, Gii). Ti sao khng chn B (Implement padding): Padding l k thut thm gi tr (thng l 0)  lm ng u  di ca cc chui d liu (sequence length), thng dng trong x l ngn ng t nhin (NLP) hoc chui thi gian. N khng dng  m ha mt bin danh mc n l nh mu sc. Ti sao khng chn C (Dimensionality reduction): Dimensionality reduction (nh PCA) dng  gim s lng chiu d liu khi chng qu ln hoc b d tha. Trong khi , bin mu sc ang  dng th (text/category), cn c m ha thnh s trc khi c th p dng bt k thut ton ton hc no. Hn na, One- hot encoding thc t lm tng s chiu d liu, ngc li vi gim chiu."
  },
  {
    "id": 93,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses a hybrid cloud environment. A model that is deployed on premises uses data in Amazon 53 to provide customers with a live conversational engine. The model is using sensitive data. An ML engineer needs to implement a solution to identify and remove the sensitive data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the model on Amazon SageMaker. Create a set of AWS Lambda functions to identify and remove the sensitive data."
      },
      {
        "id": "B",
        "text": "Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. Create an AWS Batch job to identify and remove the sensitive data."
      },
      {
        "id": "C",
        "text": "Use Amazon Macie to identify the sensitive data. Create a set of AWS Lambda functions to remove the sensitive data."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend to identify the sensitive data. Launch Amazon EC2 instances to remove the sensitive data."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Amazon Macie + AWS Lambda): o Ph hp vi mc ch (Purpose-built): Amazon Macie l dch v bo mt c qun l hon ton (fully managed), s dng Machine Learning v pattern matching chuyn bit  khm ph v bo v d liu nhy cm (PII) lu tr trong Amazon S3.  bi nu r d liu nm trong S3, y l trng hp s dng chnh (primary use case) ca Macie. o Ti u vn hnh (Least Operational Overhead): Macie t ng ha vic qut (scan) m khng cn ngi dng xy dng m hnh hay logic pht hin. Kt hp vi AWS Lambda (serverless)  thc thi hnh ng \"remove\" (xa hoc che/redact) da trn cc pht hin (findings) ca Macie gip loi b gnh nng qun l my ch. y l kin trc Event-driven (Macie $\\rightarrow$ EventBridge $\\rightarrow$ Lambda) chun mc v t tn cng sc qun tr nht. Ti sao khng chn A (Custom identification in Lambda): o Gnh nng pht trin (High Development Overhead): Phng n ny yu cu k s phi t vit code trong Lambda  \"identify\" (nhn din) d liu nhy cm. Vic t xy dng logic pht hin PII (bng Regex hoc custom ML) cc k phc tp, d sai st v tn cng bo tr so vi vic dng dch v c sn nh Macie. Ti sao khng chn B (ECS/Fargate + AWS Batch): o Sai mc ch v chi ph qun l cao: Tng t nh p n A, vic dng AWS Batch yu cu t vit logic pht hin PII. Ngoi ra, AWS Batch thng dng cho cc tc v x l hng lot quy m ln (heavy computing), vic thit lp mi trng Docker, nh ngha Job definition v qun l container orchestration cho mt tc v qun tr d liu ny l phc tp hn nhiu so vi m hnh Serverless ca p n C. Ti sao khng chn D (Comprehend + EC2): o Gnh nng vn hnh h tng (High Operational Overhead): Mc d Amazon Comprehend c kh nng pht hin PII rt tt (c bit cho vn bn/NLP), nhng im cht ngi ca phng n ny l vic s dng EC2. EC2 yu cu bn phi qun l h iu hnh (OS patching), cu hnh mng, auto-scaling v bo mt server. ..."
  },
  {
    "id": 94,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to create data ingestion pipelines and ML model deployment pipelines on AWS. All the raw data is stored in Amazon S3 buckets. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Data Firehose to create the data ingestion pipelines. Use Amazon SageMaker Studio Classic to create the model deployment pipelines."
      },
      {
        "id": "B",
        "text": "Use AWS Glue to create the data ingestion pipelines. Use Amazon SageMaker Studio Classic to create the model deployment pipelines."
      },
      {
        "id": "C",
        "text": "Use Amazon Redshift ML to create the data ingestion pipelines. Use Amazon SageMaker Studio Classic to create the model deployment pipelines."
      },
      {
        "id": "D",
        "text": "Use Amazon Athena to create the data ingestion pipelines. Use an Amazon SageMaker notebook to create the model deployment pipelines."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (AWS Glue + SageMaker Studio Classic): o AWS Glue cho Data Ingestion:  bi cho bit d liu th (raw data) ang nm trong Amazon S3. AWS Glue l dch v ETL (Extract, Transform, Load) serverless c thit k chuyn bit  xy dng cc pipeline x l, lm sch v t chc d liu (ingestion pipelines) t S3  chun b cho Machine Learning. o SageMaker Studio Classic cho Deployment: SageMaker Studio cung cp giao din tch hp  to v qun l SageMaker Pipelines (cho CI/CD ca ML). N h tr trc quan ha, to cc \"Model deployment pipelines\" thng qua SageMaker Projects v MLOps template, p ng chnh xc yu cu th hai. Ti sao khng chn A (Amazon Data Firehose): o Sai trng hp s dng (Use Case Mismatch): Amazon Data Firehose l dch v  np d liu streaming (thi gian thc) vo cc ch n (nh S3, Redshift). Trong bi ton ny, d liu  nm sn  dng tnh trong S3 (batch data), vic dng Firehose  x l d liu  lu tr l khng ph hp. Firehose thng l bc trc khi d liu vo S3. Ti sao khng chn C (Amazon Redshift ML): o Gii hn phm vi (Scope Limitation): Amazon Redshift ML cho php chy ML bng SQL trc tip trong kho d liu (Data Warehouse). Tuy nhin, n khng phi l cng c a nng  xy dng \"data ingestion pipelines\" tng qut t S3. N buc d liu phi c load vo Redshift trc, lm gim tnh linh hot so vi Glue (c th x l d liu ngay ti S3 - Data Lake). Ti sao khng chn D (Amazon Athena): o Sai chc nng: Amazon Athena l dch v truy vn tng tc (Interactive Query Service) dng  phn tch d liu trong S3 bng SQL, khng phi l cng c  xy dng v iu phi cc \"pipeline np d liu\" t ng v phc tp (orchestration). Athena thng l cng c tiu th d liu sau khi Glue  x l."
  },
  {
    "id": 95,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company that has hundreds of data scientists is using Amazon SageMaker to create ML models. The models are in model groups in the SageMaker Model Registry. The data scientists are grouped into three categories: computer vision, natural language processing (NLP), and speech recognition. An ML engineer needs to implement a solution to organize the existing models into these groups to improve model discoverability at scale. The solution must not affect the integrity of the model artifacts and their existing groupings. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a custom tag for each of the three categories. Add the tags to the model packages in the SageMaker Model Registry."
      },
      {
        "id": "B",
        "text": "Create a model group for each category. Move the existing models into these category model groups."
      },
      {
        "id": "C",
        "text": "Use SageMaker ML Lineage Tracking to automatically identify and tag which model groups should contain the models."
      },
      {
        "id": "D",
        "text": "Create a Model Registry collection for each of the three categories. Move the existing model groups into the collections."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Model Registry Collections): o ng tnh nng (Feature Fit): SageMaker Model Registry Collections l tnh nng c thit k c bit  nhm cc \"Model Groups\" c lin quan li vi nhau (theo phn cp). iu ny gii quyt bi ton t chc hng trm models thnh cc danh mc ln (CV, NLP, Speech)  d dng tm kim (discoverability). o Tun th rng buc (Constraints Compliance):  bi yu cu \"khng lm nh hng n tnh ton vn v cch nhm hin ti\" (not affect the integrity... and their existing groupings). Khi bn thm mt Model Group vo mt Collection, bn thn Model Group  khng b thay i, khng b di chuyn vt l, v cc artifact trong S3/ECR vn gi nguyn. N ch to ra mt lp tham chiu logic bn trn. Ti sao khng chn A (Custom Tags): o Kh nng m rng km hn (Scalability & Hierarchy): Mc d Tags c th dng  lc (filter), nhng chng ch l metadata phng (flat metadata). Khi  bi nhc n vic \"organize... into groups\" cho hng trm data scientists vi cc danh mc r rng, vic s dng cu trc phn cp (Hierarchy) ca Collections s qun l tt hn v chuyn nghip hn so vi vic ch da vo tag tm kim. Tag khng to ra mt \"view\" t chc r rng nh Collection. Ti sao khng chn B (Create new groups and move): o Vi phm rng buc (Violates Constraints):  bi yu cu \"not affect... their existing groupings\". Phng n ny  xut di chuyn (move) cc model t nhm c sang nhm mi. Hnh ng ny s ph v cu trc t chc hin c, lm thay i ARN v lch s phin bn gn lin vi Model Group c. Ti sao khng chn C (ML Lineage Tracking): o Sai mc ch (Wrong Purpose): SageMaker ML Lineage Tracking dng  truy vt ngun gc (d liu no to ra model no, pipeline no  chy). N l cng c  kim ton (audit) v ti lp quy trnh, khng phi l cng c  t chc hay sp xp th mc cho cc Model Groups."
  },
  {
    "id": 96,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company runs an Amazon SageMaker domain in a public subnet of a newly created VPC. The network is configured properly, and ML engineers can access the SageMaker domain. Recently, the company discovered suspicious traffic to the domain from a specific IP address. The company needs to block traffic from the specific IP address. Which update to the network configuration will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Create a security group inbound rule to deny traffic from the specific IP address. Assign the security group to the domain."
      },
      {
        "id": "B",
        "text": "Create a network ACL inbound rule to deny traffic from the specific IP address. Assign the rule to the default network Ad for the subnet where the domain is located."
      },
      {
        "id": "C",
        "text": "Create a shadow variant for the domain. Configure SageMaker Inference Recommender to send traffic from the specific IP address to the shadow endpoint."
      },
      {
        "id": "D",
        "text": "Create a VPC route table to deny inbound traffic from the specific IP address. Assign the route table to the domain."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Network ACL): o Tnh nng DENY: Network Access Control List (Network ACL) l lp bo mt hot ng  cp  Subnet v l tng la \"stateless\". im quan trng nht l NACL h tr c quy tc cho php (ALLOW) v t chi (DENY).  chn mt IP c th, bn ch cn to mt rule DENY cho IP  v t s th t u tin (Rule Number) thp hn rule ALLOW mc nh. y l c ch chun  \"blacklist\" cc IP c hi. Ti sao khng chn A (Security Group): o Ch c ALLOW (Allow-only): Security Groups hot ng  cp  Instance/Interface v l \"stateful\". Tuy nhin, Security Group ch h tr cc quy tc cho php (ALLOW rules). Bn khng th to quy tc DENY trong Security Group. Mc nh n chn tt c (deny all), nhng bn khng th thm mt rule  chn c th mt IP trong khi vn cho php cc IP khc. Ti sao khng chn C (Shadow Variant): o Sai min kin thc (Wrong Domain): Shadow Variant l mt tnh nng ca SageMaker Inference dng  th nghim m hnh mi (shadow testing) bng cch gi bn sao lu lng thc t n m hnh mi  nh gi hiu nng m khng nh hng n phn hi tr v cho ngi dng. N hon ton khng lin quan n bo mt mng hay chn IP. Ti sao khng chn D (VPC Route Table): o Sai chc nng: Route Table dng  nh tuyn (routing) lu lng mng (v d: i ra Internet Gateway hay i qua NAT Gateway). N khng c chc nng lc gi tin (filtering) hay chn traffic da trn IP ngun."
  },
  {
    "id": 97,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is gathering audio, video, and text data in various languages. The company needs to use a large language model (LLM) to summarize the gathered data that is in Spanish. Which solution will meet these requirements in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Train and deploy a model in Amazon SageMaker to convert the data into English text. Train and deploy an LLM in SageMaker to summarize the text."
      },
      {
        "id": "B",
        "text": "Use Amazon Transcribe and Amazon Translate to convert the data into English text. Use Amazon Bedrock with the Jurassic model to summarize the text."
      },
      {
        "id": "C",
        "text": "Use Amazon Rekognition and Amazon Translate to convert the data into English text. Use Amazon Bedrock with the Anthropic Claude model to summarize the text."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend and Amazon Translate to convert the data into English text. Use Amazon Bedrock with the Stable Diffusion model to summarize the text."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Transcribe + Translate + Bedrock/Jurassic): o X l Audio/Video (Speech-to-Text): Amazon Transcribe l dch v chuyn dng  chuyn i ging ni thnh vn bn t c tp m thanh (Audio) v video (ly track m thanh). y l bc bt buc  s ha d liu phi cu trc ny. o Tm tt vn bn (Summarization): Amazon Bedrock cung cp quyn truy cp vo cc m hnh nn tng (FM). M hnh Jurassic (ca AI21 Labs) ni ting vi kh nng x l ngn ng t nhin (NLP) mnh m, c bit l nhim v c hiu v tm tt vn bn phc tp. o Ti u thi gian (Least Time): Vic s dng cc dch v AI \"serverless\" c sn (Managed AI Services) nh Transcribe, Translate v Bedrock gip loi b hon ton thi gian train m hnh, ch cn gi API  tch hp. Ti sao khng chn A (Train custom models in SageMaker): o Tn thi gian v cng sc (High Operational Overhead):  bi yu cu gii php tn t thi gian nht. Vic t thu thp d liu, gn nhn, hun luyn (train), tinh chnh (fine-tune) v trin khai (deploy) cc m hnh ring bit cho speech-to-text v summarization trn SageMaker s tn hng tun hoc hng thng so vi vic dng API c sn  p n B. Ti sao khng chn C (Rekognition + Translate + Claude): o Sai cng c x l u vo: Amazon Rekognition l dch v phn tch hnh nh/video (Computer Vision)  nhn din khun mt, vt th, hoc vn bn trong khung hnh (OCR). N khng c chc nng chuyn i ging ni (speech) trong video/audio thnh vn bn. Do , phn ni dung li ni s b mt. Ti sao khng chn D (Comprehend + Stable Diffusion): o Sai hon ton v m hnh GenAI: Stable Diffusion l m hnh dng  to sinh hnh nh (Text-to-Image), khng phi l m hnh ngn ng (LLM)  tm tt vn bn. o Sai cng c u vo: Amazon Comprehend dng  phn tch vn bn (sentiment, entities...), khng th dng  chuyn i file Audio/Video sang Text."
  },
  {
    "id": 98,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A financial company receives a high volume of real-time market data streams from an external provider. The streams consist of thousands of JSON records every second. The company needs to implement a scalable solution on AWS to identify anomalous data points. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Ingest real-time data into Amazon Kinesis data streams. Use the built-in RANDOM_CUT_FOREST function in Amazon Managed Service for Apache Flink to process the data streams and to detect data anomalies."
      },
      {
        "id": "B",
        "text": "Ingest real-time data into Amazon Kinesis data streams. Deploy an Amazon SageMaker endpoint for real-time outlier detection. Create an AWS Lambda function to detect anomalies. Use the data streams to invoke the Lambda function."
      },
      {
        "id": "C",
        "text": "Ingest real-time data into Apache Kafka on Amazon EC2 instances. Deploy an Amazon SageMaker endpoint for real-time outlier detection. Create an AWS Lambda function to detect anomalies. Use the data streams to invoke the Lambda function."
      },
      {
        "id": "D",
        "text": "Send real-time data to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create an AWS Lambda function to consume the queue messages. Program the Lambda function to start an AWS Glue extract, transform, and load (ETL) job for batch processing and anomaly detection."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Kinesis Data Streams + Managed Service for Apache Flink): o X l lung tc  cao (High Volume Ingestion): Amazon Kinesis Data Streams l dch v serverless tiu chun  nhn hng nghn bn ghi/giy vi  tr thp. o Thut ton chuyn dng (Built-in Algorithm): RANDOM_CUT_FOREST (RCF) l thut ton SQL tch hp sn trong dch v phn tch lung ca AWS (trc y l Kinesis Data Analytics for SQL, nay c hp nht vo Managed Service for Apache Flink/SQL). N c thit k c bit  pht hin im bt thng (anomalies) trong dng d liu m khng cn hun luyn m hnh trc (unsupervised). o Ti u vn hnh (Least Operational Overhead): y l gii php hon ton c qun l (Fully Managed). Bn khng cn qun l h tng my ch, khng cn quy trnh train/deploy model phc tp nh SageMaker. Ch cn vit truy vn SQL/Flink  gi hm RCF. Ti sao khng chn B (SageMaker Endpoint + Lambda): o Gnh nng tch hp (High Integration Overhead): Vic s dng Lambda  gi SageMaker Endpoint cho hng nghn bn ghi mi giy l khng hiu qu v mt kin trc. N to ra  tr (latency) mng khng cn thit v chi ph cao do s lng invocations qu ln. Ngoi ra, bn phi qun l vng i ca model trong SageMaker (train, deploy, monitor). Ti sao khng chn C (Kafka on EC2): o Gnh nng qun tr h tng (Extreme Operational Overhead): Cm t \"Apache Kafka on Amazon EC2\" l du hiu nhn bit ca gnh nng vn hnh ln nht. Bn phi t ci t, cu hnh, v li, v m rng cm Kafka (Brokers, Zookeeper) thay v dng dch v serverless nh Kinesis. Ti sao khng chn D (SQS + Glue): o Sai m hnh x l (Batch vs Streaming): AWS Glue l dch v ETL hng ti x l theo l (Batch processing) vi thi gian khi ng (cold start) tnh bng pht. N khng th p ng yu cu x l d liu th trng theo thi gian thc (real-time) tng giy."
  },
  {
    "id": 99,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a large collection of chat recordings from customer interactions after a product release. An ML engineer needs to create an ML model to analyze the chat data. The ML engineer needs to determine the success of the product by reviewing customer sentiments about the product. Which action should the ML engineer take to complete the evaluation in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Rekognition to analyze sentiments of the chat conversations."
      },
      {
        "id": "B",
        "text": "Train a Naive Bayes classifier to analyze sentiments of the chat conversations."
      },
      {
        "id": "C",
        "text": "Use Amazon Comprehend to analyze sentiments of the chat conversations."
      },
      {
        "id": "D",
        "text": "Use random forests to classify sentiments of the chat conversations."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Amazon Comprehend): o Dch v c sn (Managed Service): Amazon Comprehend l dch v NLP (Natural Language Processing) c AWS qun l hon ton v  c hun luyn trc (pre-trained). Tnh nng ct li ca n l Sentiment Analysis (phn tch cm xc: Tch cc, Tiu cc, Trung tnh, Hn hp). o Ti u thi gian (Least amount of time): V m hnh  c AWS xy dng sn, k s ML ch cn gi API  nhn kt qu ngay lp tc m khng cn tn thi gian thu thp d liu, gn nhn, hun luyn hay tinh chnh m hnh. y l gii php nhanh nht (Turn-key solution). Ti sao khng chn A (Amazon Rekognition): o Sai min d liu (Wrong Domain): Amazon Rekognition l dch v th gic my tnh (Computer Vision) dng  phn tch hnh nh v video (nhn din khun mt, vt th). N khng c kh nng c hiu hay phn tch cm xc t vn bn (chat logs). Ti sao khng chn B (Naive Bayes classifier): o Tn thi gian pht trin (High Development Time): Naive Bayes l mt thut ton ML c in.  s dng, bn phi t chun b dataset, lm sch d liu (cleaning), vector ha (tokenization/vectorization), hun luyn (training) v nh gi m hnh. Quy trnh ny tn nhiu thi gian hn rt nhiu so vi vic gi mt API c sn nh Comprehend. Ti sao khng chn D (Random forests): o Tn thi gian pht trin: Tng t nh p n B, Random Forest l mt thut ton hc my cn c hun luyn th cng (Custom Model). Vic xy dng pipeline hun luyn t u vi phm yu cu \"LEAST amount of time\" ca  bi."
  },
  {
    "id": 100,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has a conversational AI assistant that sends requests through Amazon Bedrock to an Anthropic Claude large language model (LLM). Users report that when they ask similar questions multiple times, they sometimes receive different answers. An ML engineer needs to improve the responses to be more consistent and less random. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Increase the temperature parameter and the top_k parameter."
      },
      {
        "id": "B",
        "text": "Increase the temperature parameter. Decrease the top_k parameter."
      },
      {
        "id": "C",
        "text": "Decrease the temperature parameter. Increase the top_k parameter."
      },
      {
        "id": "D",
        "text": "Decrease the temperature parameter and the top_k parameter. y l phn tch chi tit cho cu hi v cch tinh chnh tham s suy lun (Inference Parameters) ca m hnh ngn ng ln (LLM) trn Amazon Bedrock."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Gim Temperature v Gim Top_k): o Gim Temperature (Temperature $\\to$ 0): Tham s Temperature kim sot  \"sng to\" hay tnh ngu nhin ca m hnh. Gi tr cng thp (gn 0), m hnh cng t tin chn t c xc sut cao nht tip theo. iu ny lm cho cu tr li tr nn nh hng (deterministic), nht qun v t ngu nhin hn. o Gim Top_k: Tham s Top_k gii hn s lng cc token tim nng c xem xt cho bc tip theo. Khi gim Top_k xung thp (v d: k=1), m hnh b p buc ch chn trong s rt t cc t c kh nng cao nht, loi b cc la chn l hoc t gp. o Kt hp: Vic gim c hai tham s ny l cch mnh nht  p m hnh hot ng nh mt c my chnh xc, lun tr v cng mt kt qu cho cng mt u vo (Consistent & Less Random). Ti sao khng chn A (Increase both): o Tng tnh ngu nhin: Tng Temperature lm phng phn phi xc sut (lm cc t t kh nng cng c c hi c chn). Tng Top_k m rng tp hp cc t c chn. Kt hp li s to ra kt qu cc k a dng, sng to nhng rt ngu nhin v thiu nht qun, hon ton tri ngc vi yu cu  bi. Ti sao khng chn B (Increase Temp, Decrease Top_k): o Xung t mc tiu: Mc d gim Top_k gip hn ch phm vi, nhng vic tng Temperature li khuyn khch s hn lon trong phn phi xc sut. Nhit  cao vn s khin m hnh a ra cc cu tr li khc nhau v o gic (hallucination) nhiu hn l s n nh. Ti sao khng chn C (Decrease Temp, Increase Top_k): o Cha ti u: Gim Temperature l ng hng  tng s n nh. Tuy nhin, vic gi Top_k  mc cao (Increase Top_k) vn m ra mt ca s la chn ln cho m hnh. So vi p n D (ni c hai u b sit cht), p n C vn c kh nng sinh ra s bin thin (variation) cao hn."
  },
  {
    "id": 101,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using ML to predict the presence of a specific weed in a farmer's field. The company is using the Amazon SageMaker linear learner built-in algorithm with a value of multiclass_dassifier for the predictorjype hyperparameter. What should the company do to MINIMIZE false positives?",
    "options": [
      {
        "id": "A",
        "text": "Set the value of the weight decay hyperparameter to zero."
      },
      {
        "id": "B",
        "text": "Increase the number of training epochs."
      },
      {
        "id": "C",
        "text": "Increase the value of the target_precision hyperparameter."
      },
      {
        "id": "D",
        "text": "Change the value of the predictorjype hyperparameter to regressor."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Increase target_precision): o Nguyn l Precision - False Positive: Trong bi ton phn loi, Precision ( chnh xc) c tnh bng cng thc: $Precision = \\frac{TP}{TP + FP}$.  gim thiu False Positives (FP - bo ng gi, tc l bo c c di nhng thc t khng c), ta buc phi tng Precision. o Hyperparameter ca Linear Learner: Thut ton SageMaker Linear Learner h tr hyperparameter target_precision (khi kt hp vi tiu ch la chn m hnh). Bng cch tng gi tr ny, bn ang ra lnh cho thut ton tm kim ngng phn loi (threshold) sao cho xc sut d on ng l cao nht, chp nhn vic c th b st mt s c di (gim Recall) min l khng bo sai. y l cch trc tip nht  gii quyt yu cu \"MINIMIZE false positives\". Ti sao khng chn A (Weight decay to zero): o Sai chc nng: weight_decay (suy gim trng s) l k thut Regularization (chng qu khp/overfitting) bng cch cng thm hnh pht vo hm mt mt  gi cho trng s nh. Vic t n v 0 ch lm tt tnh nng ny, c th khin m hnh b overfit, nhng khng c c ch trc tip no  u tin gim False Positives so vi False Negatives. Ti sao khng chn B (Increase training epochs): o Khng gii quyt vn  ngng: Tng s lng epochs ch gip m hnh hc lu hn  hi t v im cc tr ca hm mt mt (Loss function). N khng thay i mc tiu ti u ha (optimization objective) t cn bng sang u tin Precision. M hnh c th hc tt hn nhng vn gi t l FP/FN mc nh. Ti sao khng chn D (Change to regressor): o Sai loi bi ton: predictor_type='regressor' dng cho bi ton Hi quy (d on gi tr s lin tc, v d: d on gi nh, nhit ). Bi ton  y l d on s hin din ca c di (C/Khng hoc Loi A/B/C), tc l bi ton Phn loi (Classification). Chuyn sang Regressor l sai hon ton v mt k thut."
  },
  {
    "id": 102,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has implemented a data ingestion pipeline for sales transactions from its ecommerce website. The company uses Amazon Data Firehose to ingest data into Amazon OpenSearch Service. The buffer interval of the Firehose stream is set for 60 seconds. An OpenSearch linear model generates real-time sales forecasts based on the data and presents the data in an OpenSearch dashboard. The company needs to optimize the data ingestion pipeline to support sub-second latency for the real-time dashboard. Which change to the architecture will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use zero buffering in the Firehose stream. Tune the batch size that is used in the PutRecordBatch operation."
      },
      {
        "id": "B",
        "text": "Replace the Firehose stream with an AWS DataSync task. Configure the task with enhanced fan-out consumers."
      },
      {
        "id": "C",
        "text": "Increase the buffer interval of the Firehose stream from 60 seconds to 120 seconds."
      },
      {
        "id": "D",
        "text": "Replace the Firehose stream with an Amazon Simple Queue Service (Amazon SQS) queue."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Zero Buffering): o Gim  tr ti a: Theo mc nh, Amazon Data Firehose s gom d liu (buffer) trong mt khong thi gian (v d: 60 giy) hoc dung lng nht nh trc khi gi n ch  ti u hiu sut I/O. Tuy nhin, iu ny to ra  tr. Tnh nng Zero Buffering (t khong thi gian buffer v 0) cho php Firehose gi d liu i ngay lp tc sau khi nhn c, gip gim  tr xung mc thp nht c th (gn thi gian thc), p ng yu cu ca dashboard. o Ti u Throughput: Vic tinh chnh Batch size trong PutRecordBatch gip m bo rng d gi nhanh nhng pipeline vn x l hiu qu lng d liu u vo ln m khng b nghn. Ti sao khng chn B (AWS DataSync): o Sai dch v: AWS DataSync l dch v dng  di chuyn d liu lu tr khi lng ln (bulk data transfer) gia cc h thng lu tr (nh on-premise NAS, EFS sang S3). N khng phi l dch v x l lung d liu (streaming ingestion) cho cc giao dch bn hng thi gian thc tng bn ghi mt. Ti sao khng chn C (Increase buffer interval): o i ngc mc tiu: Tng buffer interval t 60s ln 120s ng ngha vi vic gi d liu li lu hn trong ng dn trc khi gi i. iu ny lm tng  tr gp i, hon ton tri ngc vi yu cu \"sub-second latency\". Ti sao khng chn D (Amazon SQS): o Mt tnh nng tch hp sn: Amazon SQS l hng i thng ip, n khng c kh nng t ng \"ingest\" (np) d liu vo OpenSearch. Nu dng SQS, bn phi t xy dng thm mt lp compute (v d: Lambda hoc EC2 worker)  c t SQS v ghi vo OpenSearch. iu ny lm phc tp ha kin trc v tng gnh nng vn hnh so vi vic ch cn chnh cu hnh Firehose."
  },
  {
    "id": 103,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has trained an ML model in Amazon SageMaker. The company needs to host the model to provide inferences in a production environment. The model must be highly available and must respond with minimum latency. The size of each request will be between 1 KB and 3 MB. The model will receive unpredictable bursts of requests during the day. The inferences must adapt proportionally to the changes in demand. How should the company deploy the model into production to meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a SageMaker real-time inference endpoint. Configure auto scaling. Configure the endpoint to present the existing model."
      },
      {
        "id": "B",
        "text": "Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster. Use ECS scheduled scaling that is based on the CPU of the ECS cluster."
      },
      {
        "id": "C",
        "text": "Install SageMaker Operator on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Deploy the model in Amazon EKS. Set horizontal pod auto scaling to scale replicas based on the memory metric."
      },
      {
        "id": "D",
        "text": "Use Spot Instances with a Spot Fleet behind an Application Load Balancer (ALB) for inferences. Use the ALBRequestCountPerTarget metric as the metric for auto scaling."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Real-time inference endpoint + Auto scaling): o Minimum Latency ( tr ti thiu): SageMaker Real-time inference endpoints c thit k chuyn bit cho cc ng dng yu cu phn hi tc th vi  tr thp (low latency), ph hp trc tip vi yu cu \"respond with minimum latency\". o Unpredictable bursts & Adapt proportionally (X l t bin & Thch ng): Tnh nng Auto Scaling ca SageMaker cho php endpoint t ng tng gim s lng instance (instances count) da trn cc metric (nh InvocationsPerInstance), gip h thng thch ng linh hot vi cc t request tng t bin khng th d on trc (\"unpredictable bursts\"). o Request Size: Kch thc payload t 1 KB n 3 MB nm trong gii hn h tr tt ca Real-time endpoints (h tr payload ln ti 6MB cho synchronous requests). Ti sao khng chn B (Amazon ECS + Scheduled scaling): o Scheduled scaling (Scaling theo lch trnh): Gii php ny ch hot ng hiu qu khi bn bit trc m hnh lu lng truy cp (v d: traffic tng vo 8 gi sng).  bi nhn mnh \"unpredictable bursts\" (t bin khng d bo trc), do  Scheduled scaling s tht bi trong vic p ng nhu cu tc thi, gy ra  tr hoc li. Ti sao khng chn C (Amazon EKS + SageMaker Operator): o Overhead vn hnh: Mc d EKS c th chy inference, nhng vic thit lp v qun l mt cluster Kubernetes ch  host model lm tng  phc tp khng cn thit so vi dch v native SageMaker. o Scaling Metric (Memory): Scaling da trn Memory (b nh) thng khng nhy bn cho inference bng scaling da trn s lng request hoc GPU utilization. Quan trng hn, gii php ny khng ti u ha \"out-of-the-box\" cho latency tt bng SageMaker Real-time endpoint. Ti sao khng chn D (Spot Instances + Spot Fleet): o Vi phm tnh sn sng (Availability): Spot Instances c th b AWS thu hi bt c lc no (vi cnh bo 2 pht). Vic ny vi phm yu cu \"highly available\" ca h thng Production. Nu instance b thu hi trong l..."
  },
  {
    "id": 104,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use an Amazon EMR cluster to process large volumes of data in batches. Any data loss is unacceptable. Which instance purchasing option will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": "A",
        "text": "Run the primary node, core nodes, and task nodes on On-Demand Instances."
      },
      {
        "id": "B",
        "text": "Run the primary node, core nodes, and task nodes on Spot Instances."
      },
      {
        "id": "C",
        "text": "Run the primary node on an On-Demand Instance. Run the core nodes and task nodes on Spot Instances."
      },
      {
        "id": "D",
        "text": "Run the primary node and core nodes on On-Demand Instances. Run the task nodes on Spot Instances."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Primary/Core On-Demand + Task Spot): o Bo v d liu (Data Integrity): Trong kin trc EMR, Core nodes chu trch nhim lu tr d liu trn HDFS (Hadoop Distributed File System). Nu chy Core nodes bng Spot Instances v b AWS thu hi, bn c nguy c mt d liu c lu trn  (vi phm yu cu \"Any data loss is unacceptable\"). Do , Core nodes bt buc phi dng On-Demand (hoc Reserved)  m bo  bn d liu. o Ti u chi ph (Cost-effective): Task nodes ch thc hin tnh ton (compute) v khng lu tr d liu HDFS. Nu mt Task node b thu hi, tc v  ch n gin l c gi li cho node khc x l m khng gy mt mt d liu. V vy, s dng Spot Instances cho Task nodes l cch an ton nht  gim chi ph m khng hy sinh tnh ton vn d liu. o Primary Node: Lun cn n nh  qun l cluster, nn dng On-Demand. Ti sao khng chn A (All On-Demand): o Khng ti u chi ph: Chy Task nodes bng On-Demand l lng ph tin bc v Task nodes hon ton c th chu li (fault-tolerant) v tn dng c gi r ca Spot Instances. Ti sao khng chn B (All Spot): o Ri ro cao nht: Nu Primary node (Spot) b thu hi, ton b cluster s dng hot ng (terminate). Nu Core nodes (Spot) b thu hi, d liu HDFS c th b mt vnh vin. Ti sao khng chn C (Core nodes on Spot): o Ri ro mt d liu: Nh  phn tch, Core nodes lu tr data. D HDFS c c ch replication, vic mt Core nodes t ngt do Spot interruption vn gy ri ro cao cho d liu v tnh n nh ca HDFS, vi phm yu cu \"khng chp nhn mt d liu\"."
  },
  {
    "id": 105,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to improve the sustainability of its ML operations. Which actions will reduce the energy usage and computational resources that are associated with the company's training jobs? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker Debugger to stop training jobs when non-converging conditions are detected."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker Ground Truth for data labeling."
      },
      {
        "id": "C",
        "text": "Deploy models by using AWS Lambda functions."
      },
      {
        "id": "D",
        "text": "Use AWS Trainium instances for training."
      },
      {
        "id": "E",
        "text": "Use PyTorch or TensorFlow with the distributed training option."
      },
      {
        "id": "A",
        "text": "Use Amazon SageMaker Debugger to stop training jobs when non-converging conditions are detected."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (SageMaker Debugger): o Loi b lng ph ti nguyn (Waste Reduction): SageMaker Debugger c kh nng gim st qu trnh training theo thi gian thc (real-time monitoring). Nu n pht hin loss function khng gim (non-converging) hoc cc li k thut (vanishing gradients), n c th t ng dng job. Vic ny ngn chn vic chy my trong nhiu gi hoc nhiu ngy v ch, gip tit kim in nng v ti nguyn tnh ton ng k. Ti sao chn D (AWS Trainium): o Ti u ha phn cng (Hardware Efficiency): AWS Trainium l dng chip c AWS thit k chuyn bit (custom silicon) cho mc ch Deep Learning training. Theo cng b ca AWS, Trainium cung cp hiu nng trn mi watt (performance per watt) tt hn nhiu so vi cc instance GPU tiu chun, trc tip gip gim lng tiu th nng lng cho cng mt khi lng cng vic training. Ti sao khng chn B (SageMaker Ground Truth): o Sai giai on (Wrong Phase): Ground Truth l dch v h tr gn nhn d liu (data labeling), thuc giai on chun b d liu (Data Prep). Vic gn nhn hiu qu khng tc ng trc tip n mc tiu th in nng ca h tng tnh ton (compute infrastructure) khi chy training job. Ti sao khng chn C (AWS Lambda): o Sai mc ch (Inference vs Training): AWS Lambda thng c dng  trin khai m hnh (Inference) cho cc workload nh hoc khng thng xuyn. Lambda khng c thit k v khng ph hp  chy cc training job nng n, ko di (long-running) v i hi ti nguyn GPU ln. Ti sao khng chn E (Distributed Training): o Khng m bo tit kim tng nng lng: Distributed training gip gim thi gian chy thc t (wall-clock time) nhng thng lm tng tng ti nguyn tiu th do chi ph giao tip mng (communication overhead) v ng b ha gia cc node. V d: Chy 1 GPU trong 10 gi (10 GPU-hours) thng tn t nng lng hn chy 10 GPU trong 1.2 gi (12 GPU-hours) do hiu sut scaling khng bao gi t 100%."
  },
  {
    "id": 106,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to create several ML prediction models. The training data is stored in Amazon S3. The entire dataset is more than 5  in size and consists of CSV, JSON, Apache Parquet, and simple text files. The data must be processed in several consecutive steps. The steps include complex manipulations that can take hours to finish running. Some of the processing involves natural language processing (NLP) transformations. The entire process must be automated. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Process data at each step by using Amazon SageMaker Data Wrangler. Automate the process by using Data Wrangler jobs."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker notebooks for each data processing step. Automate the process by using Amazon EventBridge."
      },
      {
        "id": "C",
        "text": "Process data at each step by using AWS Lambda functions. Automate the process by using AWS Step Functions and Amazon EventBridge."
      },
      {
        "id": "D",
        "text": "Use Amazon SageMaker Pipelines to create a pipeline of data processing steps. Automate the pipeline by using Amazon EventBridge."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Pipelines + EventBridge): o X l tc v di (Long-running tasks):  bi ghi r cc bc x l \"take hours to finish\" (mt hng gi). SageMaker Pipelines s dng SageMaker Processing Jobs  bn di, chy trn cc EC2 instances chuyn dng, cho php x l khi lng cng vic ln (5 TB) trong thi gian di m khng b gii hn thi gian (timeout) ngn hn. o Orchestration (iu phi quy trnh): SageMaker Pipelines l dch v native CI/CD cho ML, c thit k chuyn bit  qun l cc \"consecutive steps\" (cc bc lin tip), qun l s ph thuc (dependencies), v lu vt (lineage) ca d liu/model. o Automation: EventBridge tch hp hon ho  kch hot (trigger) Pipeline chy t ng da trn s kin (v d: khi c file mi upload ln S3). Ti sao khng chn C (AWS Lambda + Step Functions): o Gii hn thi gian (Timeout): y l li k thut ch mng (Hard Fail). AWS Lambda c gii hn thi gian chy ti a l 15 pht. Vi yu cu x l mt \"hng gi\" (hours), Lambda chc chn s b timeout v tht bi gia chng. Ti sao khng chn B (SageMaker Notebooks): o Khng phi mi trng Production: Notebooks c thit k cho vic pht trin (development) v tng tc (interactive), khng phi  chy cc quy trnh t ng ha quy m ln (production pipelines). Vic dng EventBridge kch hot Notebook l mt gii php chp v, kh qun l li (error handling) v kh m rng. Ti sao khng chn A (SageMaker Data Wrangler): o Hn ch v tnh ty bin (Custom Logic): Mc d Data Wrangler tt cho vic lm sch d liu (cleaning), nhng vi cc tc v \"complex NLP manipulations\" (x l ngn ng t nhin phc tp) thng i hi code ty chnh su (custom scripts/containers) hoc th vin chuyn bit. SageMaker Processing Jobs (thnh phn ca Pipelines) linh hot v mnh m hn nhiu cho cc tc v x l hng nng (heavy lifting) so vi giao din low-code ca Data Wrangler."
  },
  {
    "id": 107,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use AWS CloudFormation to create an ML model that an Amazon SageMaker endpoint will host. Which resource should the ML engineer declare in the CloudFormation template to meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "AWS::SageMaker::Model"
      },
      {
        "id": "B",
        "text": "AWS::SageMaker::Endpoint"
      },
      {
        "id": "C",
        "text": "AWS::SageMaker::NotebookInstance"
      },
      {
        "id": "D",
        "text": "AWS::SageMaker::Pipeline"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (AWS::SageMaker::Model): Trong kin trc trin khai ca Amazon SageMaker, resource AWS::SageMaker::Model chnh l thnh phn chu trch nhim \"to\" i tng model logic. N nh ngha cc thng s ct li gm: v tr ca model artifacts (trong S3), Docker container image dng  chy inference (Inference Image), v IAM Execution Role. y l resource bt buc phi c khai bo trc khi bn c th to Endpoint Configuration hay Endpoint. Ti sao khng chn B (AWS::SageMaker::Endpoint): Resource ny c dng  bc cui cng  provision (cp pht) ti nguyn HTTPS endpoint thc t phc v cho vic gi request inference. AWS::SageMaker::Endpoint ph thuc vo AWS::SageMaker::EndpointConfig, ch n khng dng  nh ngha bn thn model. Ti sao khng chn C (AWS::SageMaker::NotebookInstance): y l mt EC2 instance c ci t sn Jupyter/JupyterLab, ng vai tr l mi trng pht trin (IDE) cho Data Scientist  vit code v train model. N khng phi l resource i din cho model  hosting/deployment. Ti sao khng chn D (AWS::SageMaker::Pipeline): Resource ny dng  nh ngha workflow t ng ha (MLOps pipeline) gm cc bc nh x l d liu, training, nh gi model. Mc d pipeline c th to ra model (thng qua bc CreateModel), nhng bn thn resource AWS::SageMaker::Pipeline l cng c qun l quy trnh, khng phi l i tng model c host."
  },
  {
    "id": 108,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An advertising company uses AWS Lake Formation to manage a data lake. The data lake contains structured data and unstructured data. The company's ML engineers are assigned to specific advertisement campaigns. The ML engineers must interact with the data through Amazon Athena and by browsing the data directly in an Amazon S3 bucket. The ML engineers must have access to only the resources that are specific to their assigned advertisement campaigns. Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": "A",
        "text": "Configure IAM policies on an AWS Glue Data Catalog to restrict access to Athena based on the ML engineers' campaigns."
      },
      {
        "id": "B",
        "text": "Store users and campaign information in an Amazon DynamoDB table. Configure DynamoDB Streams to invoke an AWS Lambda function to update S3 bucket policies."
      },
      {
        "id": "C",
        "text": "Use Lake Formation to authorize AWS Glue to access the S3 bucket. Configure Lake Formation tags to map ML engineers to their campaigns."
      },
      {
        "id": "D",
        "text": "Configure S3 bucket policies to restrict access to the S3 bucket based on the ML engineers' campaigns."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Lake Formation tags): y l gii php hiu qu nht v mt vn hnh (Operationally Efficient)  qun l quyn truy cp chi tit (fine-grained access control) trong Data Lake. o Lake Formation s dng m hnh Tag-based Access Control (LF-Tags). Bn c th gn tag (v d: Campaign=A) cho database, table, hoc column, sau  cp quyn cho ngi dng da trn tag ny. o Khi c engineer mi hoc campaign mi, ch cn update tag hoc policy trong Lake Formation mt ln duy nht, thay v sa tng IAM Policy hay Bucket Policy phc tp. o Lake Formation t ng qun l credential tm thi  truy cp S3 khi user query qua Athena, m bo user ch thy d liu h c php thy. Ti sao khng chn A (IAM policies on Glue Catalog): o IAM Policy qun l quyn truy cp vo metadata trong Glue Data Catalog, nhng n khng th kim sot chi tit quyn truy cp vo d liu gc (underlying data) nm trong S3 mt cch linh hot v bo mt granular nh Lake Formation (v d: column-level security). o Vic vit IAM condition phc tp cho tng campaign s rt kh qun l v bo tr (Operational burden cao). Ti sao khng chn B (DynamoDB + Lambda + S3 Bucket Policy): o y l mt gii php \"t ch\" (custom solution) cc k phc tp v ri ro. Bn phi vit code Lambda, qun l DynamoDB, v quan trng nht l S3 Bucket Policy c gii hn v kch thc k t (policy size limit). o Khi s lng user v campaign tng, bucket policy s nhanh chng chm trn gii hn kch thc, khin gii php ny khng th m rng (not scalable). Ti sao khng chn D (S3 Bucket Policies): o Tng t nh B, S3 Bucket Policy khng c thit k  qun l hng trm user mapping vi hng trm campaign ring bit. o Vic hard-code quyn truy cp vo bucket policy cho tng engineer/campaign l c mng vn hnh (operational nightmare), d gy li v kh audit."
  },
  {
    "id": 109,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use data with Amazon SageMaker Canvas to train an ML model. The data is stored in Amazon S3 and is complex in structure. The ML engineer must use a file format that minimizes processing time for the data. Which file format will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "CSV files compressed with Snappy"
      },
      {
        "id": "B",
        "text": "JSON objects in JSONL format"
      },
      {
        "id": "C",
        "text": "JSON files compressed with gzip"
      },
      {
        "id": "D",
        "text": "Apache Parquet files"
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Apache Parquet files): o Hiu sut ti u (Minimize processing time): Apache Parquet l nh dng lu tr dng ct (columnar storage format). N cho php h thng (nh SageMaker Canvas) ch c nhng ct d liu cn thit thay v qut ton b dng, gip gim ng k lng I/O v tng tc  x l. o H tr cu trc phc tp (Complex structure):  bi nhc n d liu c \"complex in structure\". Parquet h tr rt tt cc kiu d liu lng nhau (nested data structures) mt cch hiu qu m vn gi c schema cht ch. o Nn hiu qu: Parquet thng i km vi thut ton nn (nh Snappy) ngay trong nh dng, gip gim dung lng lu tr v bng thng mng tt hn so vi cc nh dng vn bn thun ty. Ti sao khng chn A (CSV files compressed with Snappy): o CSV l nh dng da trn hng (row-based).  truy xut d liu, h thng phi phn tch c php (parse) tng dng vn bn, iu ny tn nhiu CPU hn v chm hn so vi format binary nh Parquet. o CSV x l cc cu trc d liu phc tp (nh mng hoc object lng nhau) rt km v thng d gy li nh dng. Ti sao khng chn B (JSON objects in JSONL format): o Mc d JSONL (JSON Lines) h tr cu trc phc tp tt, nhng n vn l nh dng vn bn (text-based). Vic parse JSON tn km ti nguyn CPU v dung lng file thng ln hn nhiu so vi Parquet (do lp li tn trng  mi bn ghi). Ti sao khng chn C (JSON files compressed with gzip): o Gzip gip gim dung lng file khi truyn ti, nhng n l nh dng nn khng th chia nh (non-splittable) mt cch hiu qu trong x l song song (parallel processing) nh Snappy hay cu trc block ca Parquet. o Khi cn c mt phn d liu, h thng vn phi gii nn (decompress) ton b block hoc file, gy chm tr trong x l (processing time)."
  },
  {
    "id": 110,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has trained and deployed an ML model by using Amazon SageMaker. The company needs to implement a solution to record and monitor all the API call events for the SageMaker endpoint. The solution also must provide a notification when the number of API call events breaches a threshold. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker Debugger to track the inferences and to report metrics. Create a custom rule to provide a notification when the threshold is breached."
      },
      {
        "id": "B",
        "text": "Use SageMaker Debugger to track the inferences and to report metrics. Use the tensor_variance built-in rule to provide a notification when the threshold is breached."
      },
      {
        "id": "C",
        "text": "Log all the endpoint invocation API events by using AWS CloudTrail. Use an Amazon CloudWatch dashboard for monitoring. Set up a CloudWatch alarm to provide notification when the threshold is breached."
      },
      {
        "id": "D",
        "text": "Add the Invocations metric to an Amazon CloudWatch dashboard for monitoring. Set up a CloudWatch alarm to provide notification when the threshold is breached."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (CloudTrail + CloudWatch): o Record API call events:  bi yu cu \"record... all the API call events\" (ghi li chi tit ton b s kin gi API). AWS CloudTrail l dch v duy nht c thit k chuyn bit  ghi nht k (logging/auditing) mi hnh ng gi API trong ti khon AWS, bao gm c thng tin ai gi, t IP no, v thi gian gi. i vi SageMaker Endpoint, hnh ng InvokeEndpoint c coi l \"Data Event\" v CloudTrail c th ghi li n. o Monitor & Notify: CloudWatch Dashboard dng  trc quan ha d liu log/metric ny v CloudWatch Alarm dng  gi thng bo khi s lng s kin vt ngng. S kt hp ny gii quyt trn vn c hai v \"Record\" v \"Monitor/Notify\". Ti sao khng chn D (Invocations metric only): o Mc d CloudWatch Metric Invocations c th m s lng request v kch hot Alarm (p ng v Monitor/Notify), nhng n khng p ng c yu cu \"Record all the API call events\". Metric ch lu tr con s thng k (v d: c 100 calls), ch khng lu tr bn ghi chi tit ca tng event (log entry)  phc v vic kim tra hay audit sau ny. Ti sao khng chn A (SageMaker Debugger - Custom rule): o SageMaker Debugger c thit k  theo di trng thi ni ti ca model (internal model state) trong qu trnh training (nh gi tr weights, gradients, loss)  tm li thut ton. N khng dng  ghi nhn cc s kin gi API  tng h tng (Infrastructure/API level). Ti sao khng chn B (SageMaker Debugger - tensor_variance): o Tng t nh A, rule tensor_variance dng  pht hin s bt thng trong tnh ton tensor ca model (v d: gradient vanishing/exploding), hon ton khng lin quan n vic m hay ghi log s ln gi API ca endpoint."
  },
  {
    "id": 111,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has AWS Glue data processing jobs that are orchestrated by an AWS Glue workflow. The AWS Glue jobs can run on a schedule or can be launched manually. The company is developing pipelines in Amazon SageMaker Pipelines for ML model development. The pipelines will use the output of the AWS Glue jobs during the data processing phase of model development. An ML engineer needs to implement a solution that integrates the AWS Glue jobs with the pipelines. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS Step Functions for orchestration of the pipelines and the AWS Glue jobs."
      },
      {
        "id": "B",
        "text": "Use processing steps in SageMaker Pipelines. Configure inputs that point to the Amazon Resource Names (ARNs) of the AWS Glue jobs."
      },
      {
        "id": "C",
        "text": "Use Callback steps in SageMaker Pipelines to start the AWS Glue workflow and to stop the pipelines until the AWS Glue jobs finish running."
      },
      {
        "id": "D",
        "text": "Use Amazon EventBridge to invoke the pipelines and the AWS Glue jobs in the desired order."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Callback steps): o C ch hot ng: CallbackStep trong Amazon SageMaker Pipelines c thit k c bit  tch hp cc quy trnh x l bn ngoi (external processes) vo pipeline. Khi pipeline chy n bc ny, n s gi mt token (thng qua SQS) v chuyn sang trng thi \"Waiting\". Mt Lambda function c th nhn token ny, kch hot AWS Glue Workflow hin c, v khi Glue Workflow chy xong, Lambda s gi ngc li API SendPipelineCallbackStepSuccess  pipeline tip tc. o Least Operational Overhead: Gii php ny tn dng li ton b logic ca AWS Glue Workflow  c sn (ng nh  bi yu cu l cng ty  c jobs v workflow). Bn khng cn phi vit li code x l d liu (ETL) hay cu hnh li h tng chy job, ch cn thm mt lp keo dn (integration layer) nh l xong. Ti sao khng chn A (AWS Step Functions): o Mc d Step Functions l cng c iu phi (orchestrator) rt mnh, nhng vic chuyn i c SageMaker Pipelines v AWS Glue Workflow hin c sang mt State Machine mi trong Step Functions i hi n lc ti cu trc (re- architecting) rt ln. iu ny vi phm nguyn tc \"LEAST operational overhead\". SageMaker Pipelines  l mt orchestrator chuyn dng cho ML, khng cn chng thm mt orchestrator khc. Ti sao khng chn B (Processing steps): o ProcessingStep trong SageMaker Pipelines dng  chy code (Python/bash) bn trong cc container do SageMaker qun l (Processing Job). N khng c chc nng nhn u vo l ARN ca mt AWS Glue Job  kch hot job  t xa. o  dng cch ny, bn phi migrate (di chuyn) code t Glue sang SageMaker Processing, iu ny tn rt nhiu cng sc v b ph ti nguyn Glue  thit lp. Ti sao khng chn D (Amazon EventBridge): o EventBridge c dng cho kin trc hng s kin (Event-driven) lng lo (loose coupling). N rt kh  thit lp mt quy trnh tun t cht ch (Sequential workflow) theo kiu: \"Pipeline dng ch -> Glue chy xong -> Pipeline chy tip\". Vic qun l tr..."
  },
  {
    "id": 112,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using an Amazon Redshift database as its single data source. Some of the data is sensitive. A data scientist needs to use some of the sensitive data from the database. An ML engineer must give the data scientist access to the data without transforming the source data and without storing anonymized data in the database. Which solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": "A",
        "text": "Configure dynamic data masking policies to control how sensitive data is shared with the data scientist at query time."
      },
      {
        "id": "B",
        "text": "Create a materialized view with masking logic on top of the database. Grant the necessary read permissions to the data scientist."
      },
      {
        "id": "C",
        "text": "Unload the Amazon Redshift data to Amazon S3. Use Amazon Athena to create schema-on-read with masking logic. Share the view with the data scientist."
      },
      {
        "id": "D",
        "text": "Unload the Amazon Redshift data to Amazon S3. Create an AWS Glue job to anonymize the data. Share the dataset with the data scientist."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Dynamic Data Masking): o Mapping vi  bi:  bi yu cu \"access... without transforming the source data\" (khng sa i d liu gc) v \"without storing anonymized data\" (khng lu tr thm bn sao d liu  n danh). o Gii php k thut: Amazon Redshift h tr tnh nng Dynamic Data Masking (DDM). Tnh nng ny cho php bn nh ngha cc chnh sch bo mt (masking policies)  n hoc thay i nh dng d liu nhy cm (nh s th tn dng, SSN) ngay ti thi im truy vn (query time) da trn vai tr ca ngi dng (Data Scientist). o Least Implementation Effort: Bn ch cn p dng policy ln ct d liu v gn n vi role ca Data Scientist. Khng cn to bng mi, view mi hay job ETL phc tp. Ti sao khng chn B (Materialized view with masking logic): o Vi phm yu cu v lu tr: Materialized View v bn cht l mt bng vt l lu tr kt qu ca cu query. iu ny vi phm yu cu \"without storing anonymized data in the database\" ca  bi. o Materialized View cng tiu tn ti nguyn lu tr v cn c ch refresh  ng b d liu, gy thm gnh nng qun tr (overhead) so vi DDM. Ti sao khng chn C (Unload to S3 + Athena): o Implementation Effort cao: Bn phi thc hin nhiu bc: Unload d liu ra S3 $\\rightarrow$ Crawler/Define Schema trong Glue/Athena $\\rightarrow$ Vit View trong Athena. Qu trnh ny phc tp hn nhiu so vi vic cu hnh policy trc tip trn Redshift. o Vic di chuyn d liu ra khi Redshift cng lm tng ri ro bo mt v  tr khi truy cp. Ti sao khng chn D (Unload to S3 + Glue Job): o Vi phm yu cu:  bi cm \"storing anonymized data\". Glue job s x l v sinh ra mt dataset mi  n danh v lu n xung S3 (hoc load li DB), tc l to ra bn sao d liu. o y l gii php tn km nht v cng sc trin khai (vit code ETL) v chi ph vn hnh (tr tin cho Glue DPU)."
  },
  {
    "id": 113,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is using a training job to fine-tune a deep learning model in Amazon SageMaker Studio. The ML engineer previously used the same pre-trained model with a similar dataset. The ML engineer expects vanishing gradient, underutilized GPU, and overfitting problems. The ML engineer needs to implement a solution to detect these issues and to react in predefined ways when the issues occur. The solution also must provide comprehensive real-time metrics during the training. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use TensorBoard to monitor the training job. Publish the findings to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function to consume the findings and to initiate the predefined actions."
      },
      {
        "id": "B",
        "text": "Use Amazon CloudWatch default metrics to gain insights about the training job. Use the metrics to invoke an AWS Lambda function to initiate the predefined actions."
      },
      {
        "id": "C",
        "text": "Expand the metrics in Amazon CloudWatch to include the gradients in each training step. Use the metrics to invoke an AWS Lambda function to initiate the predefined actions."
      },
      {
        "id": "D",
        "text": "Use SageMaker Debugger built-in rules to monitor the training job. Configure the rules to initiate the predefined actions."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Debugger): o Kh nng pht hin li chuyn su: Amazon SageMaker Debugger c thit k c bit  nhn su vo \"ni tng\" ca qu trnh training (Internal Model State). N cung cp cc Built-in Rules (quy tc c sn)  pht hin chnh xc cc vn  k thut m  bi nu:  VanishingGradient: Theo di gi tr gradient xem c v 0 hay khng.  LowGPUUtilization: Theo di hiu sut phn cng.  Overfit: So snh loss gia training v validation set. o T ng ha phn ng (React): Khi mt Rule b vi phm (trigger), SageMaker Debugger s gi s kin ti Amazon EventBridge. T , bn c th kch hot cc hnh ng nh sn (nh StopTrainingJob hoc gi SNS notification) m khng cn vit code gim st phc tp. o Least Operational Overhead: V s dng rules \"built-in\" (c sn), Engineer ch cn bt cu hnh trong Estimator SDK m khng cn xy dng h thng thu thp log hay phn tch th cng. Ti sao khng chn A (TensorBoard + SNS + Lambda): o TensorBoard l cng c tuyt vi  con ngi trc quan ha (visualize) qu trnh training, nhng n khng c thit k  t ng kch hot hnh ng khc phc li (remediation). o Vic xy dng mt pipeline  trch xut d liu t TensorBoard, y qua SNS v kch hot Lambda l mt gii php th cng tn nhiu cng sc bo tr (high operational overhead). Ti sao khng chn B (CloudWatch default metrics + Lambda): o Phm vi gim st hn ch: CloudWatch Default Metrics ch cung cp thng s v h tng (CPU, Memory, Disk I/O). N hon ton \"m\" trc cc vn  v thut ton nh Vanishing Gradient (i hi phi xem gi tr tensor) hay Overfitting (i hi logic so snh loss function). Ti sao khng chn C (Expand CloudWatch metrics + Lambda): o Khng kh thi v hiu nng: Vic y ton b d liu Gradients (vn rt ln v cp nht lin tc tng step) ln CloudWatch Logs s gy ra  tr ln v chi ph cao. o Vic vit logic Lambda  phn tch dng d liu gradient thi gian thc nhm pht hin va..."
  },
  {
    "id": 114,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A credit card company has a fraud detection model in production on an Amazon SageMaker endpoint. The company develops a new version of the model. The company needs to assess the new model's performance by using live data and without affecting production end users. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Set up SageMaker Debugger and create a custom rule."
      },
      {
        "id": "B",
        "text": "Set up blue/green deployments with all-at-once traffic shifting."
      },
      {
        "id": "C",
        "text": "Set up blue/green deployments with canary traffic shifting."
      },
      {
        "id": "D",
        "text": "Set up shadow testing with a shadow variant of the new model."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Shadow testing): o C ch hot ng: Shadow Testing (Th nghim bng) cho php bn trin khai phin bn model mi (Shadow variant) chy song song vi phin bn sn phm (Production variant). o Live Data: Mi request gi n endpoint s c gi n c hai model. o No User Impact: Ch c phn hi t Production variant c tr v cho ngi dng. Phn hi t Shadow variant c ghi li  phn tch sau ( so snh  chnh xc) nhng hon ton b loi b khi lung phn hi ngi dng. Do , nu model mi b li hoc chy chm, n khng h nh hng n tri nghim ca ngi dng cui. y chnh xc l nhng g  bi yu cu. Ti sao khng chn A (SageMaker Debugger): o SageMaker Debugger ch yu dng  gim st trng thi ni ti (tensor, weights) trong qu trnh Training hoc pht hin li k thut. N khng phi l phng php deployment  so snh hiu nng suy lun (inference performance) gia hai phin bn model trn production. Ti sao khng chn B (Blue/Green all-at-once): o Phng php ny chuyn ton b 100% traffic sang model mi ngay lp tc. Nu model mi c li, ton b ngi dng s b nh hng ngay lp tc. iu ny vi phm yu cu \"without affecting production end users\". Ti sao khng chn C (Blue/Green canary): o Canary deployment chuyn mt phn nh traffic (v d 10%) sang model mi  th nghim. Mc d ri ro thp hn All-at-once, nhng vn c 10% ngi dng nhn phn hi t model mi. Nu model mi sai, nhng ngi dng ny s b nh hng."
  },
  {
    "id": 115,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company stores time-series data about user clicks in an Amazon S3 bucket. The raw data consists of millions of rows of user activity every day. ML engineers access the data to develop their ML models. The ML engineers need to generate daily reports and analyze click trends over the past 3 days by using Amazon Athena. The company must retain the data for 30 days before archiving the data. Which solution will provide the HIGHEST performance for data retrieval?",
    "options": [
      {
        "id": "A",
        "text": "Keep all the time-series data without partitioning in the S3 bucket. Manually move data that is older than 30 days to separate S3 buckets."
      },
      {
        "id": "B",
        "text": "Create AWS Lambda functions to copy the time-series data into separate S3 buckets. Apply S3 Lifecycle policies to archive data that is older than 30 days to S3 Glacier Flexible Retrieval."
      },
      {
        "id": "C",
        "text": "Organize the time-series data into partitions by date prefix in the S3 bucket. Apply S3 Lifecycle policies to archive partitions that are older than 30 days to S3 Glacier Flexible Retrieval."
      },
      {
        "id": "D",
        "text": "Put each day's time-series data into its own S3 bucket. Use S3 Lifecycle policies to archive S3 buckets that hold data that is older than 30 days to S3 Glacier Flexible Retrieval."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Partitioning by date): o Ti u hiu nng Athena (Highest Performance): Amazon Athena hot ng theo c ch tnh ph v hiu nng da trn lng d liu qut (data scanned). Khi d liu time-series c t chc thnh cc partition theo cu trc th mc (v d: s3://bucket/data/year=2024/month=01/day=11/), Athena c th s dng k thut Partition Pruning. o Khi Engineer truy vn d liu \"3 ngy qua\", Athena ch qut ng 3 th mc tng ng v b qua ton b d liu ca 27 ngy cn li. iu ny gip gim ng k I/O, lm cho truy vn nhanh hn rt nhiu so vi vic qut ton b (Full scan). o T ng ha lu tr: S3 Lifecycle Policies hot ng hiu qu trn cc prefix (th mc), cho php t ng chuyn data c (>30 ngy) sang Glacier  tit kim chi ph m khng cn can thip th cng. Ti sao khng chn A (No partitioning): o Nu khng phn vng (partition), Athena buc phi thc hin Full Table Scan (qut ton b file trong bucket) cho bt k cu lnh truy vn no, d ch cn data ca 1 ngy. iu ny khin tc  truy vn cc chm khi d liu ln dn v chi ph tng vt. o Vic di chuyn d liu th cng (Manually move) l gnh nng vn hnh khng cn thit. Ti sao khng chn B (Lambda to separate buckets): o Vic sao chp d liu (copy) to ra s d tha lu tr (duplication) v tn chi ph khng ng c. o Phn tn d liu ra nhiu bucket lm phc tp ha vic nh ngha bng trong Athena (Athena table thng tr v mt location gc duy nht). Vic join d liu t nhiu bucket khc nhau s lm gim hiu nng truy vn. Ti sao khng chn D (One bucket per day): o y l mt thit k \"anti-pattern\" trong Data Lake. Vic to hng ngn bucket (mi ngy 1 ci) s chm gii hn s lng bucket ca AWS (mc nh 100). o Athena s gp rt nhiu kh khn  truy vn tng hp (aggregate) d liu xuyn sut nhiu ngy v khng th nh ngha mt bng (table) duy nht bao trm hng lot bucket ri rc mt cch hiu qu."
  },
  {
    "id": 116,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has deployed an ML model that detects fraudulent credit card transactions in real time in a banking application. The model uses Amazon SageMaker Asynchronous Inference. Consumers are reporting delays in receiving the inference results. An ML engineer needs to implement a solution to improve the inference performance. The solution also must provide a notification when a deviation in model quality occurs. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker real-time inference for inference. Use SageMaker Model Monitor for notifications about model quality."
      },
      {
        "id": "B",
        "text": "Use SageMaker batch transform for inference. Use SageMaker Model Monitor for notifications about model quality."
      },
      {
        "id": "C",
        "text": "Use SageMaker Serverless Inference for inference. Use SageMaker Inference Recommender for notifications about model quality."
      },
      {
        "id": "D",
        "text": "Keep using SageMaker Asynchronous Inference for inference. Use SageMaker Inference Recommender for notifications about model quality."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Real-time Inference + Model Monitor): o Gii quyt vn   tr (Delays):  bi m t ng dng pht hin gian ln th tn dng \"in real time\" (thi gian thc), nhng ngi dng ang gp s c tr (delays) do s dng Asynchronous Inference. Asynchronous Inference c thit k cho cc payload ln v thi gian x l di (c th ln ti 1 gi), nn n c  tr hng i (queueing delay) c hu, khng ph hp cho giao dch cn phn hi tc th. Chuyn sang Real-time Inference l gii php chun xc  m bo  tr thp (low latency)  mc mili-giy. o Gim st cht lng (Model Quality): SageMaker Model Monitor l cng c chuyn dng  theo di drift (s tri dt) v cht lng model (Model Quality Drift) hoc d liu (Data Drift) trn production endpoint v gi thng bo qua CloudWatch/SNS khi pht hin sai lch. Ti sao khng chn B (Batch Transform): o Batch Transform dng  x l d liu hng lot (offline processing) cho lng ln d liu c lu sn trong S3, khng phi cho ng dng tng tc thi gian thc.  tr ca n l pht hoc gi, t hn c Asynchronous Inference. Ti sao khng chn C (Serverless Inference + Inference Recommender): o Serverless Inference: Mc d tin li, nhng Serverless Inference c th gp vn  \"Cold Start\" (khi ng ngui) gy  tr ngu nhin, khng n nh bng Real-time Inference (Provisioned instances) cho cc ng dng ngn hng quan trng. o Sai cng c: SageMaker Inference Recommender l cng c dng  load test v gi  chn loi instance/cu hnh ti u trc khi trin khai, ch khng phi l cng c gim st cht lng model (quality deviation) lin tc trong khi vn hnh (runtime). Ti sao khng chn D (Keep Asynchronous Inference): o Nh  gii thch  mc A, bn cht ca Asynchronous Inference l nguyn nhn gy ra  tr. Gi nguyn kin trc ny s khng gii quyt c vn  ct li ca ngi dng (\"consumers are reporting delays\")."
  },
  {
    "id": 117,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to implement a solution to host a trained ML model. The rate of requests to the model will be inconsistent throughout the day. The ML engineer needs a scalable solution that minimizes costs when the model is not in use. The solution also must maintain the model's capacity to respond to requests during times of peak usage. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create AWS Lambda functions that have fixed concurrency to host the model. Configure the Lambda functions to automatically scale based on the number of requests to the model."
      },
      {
        "id": "B",
        "text": "Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. Set a static number of tasks to handle requests during times of peak usage."
      },
      {
        "id": "C",
        "text": "Deploy the model to an Amazon SageMaker endpoint. Deploy multiple copies of the model to the endpoint. Create an Application Load Balancer to route traffic between the different copies of the model at the endpoint."
      },
      {
        "id": "D",
        "text": "Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies that are based on Amazon CloudWatch metrics to adjust the number of instances dynamically."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Endpoint Auto Scaling): o Kh nng m rng (Scalable): Amazon SageMaker Endpoints h tr tnh nng Application Auto Scaling tch hp sn. Bn c th nh ngha chnh sch (scaling policy) da trn cc ch s CloudWatch (nh InvocationsPerInstance hoc CPUUtilization). o Ti u chi ph & Hiu nng: H thng s t ng thm instance (Scale Out) khi traffic tng  m bo nng lc x l gi cao im, v quan trng hn l t ng tt bt instance (Scale In) khi traffic gim hoc khng c ngi dng, gip gim thiu chi ph ng theo yu cu  bi. Ti sao khng chn A (Lambda with fixed concurrency): o Mu thun k thut: Vic thit lp \"fixed concurrency\" (ng thi c nh) ng ngha vi vic bn gii hn kh nng m rng ca Lambda, i ngc li vi yu cu \"configure... to automatically scale\". o Ngoi ra, Lambda c gii hn v thi gian chy (timeout) v dung lng b nh, thng khng phi l la chn ti u cho cc model ML phc tp cn stateful hosting. Ti sao khng chn B (ECS Fargate with static tasks): o Lng ph chi ph: \"Static number of tasks\" (S lng tc v tnh) c ngha l bn phi lun duy tr s lng container   chu ti cho lc cao im nht (peak usage). Khi traffic thp, cc resource ny vn chy v tnh tin, vi phm yu cu \"minimizes costs when the model is not in use\". Ti sao khng chn C (SageMaker Endpoint + Manual ALB): o Sai kin trc: SageMaker Endpoint bn thn n  bao gm mt lp Load Balancer ni b  phn phi traffic ti cc instance pha sau. Bn khng cn (v khng th) to th cng mt Application Load Balancer (ALB)  iu hng traffic gia cc model copy bn trong mt endpoint. o Thiu c ch Auto Scaling  ti u chi ph."
  },
  {
    "id": 118,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses Amazon SageMaker Studio to develop an ML model. The company has a single SageMaker Studio domain. An ML engineer needs to implement a solution that provides an automated alert when SageMaker compute costs reach a specific threshold. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Add resource tagging by editing the SageMaker user profile in the SageMaker domain. Configure AWS Cost Explorer to send an alert when the threshold is reached."
      },
      {
        "id": "B",
        "text": "Add resource tagging by editing the SageMaker user profile in the SageMaker domain. Configure AWS Budgets to send an alert when the threshold is reached."
      },
      {
        "id": "C",
        "text": "Add resource tagging by editing each user's IAM profile. Configure AWS Cost Explorer to send an alert when the threshold is reached."
      },
      {
        "id": "D",
        "text": "Add resource tagging by editing each user's IAM profile. Configure AWS Budgets to send an alert when the threshold is reached."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (SageMaker User Profile Tags + AWS Budgets): o Resource Tagging: Trong Amazon SageMaker Studio,  theo di chi ph theo tng ngi dng hoc nhm, bn cn gn th (tag) vo SageMaker User Profile. Cc th ny s t ng c lan truyn (propagate) xung cc ti nguyn tnh ton (nh Apps, Notebooks, Training Jobs) m user  to ra. iu ny cho php phn b chi ph (Cost Allocation) chnh xc trong ha n. o Alerting: AWS Budgets l dch v chuyn dng  thit lp ngn sch v gi thng bo cnh bo (email/SNS) khi chi ph thc t hoc d bo vt qu ngng (threshold) bn t ra. y l gii php tiu chun cho yu cu \"automated alert when costs reach a specific threshold\". Ti sao khng chn A (AWS Cost Explorer): o Sai chc nng: AWS Cost Explorer ch yu l cng c  phn tch v bo co lch s chi ph hoc d bo. Mc d n c tnh nng Cost Anomaly Detection, nhng  thit lp mt cnh bo ngng c nh n gin (v d: \"Bo cho ti khi tiu ht $100\"), AWS Budgets l cng c trc tip v ph hp hn. Ti sao khng chn C v D (Tagging IAM Profile): o C ch khng hiu qu: Vic gn tag vo IAM User hoc IAM Role khng t ng gn tag  cho cc ti nguyn (nh EC2 instances, SageMaker Jobs) m IAM identity  to ra. Do , trong bng phn tch chi ph (Cost Allocation Report), cc ti nguyn SageMaker s khng mang tag ca IAM user, khin vic theo di chi ph theo user tr nn bt kh thi theo cch ny. o Ring C cn sai v dng Cost Explorer  alert nh l do  mc A."
  },
  {
    "id": 119,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses Amazon SageMaker for its ML workloads. The company's ML engineer receives a 50 MB Apache Parquet data file to build a fraud detection model. The file includes several correlated columns that are not required. What should the ML engineer do to drop the unnecessary columns in the file with the LEAST effort?",
    "options": [
      {
        "id": "A",
        "text": "Download the file to a local workstation. Perform one-hot encoding by using a custom Python script."
      },
      {
        "id": "B",
        "text": "Create an Apache Spark job that uses a custom processing script on Amazon EMR."
      },
      {
        "id": "C",
        "text": "Create a SageMaker processing job by calling the SageMaker Python SDK."
      },
      {
        "id": "D",
        "text": "Create a data flow in SageMaker Data Wrangler. Configure a transform step."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Data Wrangler): o Least Effort (N lc t nht): SageMaker Data Wrangler l cng c Low- code/No-code (Giao din ngi dng  ha) c tch hp sn trong SageMaker Studio. N cho php bn import d liu t S3, xem trc (visualize), v thc hin cc thao tc x l d liu nh Drop Column ch bng cc c click chut m khng cn vit mt dng code no. o i vi file kch thc nh (50 MB) v yu cu n gin (b ct), vic s dng cng c UI ko-th l nhanh chng v tin li nht so vi vic phi vit code v setup mi trng. Ti sao khng chn A (Download to local workstation): o Bad Practice: Vic download d liu ra khi mi trng Cloud (AWS) v my local thng b hn ch v l do bo mt v qun tr d liu (Data Governance). o Manual process: y l quy trnh th cng, kh t ng ha v khng th scale nu sau ny file ln hn hoc cn x l nh k. Ti sao khng chn B (Amazon EMR Spark job): o Overkill (Dng dao m tru git g): Amazon EMR (Elastic MapReduce) l dch v dnh cho Big Data x l hng TB/PB d liu. Vic khi to v cu hnh c mt cluster EMR ch  x l mt file 50 MB l cc k lng ph ti nguyn, tin bc v cng sc qun tr. Ti sao khng chn C (SageMaker Processing Job): o High Effort:  chy Processing Job, bn phi vit script Python, chun b Docker container, v cu hnh SDK  submit job. Mc d y l cch chun cho quy trnh t ng ha (MLOps), nhng so vi vic click chut trong Data Wrangler cho mt tc v ad-hoc n gin, th Processing Job tn nhiu cng sc trin khai hn (coding overhead)."
  },
  {
    "id": 120,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "company is creating an application that will recommend products for customers to purchase. The application will make API calls to Amazon Q Business. The company must ensure that responses from Amazon Q Business do not include the name of the company's main competitor. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Configure the competitor's name as a blocked phrase in Amazon Q Business."
      },
      {
        "id": "B",
        "text": "Configure an Amazon Q Business retriever to exclude the competitor's name."
      },
      {
        "id": "C",
        "text": "Configure an Amazon Kendra retriever for Amazon Q Business to build indexes that exclude the competitor's name."
      },
      {
        "id": "D",
        "text": "Configure document attribute boosting in Amazon Q Business to deprioritize the competitor's name."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Blocked phrase): o Tnh nng trc tip: Amazon Q Business cung cp tnh nng Global Controls (Kim sot ton cc), cho php qun tr vin nh ngha danh sch Blocked phrases (Cc cm t b chn). o C ch hot ng: Khi bn thm tn i th vo danh sch ny, Amazon Q Business s ch ng pht hin v ngn chn cm t  xut hin trong cu tr li c sinh ra (generated response) cho ngi dng cui. y l gii php \"Hard block\" (chn cng) m bo tun th yu cu  bi mt cch chnh xc nht. Ti sao khng chn B (Retriever exclusion): o Vic cu hnh Retriever  loi b tn i th thng tc ng vo vic tm kim ti liu (filtering documents). Nu mt ti liu sn phm hp l ca cng ty c cha tn i th (v d: trong bng so snh), ti liu  c th b loi b hon ton, dn n mt thng tin ng cnh cn thit. N lc  u vo (Input), khng phi kim sot u ra (Output). Ti sao khng chn C (Kendra index exclusion): o Tng t nh B, vic can thip vo qu trnh Indexing ca Amazon Kendra l  qun l d liu ngun. N khng m bo ngn chn c vic m hnh ngn ng (LLM) t sinh ra (hallucinate) tn i th trong qu trnh to cu tr li. Ti sao khng chn D (Document attribute boosting): o Deprioritize $\\neq$ Block: Vic h thp  u tin (Deprioritize) ch lm cho cc ti liu cha tn i th xut hin  v tr thp hn trong kt qu tm kim. Tuy nhin, nu khng c ti liu no khc tt hn, Amazon Q vn c th s dng ti liu  v sinh ra cu tr li cha tn i th. y l bin php \"Soft\" (mm), khng m bo yu cu cm tuyt i."
  },
  {
    "id": 121,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use Amazon SageMaker to fine-tune a large language model (LLM) for text summarization. The ML engineer must follow a low-code no-code (LCNC) approach. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker Studio to fine-tune an LLM that is deployed on Amazon EC2 instances."
      },
      {
        "id": "B",
        "text": "Use SageMaker Autopilot to fine-tune an LLM that is deployed by a custom API endpoint."
      },
      {
        "id": "C",
        "text": "Use SageMaker Autopilot to fine-tune an LLM that is deployed on Amazon EC2 instances."
      },
      {
        "id": "D",
        "text": "Use SageMaker Autopilot to fine-tune an LLM that is deployed by SageMaker JumpStart."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Autopilot + JumpStart): o Low-code/No-code (LCNC): Amazon SageMaker Autopilot  h tr tnh nng Fine-tuning LLMs (tinh chnh m hnh ngn ng ln) cho bi ton Generative AI (nh Text Summarization). o Integration: Quy trnh ny tch hp trc tip vi SageMaker JumpStart - th vin cha cc Foundation Models (FM) c hun luyn sn. Bn ch cn chn model t JumpStart, cung cp dataset, v Autopilot s t ng chy experiment  fine-tune m khng cn vit code training phc tp. o Deployment: Sau khi fine-tune xong, model c th c deploy d dng thng qua c ch \"One-click deployment\" ca JumpStart (hoc to Endpoint t Model Package ca Autopilot), m bo tnh cht Low-code t u n cui. Ti sao khng chn A (SageMaker Studio + EC2): o Vic t fine-tune trn Studio v qun l trin khai th cng ln EC2 instances l phng php High-code (nhiu code), i hi kin thc su v qun tr h tng v scripting, vi phm yu cu LCNC. Ti sao khng chn B (Custom API endpoint): o \"Custom API endpoint\" m ch vic bn phi t xy dng lp API (v d dng Flask/FastAPI, Dockerize, setup Server), y l cng vic nng v k thut (Engineering heavy), khng phi Low-code. Ti sao khng chn C (Deploy on EC2): o Tng t nh A, vic deploy trc tip ln EC2 (IaaS) i hi bn phi t qun l OS, driver GPU, security patching v scaling, khng ph hp vi tiu ch gim thiu vn hnh ca LCNC."
  },
  {
    "id": 122,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an ML model that needs to run one time each night to predict stock values. The model input is 3 MB of data that is collected during the current day. The model produces the predictions for the next day. The prediction process takes less than 1 minute to finish running. How should the company deploy the model on Amazon SageMaker to meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use a multi-model serverless endpoint. Enable caching."
      },
      {
        "id": "B",
        "text": "Use an asynchronous inference endpoint. Set the InitialInstanceCount parameter to 0."
      },
      {
        "id": "C",
        "text": "Use a real-time endpoint. Configure an auto scaling policy to scale the model to 0 when the model is not in use."
      },
      {
        "id": "D",
        "text": "Use a serverless inference endpoint. Set the MaxConcurrency parameter to 1."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Serverless inference endpoint): o Ti u chi ph cho traffic ngt qung: M hnh ch chy \"mt ln mi m\" (intermittent traffic). SageMaker Serverless Inference c kh nng t ng Scale to 0 khi khng c request, ngha l bn khng tn bt k chi ph no cho thi gian ch (idle time). Bn ch tr tin cho ng khong thi gian tnh ton (< 1 pht) m m hnh chy. o Ph hp vi workload: Serverless Inference h tr payload ln ti 4MB ( bi 3MB) v thi gian x l ti a 3 pht ( bi < 1 pht), hon ton ph hp vi yu cu k thut ny. o MaxConcurrency = 1: V ch chy 1 ln/m, khng cn x l song song, t concurrency = 1  gii hn ti nguyn l hp l. Ti sao khng chn A (Multi-model serverless): o D tha: Multi-model Endpoint (MME) c thit k  host hng trm/hng ngn model khc nhau trn cng mt container chung  tit kim chi ph.  y ch c mt model duy nht, nn vic dng MME l khng cn thit v lm phc tp cu hnh. Ti sao khng chn B (Asynchronous inference endpoint): o Sai mc ch: Asynchronous Inference thng dng cho cc payload rt ln (ln ti 1GB) hoc thi gian x l rt di (ln ti 1 gi). Vi input nh (3 MB) v x l nhanh (< 1 pht), Serverless Inference nhanh gn v n gin hn nhiu so vi vic qun l hng i (queue) ca Async. Ti sao khng chn C (Real-time endpoint scale to 0): o Li k thut: Standard Real-time Endpoint (Provisioned Instances) khng th scale v 0. S lng instance ti thiu lun l 1. Do , bn s phi tr tin cho instance  chy 24/7 d ch dng n 1 pht mi m. iu ny cc k lng ph."
  },
  {
    "id": 123,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer trained an ML model on Amazon SageMaker to detect automobile accidents from dosed-circuit TV footage. The ML engineer used SageMaker Data Wrangler to create a training dataset of images of accidents and non-accidents. The model performed well during training and validation. However, the model is underperforming in production because of variations in the quality of the images from various cameras. Which solution will improve the model's accuracy in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Collect more images from all the cameras. Use Data Wrangler to prepare a new training dataset."
      },
      {
        "id": "B",
        "text": "Recreate the training dataset by using the Data Wrangler corrupt image transform. Specify the impulse noise option."
      },
      {
        "id": "C",
        "text": "Recreate the training dataset by using the Data Wrangler enhance image contrast transform. Specify the Gamma contrast option."
      },
      {
        "id": "D",
        "text": "Recreate the training dataset by using the Data Wrangler resize image transform. Crop all images to the same size."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Corrupt image transform - Impulse noise): o Tng cng tnh bn vng (Robustness via Data Augmentation): Vn  ct li l m hnh ang b \"Overfitting\" vi d liu sch trong phng lab v tht bi khi gp d liu thc t \"km cht lng\" (nhiu, m) t CCTV. o Trong SageMaker Data Wrangler, nhm tnh nng \"Corrupt image\" (bao gm thm nhiu Impulse noise, Gaussian noise, Blur...) c thit k chnh xc  gii quyt vn  ny. Bng cch ch ng thm nhiu vo d liu hun luyn, bn p m hnh hc cc c trng quan trng (nh hnh dng xe, va chm) thay v ph thuc vo  sc nt hay cht lng im nh. o K thut ny gip m hnh \"min nhim\" tt hn vi cc bin th cht lng thp t camera thc t m khng cn thu thp thm d liu mi. Ti sao khng chn A (Collect more images): o Vi phm rng buc thi gian:  bi yu cu gii php tn \"LEAST amount of time\". Vic thu thp thm hnh nh t camera, gn nhn li (labeling), v chun b d liu mi l mt quy trnh cc k tn thi gian v cng sc so vi vic ch click chn bin i (transform) trn d liu c sn. Ti sao khng chn C (Enhance image contrast): o Sai mc tiu: \"Enhance image contrast\" (Tng  tng phn) thng dng  x l vn  v nh sng (qu ti/qu sng). Tuy nhin, \"variations in quality\" t CCTV thng m ch nhiu ht (grain), m (blur) hoc nn nh (compression artifacts). o Vic tng tng phn cho nh nhiu i khi cn lm t hn (khuch i nhiu). Quan trng hn,  m hnh chu c nh xu, ta cn train n trn nh xu (Option B), ch khng phi train trn nh  c lm p (Option C) tr khi ta cng p dng bc lm p  cho production pipeline (iu ny lm tng  tr). Ti sao khng chn D (Resize/Crop): o Khng lin quan: Resize v Crop ch gii quyt vn  v kch thc hoc t l khung hnh (dimension/aspect ratio), hon ton khng gip ci thin kh nng nhn din ca m hnh i vi cc vn  v cht lng hnh nh (nhiu/m)."
  },
  {
    "id": 124,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an application that uses different APIs to generate embeddings for input text. The company needs to implement a solution to automatically rotate the API tokens every 3 months. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Store the tokens in AWS Secrets Manager. Create an AWS Lambda function to perform the rotation."
      },
      {
        "id": "B",
        "text": "Store the tokens in AWS Systems Manager Parameter Store. Create an AWS Lambda function to perform the rotation."
      },
      {
        "id": "C",
        "text": "Store the tokens in AWS Key Management Service (AWS KMS). Use an AWS managed key to perform the rotation."
      },
      {
        "id": "D",
        "text": "Store the tokens in AWS Key Management Service (AWS KMS). Use an AWS owned key to perform the rotation."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Secrets Manager + Lambda): o Tnh nng chuyn dng: AWS Secrets Manager l dch v c thit k c bit  lu tr v qun l vng i ca cc b mt (secrets) nh API tokens, database credentials. o Automatic Rotation: N c tnh nng tch hp sn  t ng xoay vng b mt theo lch trnh (v d: mi 3 thng). o Custom Rotation Logic: V y l API token ca dch v bn ngoi (generic API), Secrets Manager khng th t bit cch \"i mt khu\". Do , bn cn vit mt AWS Lambda function (cha logic gi API  xin token mi) v gn n vo Secrets Manager. Secrets Manager s chu trch nhim kch hot Lambda ny ng lch  thc hin xoay vng. Ti sao khng chn B (Systems Manager Parameter Store): o Mc d Parameter Store (SecureString) c th lu tr b mt, nhng n khng c tnh nng t ng xoay vng tch hp sn (native rotation scheduler). o  thc hin xoay vng vi Parameter Store, bn phi t xy dng h thng kch hot (v d: dng EventBridge  trigger Lambda th cng). iu ny tn cng sc qun tr hn so vi gii php trn gi ca Secrets Manager. Ti sao khng chn C (AWS KMS - Managed key): o Sai i tng: AWS KMS qun l kha m ha (encryption keys) dng  m ha d liu, ch khng phi l ni lu tr ni dung b mt (secret value) nh API Token  ng dng truy xut. o Vic xoay vng key trong KMS (Key Rotation) ch l thay i vt liu kha m ha bn di (backing key material), n khng h thay i chui k t API Token m ng dng ang s dng. Ti sao khng chn D (AWS KMS - Owned key): o Tng t nh C, KMS xoay vng kha m ha, khng xoay vng gi tr ca API Token. Ngoi ra, bn khng th qun l vic xoay vng ca AWS Owned Key (kha do AWS s hu hon ton)."
  },
  {
    "id": 125,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer receives datasets that contain missing values, duplicates, and extreme outliers. The ML engineer must consolidate these datasets into a single data frame and must prepare the data for ML. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker Data Wrangler to import the datasets and to consolidate them into a single data frame. Use the cleansing and enrichment functionalities to prepare the data."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker Ground Truth to import the datasets and to consolidate them into a single data frame. Use the human-in-the-loop capability to prepare the data."
      },
      {
        "id": "C",
        "text": "Manually import and merge the datasets. Consolidate the datasets into a single data frame. Use Amazon Q Developer to generate code snippets that will prepare the data."
      },
      {
        "id": "D",
        "text": "Manually import and merge the datasets. Consolidate the datasets into a single data frame. Use Amazon SageMaker data labeling to prepare the data."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (SageMaker Data Wrangler): o Consolidate Data: SageMaker Data Wrangler cho php import d liu t nhiu ngun khc nhau (S3, Athena, Redshift...) v h tr cc thao tc Join hoc Concatenate trc quan ngay trn giao din  gp nhiu dataset thnh mt data frame duy nht. o Data Preparation: y l cng c chuyn dng cho vic lm sch d liu (Data Cleaning) m khng cn vit code. N c sn cc built-in transforms  t ng x l cc vn   bi nu:  Missing values: Fill gi tr thiu (Impute) hoc drop dng.  Duplicates: Drop cc dng trng lp.  Outliers: Pht hin v x l cc gi tr ngoi lai (Standard deviation, Quantile ranges...). Ti sao khng chn B (SageMaker Ground Truth): o Sai mc ch: Ground Truth l dch v dng  gn nhn d liu (Data Labeling) th cng (v d: v khung bao quanh vt th trong nh, phn loi vn bn)  to ra Ground Truth dataset. N khng c chc nng lm sch d liu dng bng (tabular cleaning) nh x l missing value hay outliers. Ti sao khng chn C (Manual + Amazon Q Developer): o Tn cng sc (High Effort): Vic import v merge th cng i hi bn phi vit code v x l mi trng. D Amazon Q c th gi  code, nhng so vi gii php trn gi (out-of-the-box) nh Data Wrangler, cch ny tn nhiu thi gian v cng sc qun tr hn cho cc tc v chun ha d liu c bn. Ti sao khng chn D (SageMaker data labeling): o Tng t nh B, Data Labeling ch gii quyt vn  thiu nhn (target variable), khng gii quyt vn  d liu bn (dirty data) nh duplicates hay outliers."
  },
  {
    "id": 126,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has historical data that shows whether customers needed long-term support from company staff. The company needs to develop an ML model to predict whether new customers will require long-term support. Which modeling approach should the company use to meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Anomaly detection"
      },
      {
        "id": "B",
        "text": "Linear regression"
      },
      {
        "id": "C",
        "text": "Logistic regression"
      },
      {
        "id": "D",
        "text": "Semantic segmentation"
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Logistic regression): o Loi bi ton: Cu hi yu cu d on \"whether... will require\" (Liu khch hng c cn h tr hay khng). y l bi ton Phn loi nh phn (Binary Classification) vi u ra l 0 (Khng) hoc 1 (C). o C ch k thut: Logistic Regression l thut ton c bn v hiu qu nht cho bi ton phn loi nh phn. N s dng hm Sigmoid  chuyn i u ra thnh xc sut (t 0 n 1) cho vic ra quyt nh Yes/No. Ti sao khng chn A (Anomaly detection): o Anomaly detection (Pht hin bt thng) c dng  tm ra cc im d liu him gp hoc khc bit so vi phn cn li (v d: pht hin gian ln th tn dng). N khng dng  phn loi hai nhm khch hng c tnh cht r rng nh nhau. Ti sao khng chn B (Linear regression): o Linear Regression (Hi quy tuyn tnh) dng  d on gi tr lin tc (Continuous values) nh gi nh, nhit , doanh thu. u ra ca n l mt con s v hn ($-\\infty$ n $+\\infty$), khng ph hp  tr li cu hi dng Yes/No. Ti sao khng chn D (Semantic segmentation): o Semantic Segmentation l mt k thut trong Computer Vision (Th gic my tnh) dng  phn loi tng pixel trong mt bc nh (v d: u l ng, u l xe). N hon ton khng lin quan n d liu dng bng (tabular data) v khch hng."
  },
  {
    "id": 127,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer has developed a binary classification model outside of Amazon SageMaker. The ML engineer needs to make the model accessible to a SageMaker Canvas user for additional tuning. The model artifacts are stored in an Amazon S3 bucket. The ML engineer and the Canvas user are part of the same SageMaker domain. Which combination of requirements must be met so that the ML engineer can share the model with the Canvas user? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "The ML engineer and the Canvas user must be in separate SageMaker domains."
      },
      {
        "id": "B",
        "text": "The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored."
      },
      {
        "id": "C",
        "text": "The model must be registered in the SageMaker Model Registry."
      },
      {
        "id": "D",
        "text": "The ML engineer must host the model on AWS Marketplace."
      },
      {
        "id": "E",
        "text": "The ML engineer must deploy the model to a SageMaker endpoint."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (S3 Permissions): o Quyn truy cp vt l: Model artifacts (file model.tar.gz, weights, code) ang nm trong Amazon S3.  SageMaker Canvas (v ngi dng Canvas) c th load c model ny ln  tip tc fine-tune, role IAM ca ngi dng Canvas bt buc phi c quyn s3:GetObject i vi bucket cha artifacts. Nu khng c quyn ny, qu trnh import s tht bi ngay lp tc do \"Access Denied\". Ti sao chn C (SageMaker Model Registry): o C ch tch hp (The Bridge): SageMaker Canvas c thit k  tch hp cht ch vi SageMaker Model Registry.  mang mt model c train bn ngoi (outside SageMaker) vo trong Canvas, quy trnh chun l bn phi ng gi v ng k model  vo Model Registry (to Model Package Group). o Sau khi ng k, ngi dng Canvas c th d dng nhn thy model trong giao din Canvas v chn \"Import\"  tinh chnh tip. y l cu ni qun l phin bn v metadata cn thit. Ti sao khng chn A (Separate SageMaker domains): o y khng phi l yu cu k thut. Thc t, vic hai ngi dng nm trong cng mt Domain (nh  bi  nu) cng thun li cho vic chia s ti nguyn v profile. Vic tch ra separate domains s lm phc tp ha vn  permission (cross-account/cross-domain access) ch khng gip ch g cho vic share model. Ti sao khng chn D (AWS Marketplace): o AWS Marketplace dng  bn hoc chia s model cng khai cho cng ng/khch hng bn ngoi. Vic chia s ni b gia Engineer v Canvas user trong cng cng ty khng cn thng qua Marketplace. Ti sao khng chn E (Deploy to endpoint): o Sai mc ch: Endpoint dng  phc v suy lun (Inference/Prediction). Ngi dng Canvas cn thc hin \"additional tuning\" (tc l hun luyn li/fine-tuning).  tune model, Canvas cn truy cp vo model artifacts (file gc) ch khng phi gi API vo mt endpoint ang chy."
  },
  {
    "id": 128,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is building a deep learning model on Amazon SageMaker. The company uses a large amount of data as the training dataset. The company needs to optimize the model's hyperparameters to minimize the loss function on the validation dataset. Which hyperparameter tuning strategy will accomplish this goal with the LEAST computation time?",
    "options": [
      {
        "id": "A",
        "text": "Hyperband"
      },
      {
        "id": "B",
        "text": "Grid search"
      },
      {
        "id": "C",
        "text": "Bayesian optimization"
      },
      {
        "id": "D",
        "text": "Random search"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Hyperband): o C ch Early Stopping (Dng sm): Hyperband l mt chin lc ti u ha siu tham s (Hyperparameter Optimization - HPO) tin tin, c thit k c bit  gim thiu thi gian tnh ton. o T duy k thut: Thay v chy tt c cc th nghim (training jobs) n khi hon thnh (full epochs) nh cc phng php truyn thng, Hyperband bt u vi nhiu t hp tham s ngu nhin nhng ch train chng trong mt vi epoch ngn (budget nh). Sau , n nh gi hiu nng, loi b ngay lp tc mt na s model km hiu qu (pruning) v ch cp thm ti nguyn (epochs) cho cc model tt nht  chy tip. o Kt qu: Nh vic khng lng ph ti nguyn tnh ton vo cc model \"v vng\", Hyperband c th tm ra model ti u nhanh hn gp nhiu ln (trn AWS SageMaker nhanh hn ti 3 ln) so vi Bayesian Optimization trn cc tp d liu ln. Ti sao khng chn B (Grid search): o Duyt ton b (Exhaustive): Grid search th nghim tt c cc t hp c th. Vi Deep Learning, khng gian tham s rt ln, Grid Search s gy ra bng n t hp (combinatorial explosion), dn n thi gian tnh ton cc k ln, khng th chp nhn c. Ti sao khng chn C (Bayesian optimization): o Chy tun t: Mc d Bayesian Optimization \"thng minh\" hn Random Search v n hc t cc kt qu trc   on tham s tip theo, nhng theo mc nh, n thng chy cc training job n khi hon thnh (to completion)  ly kt qu chnh xc. iu ny khin n chm hn Hyperband trong bi cnh Deep Learning tn km, ni vic pht hin sm v loi b model xu quan trng hn l train k tng model. Ti sao khng chn D (Random search): o May ri: Random search chn ngu nhin v cng thng chy ht thi gian training cho mi job. N khng c c ch qun l ti nguyn thng minh  dng sm cc job km nh Hyperband."
  },
  {
    "id": 129,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to use Amazon Redshift ML in its primary AWS account. The source data is in an Amazon S3 bucket in a secondary account. An ML engineer needs to set up an ML pipeline in the primary account to access the S3 bucket in the secondary account. The solution must not require public IPv4 addresses. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Provision a Redshift cluster and Amazon SageMaker Studio in a VPC with no public access enabled in the primary account. Create a VPC peering connection between the accounts. Update the VPC route tables to remove the route to 0.0.0.0/0."
      },
      {
        "id": "B",
        "text": "Provision a Redshift cluster and Amazon SageMaker Studio in a VPC with no public access enabled in the primary account. Create an AWS Direct Connect connection and a transit gateway. Associate the VPCs from both accounts with the transit gateway. Update the VPC route tables to remove the route to 0.0.0.0/0."
      },
      {
        "id": "C",
        "text": "Provision a Redshift cluster and Amazon SageMaker Studio in a VPC in the primary account. Create an AWS Site-to-Site VPN connection with two encrypted IPsec tunnels between the accounts. Set up interface VPC endpoints for Amazon S3."
      },
      {
        "id": "D",
        "text": "Provision a Redshift cluster and Amazon SageMaker Studio in a VPC in the primary account. Create an S3 gateway endpoint. Update the S3 bucket policy to allow IAM principals from the primary account. Set up interface VPC endpoints for SageMaker and Amazon Redshift."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (S3 Gateway Endpoint + Cross-account Policy): o Kt ni ring t ti S3 (Private Connectivity):  truy cp Amazon S3 t mt VPC m khng i qua Internet cng cng (Public IPv4), gii php chun l s dng VPC Gateway Endpoint cho S3. Gateway Endpoint s thm mt route vo bng nh tuyn (Route Table) ca VPC, hng ton b lu lng S3 i qua mng ni b ca AWS. o Truy cp cho ti khon (Cross-account Access): V d liu nm  ti khon ph (Secondary account), bn cn cp nht S3 Bucket Policy  ti khon ph  cp quyn (Allow) cho IAM Principal (Role m Redshift/SageMaker s dng)  ti khon chnh (Primary account). o Interface Endpoints: Cn thit lp Interface VPC Endpoints cho SageMaker API v Redshift API  cc dch v ny c th giao tip vi Control Plane ca AWS mt cch ring t m khng cn NAT Gateway hay Internet Gateway. Ti sao khng chn A (VPC Peering): o VPC Peering dng  kt ni 2 VPC vi nhau. Tuy nhin, S3 l dch v nm ngoi VPC (Regional Service). Vic peering 2 VPC khng gii quyt trc tip vn  truy cp S3 mt cch hiu qu. S3 Gateway Endpoint ti VPC ngun l cch trc tip nht. Ngoi ra, VPC Peering khng thay th c nhu cu v S3 Bucket Policy  cp quyn truy cp d liu. Ti sao khng chn B (Direct Connect + Transit Gateway): o Overkill v sai mc ch: AWS Direct Connect dng  kt ni trung tm d liu on-premise vi AWS. Transit Gateway dng  kt ni nhiu VPC. C hai u l gii php h tng mng phc tp v tn km, khng cn thit cho vic truy cp S3 (vn ch cn Gateway Endpoint). Ti sao khng chn C (Site-to-Site VPN): o Site-to-Site VPN dng  to kt ni bo mt gia mng on-premise v AWS VPC qua internet. N khng dng  kt ni hai ti khon AWS vi nhau  truy cp S3. Vic nh tuyn traffic S3 qua VPN tunnel l khng kh thi v hiu nng rt thp so vi Gateway Endpoint."
  },
  {
    "id": 130,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using an AWS Lambda function to monitor the metrics from an ML model. An ML engineer needs to implement a solution to send an email message when the metrics breach a threshold. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Log the metrics from the Lambda function to AWS CloudTrail. Configure a CloudTrail trail to send the email message."
      },
      {
        "id": "B",
        "text": "Log the metrics from the Lambda function to Amazon CloudFront. Configure an Amazon CloudWatch alarm to send the email message."
      },
      {
        "id": "C",
        "text": "Log the metrics from the Lambda function to Amazon CloudWatch. Configure a CloudWatch alarm to send the email message."
      },
      {
        "id": "D",
        "text": "Log the metrics from the Lambda function to Amazon CloudWatch. Configure an Amazon CloudFront rule to send the email message."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (CloudWatch + Alarm): o Kin trc tiu chun: y l m hnh gim st v cnh bo (Monitoring & Alerting) kinh in trn AWS. o Quy trnh: . Lambda function gi metrics (v d: model accuracy, latency) ln Amazon CloudWatch Metrics (thng qua API PutMetricData hoc CloudWatch Logs). . CloudWatch Alarm s lin tc theo di metric ny. . Khi gi tr metric vt qu ngng (threshold), Alarm s chuyn sang trng thi \"ALARM\" v kch hot hnh ng gi thng bo (thng l qua Amazon SNS topic, t  gi email n ngi dng). Ti sao khng chn A (CloudTrail): o Sai chc nng: AWS CloudTrail c dng  ghi li lch s gi API (Audit logs) nhm mc ch bo mt v tun th (compliance). N khng c thit k  lu tr v v biu  cho cc ch s hiu nng (performance metrics) dng s hc. Ti sao khng chn B (CloudFront): o Sai dch v: Amazon CloudFront l dch v CDN (Content Delivery Network) dng  phn phi ni dung tnh/ng ti ngi dng cui vi  tr thp. N hon ton khng c chc nng nhn log metrics t Lambda hay gi email cnh bo. Ti sao khng chn D (CloudFront rule): o Tng t B, CloudFront khng lin quan n quy trnh monitoring backend."
  },
  {
    "id": 131,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has used Amazon SageMaker to deploy a predictive ML model in production. The company is using SageMaker Model Monitor on the model. After a model update, an ML engineer notices data quality issues in the Model Monitor checks. What should the ML engineer do to mitigate the data quality issues that Model Monitor has identified?",
    "options": [
      {
        "id": "A",
        "text": "Adjust the model's parameters and hyperparameters."
      },
      {
        "id": "B",
        "text": "Initiate a manual Model Monitor job that uses the most recent production data."
      },
      {
        "id": "C",
        "text": "Create a new baseline from the latest dataset. Update Model Monitor to use the new baseline for evaluations."
      },
      {
        "id": "D",
        "text": "Include additional data in the existing training set for the model. Retrain and redeploy the model."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Create a new baseline): Trong Amazon SageMaker Model Monitor, chc nng Data Quality Monitor hot ng bng cch so snh d liu thc t (live inference data) vi mt tp d liu tham chiu, c gi l Baseline (thng c to ra t tp d liu hun luyn - training dataset).  bi nu r vn  xut hin \"After a model update\" (Sau khi cp nht model). Khi mt model mi c deploy, n thng c hun luyn trn d liu mi hoc c s thay i v phn phi d liu (data distribution). Nu bn khng cp nht Baseline, Model Monitor s tip tc so snh d liu mi vi Baseline c (ca phin bn model trc ). S sai lch ny s dn n vic Model Monitor bo co sai cc vn  v cht lng d liu (False Positives). Do , hnh ng chnh xc l phi to mt baseline mi t tp d liu mi nht (c dng  train model hin ti) v cp nht cu hnh Model Monitor  phn nh ng thc t \"bnh thng mi\". Ti sao khng chn A (Adjust parameters/hyperparameters): Vic iu chnh tham s (parameters) hoc siu tham s (hyperparameters) ch nh hng n hiu sut v hnh vi d on ca model (model performance). N khng thay i cc quy tc (rules) hoc ngng thng k (statistics) m Model Monitor s dng  xc nh \"cht lng d liu\". Nu Baseline khng i, li vn s c bo co bt k model hot ng th no. Ti sao khng chn B (Initiate a manual job): Chy li job kim tra (Model Monitor job) mt cch th cng m khng thay i cu hnh c bn (Baseline) s ch cho ra kt qu y ht nh c. Cng c vn s dng b quy tc c  so snh vi d liu mi v li tip tc bo li. y l hnh ng lng ph ti nguyn tnh ton m khng gii quyt c nguyn nhn gc r. Ti sao khng chn D (Include additional data ... Retrain):  bi cho bit model va mi c cp nht (\"After a model update\"). Vic ngay lp tc thu thp thm d liu v retrain li model l khng cn thit v i ngc li quy trnh logic, tr khi model  thc s b l..."
  },
  {
    "id": 132,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Topic #: 1 [All AWS Certified Machine Learning Engineer - Associate MLA-C01 Questions] A company has an ML model that generates text descriptions based on images that customers upload to the company's website. The images can be up to 50 MB in total size. An ML engineer decides to store the images in an Amazon S3 bucket. The ML engineer must implement a processing solution that can scale to accommodate changes in demand. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create an Amazon SageMaker batch transform job to process all the images in the S3 bucket."
      },
      {
        "id": "B",
        "text": "Create an Amazon SageMaker Asynchronous Inference endpoint and a scaling policy. Run a script to make an inference request for each image."
      },
      {
        "id": "C",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Karpenter for auto scaling. Host the model on the EKS cluster. Run a script to make an inference request for each image."
      },
      {
        "id": "D",
        "text": "Create an AWS Batch job that uses an Amazon Elastic Container Service (Amazon ECS) cluster. Specify a list of images to process for each AWS Batch job."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (SageMaker Asynchronous Inference):  bi t ra hai thch thc chnh: kch thc payload ln (nh ln n 50 MB) v yu cu gim thiu gnh nng vn hnh (Least operational overhead) cho mt gii php c kh nng m rng (scale). SageMaker Asynchronous Inference l dch v hon ho cho trng hp ny v: . H tr Large Payload: Khng ging nh Real-time Inference (gii hn payload nh, thng < 6MB), Asynchronous Inference cho php x l payload ln ti 1 GB bng cch lu tr input/output trn Amazon S3, gii quyt trc tip vn  nh 50 MB. . Managed Auto-scaling (Scale to Zero): Dch v ny c hng i (queue) tch hp sn v kh nng t ng scale s lng instance da trn lng request trong hng i, thm ch scale xung 0 khi khng c request  tit kim chi ph. iu ny gip loi b gnh nng qun l h tng th cng. Ti sao khng chn A (SageMaker Batch Transform): Batch Transform ph hp cho vic x l offline hng lot d liu ln  c sn (v d: chy job mi m), khng ph hp cho m hnh tng tc trn website ni khch hng upload v cn kt qu trong thi gian hp l (near real-time). Batch Transform khng phi l mt endpoint lun sn sng phc v request n l ngay lp tc. Ti sao khng chn C (Amazon EKS + Karpenter): Phng n ny vi phm nghim trng tiu ch \"Least operational overhead\". Vic dng v qun l mt cm Kubernetes (EKS), ci t v cu hnh Karpenter  autoscaling, cng nh qun l container pods i hi n lc k thut v bo tr cc ln so vi vic s dng dch v managed c sn nh SageMaker. Ti sao khng chn D (AWS Batch on ECS): Tng t nh phng n A, AWS Batch c thit k cho cc tc v tnh ton hng lot (batch computing) thay v phc v suy lun m hnh theo hng s kin (event-driven inference). Vic cu hnh Job Queues, Compute Environments v Job Definitions trn ECS phc tp hn nhiu so vi vic ch cn deploy mt endpoint Asynchronous trn SageMaker."
  },
  {
    "id": 133,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use AWS services to identify and extract meaningful unique keywords from documents. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use the Natural Language Toolkit (NLTK) library on Amazon EC2 instances for text pre-processing. Use the Latent Dirichlet Allocation (LDA) algorithm to identify and extract relevant keywords."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker and the BlazingText algorithm. Apply custom pre- processing steps for stemming and removal of stop words. Calculate term frequency-inverse document frequency (TF-IDF) scores to identify and extract relevant keywords."
      },
      {
        "id": "C",
        "text": "Store the documents in an Amazon S3 bucket. Create AWS Lambda functions to process the documents and to run Python scripts for stemming and removal of stop words. Use bigram and trigram techniques to identify and extract relevant keywords."
      },
      {
        "id": "D",
        "text": "Use Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Amazon Comprehend): Cu hi yu cu gii php c \"chi ph vn hnh thp nht\" (LEAST operational overhead)  trch xut t kha (keywords). Amazon Comprehend l mt dch v AI cao cp (AI Service) v X l Ngn ng T nhin (NLP) c qun l hon ton (Serverless/Fully Managed). N cung cp sn cc API (Pre-trained models) cho tnh nng Key Phrase Extraction (Trch xut cm t kha) v Entity Recognition (Nhn din thc th) ngay lp tc m khng cn ngi dng phi qun l h tng, vit code x l d liu phc tp hay hun luyn m hnh. y l la chn \"m n lin\" ti u nht. Ti sao khng chn A (EC2 + NLTK + LDA): y l phng n t qun l (Self- managed). Bn phi chu trch nhim hon ton v vic qun l my ch (EC2), ci t mi trng, th vin (NLTK) v t vit thut ton (LDA). iu ny vi phm tiu ch \"Least operational overhead\". Ti sao khng chn B (SageMaker + BlazingText): Mc d SageMaker gip qun l vic hun luyn, nhng bn vn phi thc hin cc bc: Tin x l d liu (Pre- processing), Hun luyn (Training), v Trin khai (Deploying). Quy trnh ny tn nhiu cng sc v thi gian hn rt nhiu so vi vic gi API ca Comprehend. Ti sao khng chn C (Lambda + Custom Scripts): Tng t nh phng n A, bn phi t vit code th cng (hard-code) cc k thut x l ngn ng (stemming, bigram/trigram) bn trong Lambda. Bn khng tn dng c cc m hnh Deep Learning mnh m  c AWS ti u sn."
  },
  {
    "id": 134,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to give its ML engineers appropriate access to training data. The ML engineers must access training data from only their own business group. The ML engineers must not be allowed to access training data from other business groups. The company uses a single AWS account and stores all the training data in Amazon S3 buckets. All ML model training occurs in Amazon SageMaker. Which solution will provide the ML engineers with the appropriate access?",
    "options": [
      {
        "id": "A",
        "text": "Enable S3 bucket versioning."
      },
      {
        "id": "B",
        "text": "Configure S3 Object Lock settings for each user."
      },
      {
        "id": "C",
        "text": "Add cross-origin resource sharing (CORS) policies to the S3 buckets."
      },
      {
        "id": "D",
        "text": "Create IAM policies. Attach the policies to IAM users or IAM roles."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (IAM policies): Bi ton yu cu kim sot quyn truy cp (Access Control) mt cch chi tit: K s ca nhm no ch c truy cp d liu (S3) ca nhm . Trong AWS, IAM Policies (Chnh sch qun l truy cp) l c ch ct li  nh ngha ai (Principal) c lm g (Action) trn ti nguyn no (Resource). Bn c th to cc chnh sch ring bit (v d: Policy_GroupA ch cho php s3:GetObject trn bucket/group-a/) v gn chng vo cc IAM Users hoc Roles tng ng ca tng nhm k s. y l gii php tiu chun v chnh xc nht  phn quyn. Ti sao khng chn A (S3 bucket versioning): Versioning (nh phin bn) l tnh nng gip lu gi nhiu phin bn ca mt object, mc ch chnh l  phc hi d liu khi b xa nhm hoc ghi  (Data Protection/Recovery). N khng c chc nng ngn chn ngi dng truy cp vo d liu. Ti sao khng chn B (S3 Object Lock): Object Lock c s dng cho m hnh WORM (Write Once, Read Many), ngn chn vic xa hoc sa i object trong mt khong thi gian nht nh (thng dng cho tun th php l - Compliance). N khng dng  phn chia quyn xem d liu gia cc nhm. Ti sao khng chn C (CORS policies): Cross-Origin Resource Sharing (CORS) l c ch bo mt cho trnh duyt web, cho php mt ng dng web  domain ny truy cp ti nguyn  domain khc. N khng lin quan n vic cp quyn cho backend service (SageMaker) hay ngi dng (ML Engineers) truy cp vo d liu trong S3."
  },
  {
    "id": 135,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to host a custom ML model to perform forecast analysis. The forecast analysis will occur with predictable and sustained load during the same 2-hour period every day. Multiple invocations during the analysis period will require quick responses. The company needs AWS to manage the underlying infrastructure and any auto scaling activities. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Schedule an Amazon SageMaker batch transform job by using AWS Lambda."
      },
      {
        "id": "B",
        "text": "Configure an Auto Scaling group of Amazon EC2 instances to use scheduled scaling."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Serverless Inference with provisioned concurrency."
      },
      {
        "id": "D",
        "text": "Run the model on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2 with pod auto scaling."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Serverless Inference): Bi ton t ra mt m hnh ti (traffic pattern) rt c th: ch hot ng trong 2 gi mi ngy v khng hot ng trong 22 gi cn li. . Cost & Ops Efficiency: Nu s dng Real-time Endpoint thng thng, bn s phi tr tin cho 24/24 gi (hoc phi vit script phc tp  to/xa endpoint mi ngy). SageMaker Serverless Inference l gii php ti u nht v n c kh nng Scale to Zero (t ng tt hon ton khi khng c request), gip tit kim chi ph tuyt i trong thi gian nhn ri m khng cn qun l h tng th cng. . Handling Load: Vi yu cu \"quick responses\" (phn hi nhanh) cho \"multiple invocations\" (nhiu lt gi), y l c im ca m hnh API Endpoint ch khng phi x l Batch. Mc d thut ng \"Provisioned Concurrency\" thng gn lin vi Lambda hoc SageMaker Real-time, trong ng cnh cu hi ny, n m ch vic cu hnh kh nng x l ng thi (concurrency)  m bo endpoint chu c ti \"sustained\" (lin tc) trong 2 gi cao im m khng b nghn. Ti sao khng chn A (SageMaker batch transform): Batch Transform c thit k  x l offline cc tp d liu ln (bulk processing). Ngi dng upload file d liu ln S3, ch x l xong v nhn kt qu tr v S3. N hot ng theo c ch bt ng b (asynchronous) v khng cung cp \"quick responses\" (phn hi tc th) cho tng invocation ring l theo thi gian thc nh yu cu  bi. Ti sao khng chn B (EC2 Auto Scaling): S dng EC2 yu cu bn phi t qun l h iu hnh (OS), ci t mi trng (Docker/Python), cu hnh Load Balancer v Auto Scaling Group. iu ny vi phm yu cu quan trng ca  bi l \"AWS to manage the underlying infrastructure\" (AWS qun l h tng c s). y l gii php c chi ph vn hnh (Operational Overhead) cao nht. Ti sao khng chn D (EKS Cluster): Tng t nh EC2, vic vn hnh mt cm Kubernetes (EKS) i hi kin thc chuyn su v container orchestration v n lc qun tr rt ln (qun..."
  },
  {
    "id": 136,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company's ML engineer has deployed an ML model for sentiment analysis to an Amazon SageMaker endpoint. The ML engineer needs to explain to company stakeholders how the model makes predictions. Which solution will provide an explanation for the model's predictions?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker Model Monitor on the deployed model."
      },
      {
        "id": "B",
        "text": "Use SageMaker Clarify on the deployed model."
      },
      {
        "id": "C",
        "text": "Show the distribution of inferences from A/ testing in Amazon CloudWatch."
      },
      {
        "id": "D",
        "text": "Add a shadow endpoint. Analyze prediction differences on samples."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (SageMaker Clarify): Amazon SageMaker Clarify l dch v chuyn bit c thit k  cung cp kh nng gii thch m hnh (Model Explainability) v pht hin thin kin (Bias Detection). N s dng phng php SHAP (SHapley Additive exPlanations)  phn tch v nh lng mc  ng gp ca tng c trng (feature) vo kt qu d on cui cng. i vi bi ton phn tch cm xc (Sentiment Analysis), Clarify s ch ra nhng t ng hoc cm t no (feature importance)  khin model a ra kt qu l \"Tch cc\" hay \"Tiu cc\", gip cc bn lin quan (stakeholders) hiu r c ch ra quyt nh ca model. Ti sao khng chn A (SageMaker Model Monitor): SageMaker Model Monitor c nhim v chnh l gim st lin tc cht lng ca model v d liu theo thi gian thc (v d: pht hin Data Drift, Model Quality Drift). Mc d n c th theo di s thay i v Feature Attribution (s dng cng ngh ca Clarify bn di), nhng khi nhc n nhu cu ct li l \"gii thch model hot ng th no\" cho con ngi hiu, SageMaker Clarify l cng c trc tip v chnh xc nht. Ti sao khng chn C (A/B testing in CloudWatch): A/B Testing c s dng  so snh hiu nng (performance) gia hai phin bn model khc nhau (v d: Model A chnh xc hn Model B bao nhiu %). N ch cho bit model no tt hn, ch khng gii thch c ti sao model li a ra mt d on c th. Ti sao khng chn D (Shadow endpoint): Shadow testing (th nghim bng) l k thut trin khai model mi  nhn traffic thc t nhng khng tr kt qu cho ngi dng (ch  log li v so snh). Mc ch ca n l kim tra  n nh v li ca model mi trc khi promote ln production, khng lin quan n vic gii thch logic bn trong model."
  },
  {
    "id": 137,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is using Amazon SageMaker to train a deep learning model that requires distributed training. After some training attempts, the ML engineer observes that the instances are not performing as expected. The ML engineer identifies communication overhead between the training instances. What should the ML engineer do to MINIMIZE the communication overhead between the instances?",
    "options": [
      {
        "id": "A",
        "text": "Place the instances in the same VPC subnet. Store the data in a different AWS Region from where the instances are deployed."
      },
      {
        "id": "B",
        "text": "Place the instances in the same VPC subnet but in different Availability Zones. Store the data in a different AWS Region from where the instances are deployed."
      },
      {
        "id": "C",
        "text": "Place the instances in the same VPC subnet. Store the data in the same AWS Region and Availability Zone where the instances are deployed."
      },
      {
        "id": "D",
        "text": "Place the instances in the same VPC subnet. Store the data in the same AWS Region but in a different Availability Zone from where the instances are deployed."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Same Region & Availability Zone): Trong hun luyn phn tn (Distributed Training), cc instance (nt tnh ton) phi thng xuyn trao i thng tin (nh gradients trong Data Parallelism) vi nhau. \"Communication overhead\" ( tr mng) l k th ln nht lm chm qu trnh ny.  gim thiu  tr xung mc thp nht, gii php ti u l t tt c ti nguyn (Compute Instances v Data Storage) gn nhau nht c th v mt vt l. . Network Latency: Mt Subnet trong AWS lun nm trn trong mt Availability Zone (AZ). Vic t cc instance trong cng mt subnet m bo chng nm trong cng mt AZ (thm ch cng data center), gip lu lng mng i qua ng truyn tc  cao ni b (intra-AZ), thay v phi i vng qua cc ng truyn lin kt gia cc AZ (inter-AZ) hoc gia cc Region. . Data Proximity: t d liu (S3/EFS/FSx) cng Region v AZ vi instance gip gim thi gian ti d liu (I/O latency), trnh nghn c chai khi model c d liu hun luyn. Ti sao khng chn A (Data in different AWS Region): Vic lu tr d liu  mt Region khc vi ni t instance s to ra  tr mng cc ln (hng chc n hng trm ms) khi ti d liu. iu ny s khin GPU/CPU phi ch i (idle) trong khi d liu c truyn qua internet, lm gim hiu sut nghim trng. Ti sao khng chn B (Different Availability Zones): V mt k thut AWS, mt Subnet khng th ko di qua nhiu AZ (1 Subnet = 1 AZ). Do , mnh  \"same VPC subnet but in different Availability Zones\" l khng kh thi. Hn na, giao tip gia cc AZ (Cross-AZ traffic) lun c  tr cao hn so vi giao tip trong cng mt AZ. Ti sao khng chn D (Data in different Availability Zone): Mc d tt hn vic khc Region, nhng vic truy cp d liu t mt AZ khc vn phi chu ph truyn ti d liu (Cross-AZ data transfer cost) v  tr cao hn so vi vic d liu nm ngay ti local AZ."
  },
  {
    "id": 138,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is running ML models on premises by using custom Python scripts and proprietary datasets. The company is using PyTorch. The model building requires unique domain knowledge. The company needs to move the models to AWS. Which solution will meet these requirements with the LEAST effort?",
    "options": [
      {
        "id": "A",
        "text": "Use SageMaker built-in algorithms to train the proprietary datasets."
      },
      {
        "id": "B",
        "text": "Use SageMaker script mode and premade images for ML frameworks."
      },
      {
        "id": "C",
        "text": "Build a container on AWS that includes custom packages and a choice of ML frameworks."
      },
      {
        "id": "D",
        "text": "Purchase similar production models through AWS Marketplace."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (SageMaker Script Mode): y l gii php tn t cng sc nht (LEAST effort)  di chuyn cc on m (code) hun luyn m hnh t on- premises ln AWS m vn gi nguyn c logic ty chnh. SageMaker Script Mode cho php bn ti s dng trc tip cc file train.py (custom Python scripts) hin c. Bn khng cn phi ng gi Docker container t u. Thay vo , bn ch cn ch nh framework ( y l PyTorch) v AWS s t ng cung cp mt Premade Image (Container  ci sn PyTorch v cc th vin cn thit). Bn ch vic \"mang code n\" v chy, gip gim thiu ti a vic thay i m ngun hay cu hnh h tng. Ti sao khng chn A (SageMaker built-in algorithms): Cc thut ton tch hp sn (Built-in algorithms) ca SageMaker (nh XGBoost, Image Classification...) l cc hp en (black-box) c ti u ha cho cc tc v c th. V  bi ni m hnh yu cu \"unique domain knowledge\" v c vit bng custom scripts, vic p logic phc tp ny vo cc thut ton c sn thng khng kh thi hoc i hi phi vit li ton b logic x l d liu v hun luyn, tn rt nhiu cng sc. Ti sao khng chn C (Build a container - BYOC): Phng php \"Bring Your Own Container\" (BYOC) yu cu bn phi t vit Dockerfile, ci t mi trng, tun th cu trc th mc ca SageMaker (/opt/ml/...), build image v push ln Amazon ECR. y l quy trnh phc tp v tn cng sc vn hnh hn rt nhiu so vi vic s dng Script Mode (ni AWS  lo phn Container cho bn). Ti sao khng chn D (AWS Marketplace): M hnh ca cng ty cha \"unique domain knowledge\" (kin thc nghip v c quyn/c th). Cc m hnh bn sn trn Marketplace l cc gii php tng qut, khng th cha tri thc ring bit ca cng ty bn c. Do , phng n ny khng p ng c yu cu nghip v."
  },
  {
    "id": 139,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using Amazon SageMaker and millions of files to train an ML model. Each file is several megabytes in size. The files are stored in an Amazon S3 bucket. The company needs to improve training performance. Which solution will meet these requirements in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Transfer the data to a new S3 bucket that provides S3 Express One Zone storage. Adjust the training job to use the new S3 bucket."
      },
      {
        "id": "B",
        "text": "Create an Amazon FSx for Lustre file system. Link the file system to the existing S3 bucket. Adjust the training job to read from the file system."
      },
      {
        "id": "C",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Transfer the existing data to the file system. Adjust the training job to read from the file system."
      },
      {
        "id": "D",
        "text": "Create an Amazon ElastiCache (Redis OSS) cluster. Link the Redis OSS cluster to the existing S3 bucket. Stream the data from the Redis OSS cluster directly to the"
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (FSx for Lustre): Bi ton yu cu ci thin hiu sut hun luyn (improve training performance) cho hng triu file c kch thc vi MB (y l workload yu cu thng lng/throughput cc cao v  tr/latency thp  np d liu vo GPU/CPU). Amazon FSx for Lustre l h thng file hiu nng cao c thit k c bit cho cc ng dng tnh ton chuyn su (HPC) v Machine Learning. Tnh nng \"Data Repository Association\" cho php FSx for Lustre lin kt trc tip vi S3 bucket hin c. Khi training job chy, d liu s c \"lazy load\" (ti khi cn) hoc \"preload\" (ti trc) vo h thng file tc  cao ny, gip tng tc  c d liu ln gp nhiu ln so vi c trc tip t S3 (c bit l vi File Mode). y l gii php chun cng nghip  tng tc training. Ti sao khng chn A (S3 Express One Zone): Mc d S3 Express One Zone c  tr thp hn S3 Standard, nhng i vi cc tc v ML training quy m ln cn throughput cc i, FSx for Lustre vn vt tri hn nh kh nng caching v kin trc file system song song (parallel file system). Hn na, vic copy ton b d liu sang bucket mi (vng One Zone) s tn thi gian v cng sc qun l hn so vi vic ch cn \"link\" FSx vo bucket hin ti. Ti sao khng chn C (Amazon EFS): Amazon EFS l h thng file dng chung (General Purpose) cho Linux, ph hp vi cc ng dng web hoc chia s d liu thng thng. Mc d EFS c th dng cho SageMaker, nhng throughput v IOPS ca n thp hn ng k so vi FSx for Lustre trong cc tc v hun luyn Deep Learning yu cu bng thng d liu ln. Ti sao khng chn D (ElastiCache Redis): Redis l in-memory key-value store, khng phi l File System. SageMaker Training Job thng mong i d liu u vo di dng File System (Local, EFS, FSx) hoc S3 Object, ch khng h tr c trc tip stream d liu t Redis mt cch t nhin (native integration) cho vic hun luyn quy m ln. Vic thit lp c ch ny s cc k phc tp (cn custom data load..."
  },
  {
    "id": 140,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to develop an ML model by using tabular data from its customers. The data contains meaningful ordered features with sensitive information that should not be discarded. An ML engineer must ensure that the sensitive data is masked before another team starts to build the model. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Made to categorize the sensitive data."
      },
      {
        "id": "B",
        "text": "Prepare the data by using AWS Glue DataBrew."
      },
      {
        "id": "C",
        "text": "Run an AWS Batch job to change the sensitive data to random values."
      },
      {
        "id": "D",
        "text": "Run an Amazon EMR job to change the sensitive data to random values."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (AWS Glue DataBrew): Bi ton yu cu chun b d liu dng bng (tabular data), c th l lm sch (masking) thng tin nhy cm trc khi a cho i ng khc hun luyn model. AWS Glue DataBrew l cng c chun b d liu trc quan (visual data preparation tool) khng cn vit code (no-code). N cung cp sn cc tnh nng chuyn i (transformations) chuyn bit  x l thng tin c nhn (PII), bao gm cc k thut nh Masking (che du), Hashing (bm), hoc Shuffling (tro i). iu ny gip bo v d liu nhy cm nhng vn gi li cu trc hoc nh dng cn thit cho vic hun luyn Machine Learning, p ng chnh xc yu cu  bi. Ti sao khng chn A (Amazon Macie): Amazon Macie l dch v bo mt dng  khm ph (discover) v phn loi (classify) d liu nhy cm trong S3. Macie ch bo co cho bn bit \"D liu nhy cm nm  u\", ch khng c chc nng thc hin hnh ng bin i (transform) hoc che giu d liu   to ra tp d liu mi sch hn. Ti sao khng chn C (AWS Batch): Vic s dng AWS Batch yu cu bn phi t vit code (Python/Shell)  x l d liu. Hn na, phng n  xut thay th d liu nhy cm bng \"random values\" (gi tr ngu nhin) l mt k thut ti trong ML v n ph v mi tng quan v tnh quy lut (\"meaningful ordered features\") ca d liu, khin model khng th hc c g t cc c trng  (noise). Ti sao khng chn D (Amazon EMR): Tng t nh AWS Batch, EMR i hi chi ph vn hnh cao (qun l cluster) v k nng vit code Spark/Hadoop. Vic thay th bng gi tr ngu nhin cng mc phi li logic tng t nh phng n C, lm gim gi tr s dng ca d liu i vi model."
  },
  {
    "id": 141,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to deploy ML models to get inferences from large datasets in an asynchronous manner. The ML engineer also needs to implement scheduled monitoring of the data quality of the models. The ML engineer must receive alerts when changes in data quality occur. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the models by using scheduled AWS Glue jobs. Use Amazon CloudWatch alarms to monitor the data quality and to send alerts."
      },
      {
        "id": "B",
        "text": "Deploy the models by using scheduled AWS Batch jobs. Use AWS CloudTrail to monitor the data quality and to send alerts."
      },
      {
        "id": "C",
        "text": "Deploy the models by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to monitor the data quality and to send alerts."
      },
      {
        "id": "D",
        "text": "Deploy the models by using Amazon SageMaker batch transform. Use SageMaker Model Monitor to monitor the data quality and to send alerts."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Batch Transform + Model Monitor): Cu hi c hai yu cu chnh: x l suy lun (inference) cho tp d liu ln theo phng thc bt ng b (asynchronous) v gim st cht lng d liu. . Asynchronous Inference: SageMaker Batch Transform l tnh nng chuyn dng  x l suy lun offline cho hng lot d liu ln (Bulk data) c lu tr trn S3. N hot ng bt ng b, t ng qun l ti nguyn tnh ton v tt i khi hon thnh, rt ph hp cho \"large datasets\". . Data Quality Monitoring: SageMaker Model Monitor l dch v chun  pht hin cc vn  v cht lng d liu (Data Quality Drift) so vi Baseline. Model Monitor h tr gim st cc Batch Transform jobs bng cch phn tch d liu u vo/u ra c lu trn S3 theo lch trnh nh k v t ng y cc metrics vi phm (violations) ln CloudWatch  gi cnh bo. Ti sao khng chn A (AWS Glue + CloudWatch): Mc d AWS Glue c th chy code Python  thc hin suy lun, nhng n l cng c ETL (Trch xut, Chuyn i, Ti), khng c ti u ha cho qun l vng i ML (MLOps). Quan trng hn, CloudWatch l ni nhn metrics ch khng phi cng c phn tch d liu  pht hin li cht lng (nh distribution drift). Bn s phi t vit code phc tp  tnh ton thng k d liu. Ti sao khng chn B (AWS Batch + CloudTrail): y l sai lm nghim trng v kin thc dch v. AWS CloudTrail l dch v  ghi li lch s gi API (Audit logs - Ai lm g, lc no) nhm mc ch bo mt v tun th. N hon ton khng c kh nng c ni dung d liu (payload)  phn tch cht lng hay  chnh xc ca model. Ti sao khng chn C (ECS + EventBridge): Amazon EventBridge l mt Serverless Event Bus dng  nh tuyn s kin gia cc ng dng. N khng c kh nng phn tch d liu thng k (statistical analysis)  bit d liu c tt hay xu. Vic dng ECS i hi bn phi t xy dng ton b h thng gim st d liu t con s 0 (Self-managed), tn km thi gian hn nhiu ..."
  },
  {
    "id": 142,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Topic #: 1 [All AWS Certified Machine Learning Engineer - Associate MLA-C01 Questions] An ML engineer normalized training data by using min-max normalization in AWS Glue DataBrew. The ML engineer must normalize the production inference data in the same way as the training data before passing the production inference data to the model for predictions. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Apply statistics from a well-known dataset to normalize the production samples."
      },
      {
        "id": "B",
        "text": "Keep the min-max normalization statistics from the training set. Use these values to normalize the production samples."
      },
      {
        "id": "C",
        "text": "Calculate a new set of min-max normalization statistics from a batch of production samples. Use these values to normalize all the production samples."
      },
      {
        "id": "D",
        "text": "Calculate a new set of min-max normalization statistics from each production sample. Use these values to normalize all the production samples."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Keep training statistics): y l nguyn tc bt di bt dch trong Machine Learning: D liu Inference phi c x l ging ht d liu Training. Khi hun luyn model, trng s (weights) c iu chnh da trn phn phi v thang o (scale) c th ca d liu training (v d: gi tr Min l 0, Max l 100). Khi a model ra thc t (Inference), bn phi p dng li chnh xc cc tham s thng k (Min, Max, Mean, Variance)  tnh c t tp Training ln d liu mi. Nu bn tnh ton li Min/Max da trn d liu mi, bn s lm thay i thang o, dn n hin tng Training-Serving Skew (Lch pha gia hun luyn v phc v), khin model a ra d on sai lch hon ton. Ti sao khng chn A (Well-known dataset): Thng k t mt tp d liu cng khai (well-known dataset) khng lin quan g n d liu ring ca cng ty bn. p dng thng k ngoi lai ny s lm sai lch bn cht ca d liu u vo. Ti sao khng chn C (New stats from batch): Nu mi batch inference li c mt b Min/Max ring, th cng mt gi tr input (v d: x = 50) s b bin i thnh cc gi tr khc nhau ty thuc vo vic n nm trong batch no. iu ny lm mt tnh nht qun (inconsistency) v khin model khng th d on chnh xc. Ti sao khng chn D (New stats from sample): Tnh ton Min/Max trn mt mu n l (single sample) l v ngha. V d: nu mu ch c gi tr l 10, th Min=10, Max=10. Khi p dng cng thc (x - Min) / (Max - Min), bn s gp li chia cho 0 hoc bin mi gi tr thnh 0. N trit tiu hon ton thng tin ca d liu."
  },
  {
    "id": 143,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to use Amazon SageMaker to make classification ratings that are based on images. The company has 6  of training data that is stored on an Amazon FSx for NetApp ONTAP system virtual machine (SVM). The SVM is in the same VPC as SageMaker. An ML engineer must make the training data accessible for ML models that are in the SageMaker environment. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Mount the FSx for ONTAP file system as a volume to the SageMaker Instance."
      },
      {
        "id": "B",
        "text": "Create an Amazon S3 bucket. Use Mountpoint for Amazon S3 to link the S3 bucket to the FSx for ONTAP file system."
      },
      {
        "id": "C",
        "text": "Create a catalog connection from SageMaker Data Wrangler to the FSx for ONTAP file system."
      },
      {
        "id": "D",
        "text": "Create a direct connection from SageMaker Data Wrangler to the FSx for ONTAP file system."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Mount FSx volume): y l gii php trc tip v hiu qu nht. Amazon FSx for NetApp ONTAP h tr giao thc NFS (Network File System). Cc instance ca Amazon SageMaker (v c bn l my ch Linux) c kh nng mount cc volume NFS mt cch t nhin. V c hai dch v u nm trong cng mt VPC, bn c th cu hnh SageMaker Training Job  mount volume FSx ny trc tip vo mi trng hun luyn. iu ny cho php code hun luyn truy cp 6 TB d liu nh th chng l cc file cc b (local path) m khng cn phi di chuyn hay copy d liu i u c, m bo hiu sut cao v  tr thp. Ti sao khng chn B (Mountpoint for Amazon S3): Mountpoint for Amazon S3 l cng c  mount S3 Bucket thnh file system cc b, ch khng phi dng  link S3 vo FSx. Hn na, d liu  nm sn trn FSx, vic a S3 vo gia l d tha v lm phc tp ha kin trc khng cn thit. Ti sao khng chn C v D (SageMaker Data Wrangler): SageMaker Data Wrangler l cng c low-code dng  chun b, lm sch v feature engineering d liu (thng l dng bng/tabular hoc time-series). N khng phi l c ch  \"mount\" mt h thng file 6 TB cha nh (image data) cho mt Training Job. Vic import 6 TB nh vo Data Wrangler  x l s cc k chm chp v khng ng mc ch s dng (purpose-built) ca cng c ny so vi vic mount file system trc tip."
  },
  {
    "id": 144,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company regularly receives new training data from the vendor of an ML model. The vendor delivers cleaned and prepared data to the company's Amazon S3 bucket every 3-4 days. The company has an Amazon SageMaker pipeline to retrain the model. An ML engineer needs to implement a solution to run the pipeline when new data is uploaded to the S3 bucket. Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": "A",
        "text": "Create an S3 Lifecycle rule to transfer the data to the SageMaker training instance and to initiate training."
      },
      {
        "id": "B",
        "text": "Create an AWS Lambda function that scans the S3 bucket. Program the Lambda function to initiate the pipeline when new data is uploaded."
      },
      {
        "id": "C",
        "text": "Create an Amazon EventBridge rule that has an event pattern that matches the S3 upload. Configure the pipeline as the target of the rule."
      },
      {
        "id": "D",
        "text": "Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the pipeline when new data is uploaded."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (EventBridge): y l m hnh kin trc hng s kin (Event- Driven Architecture) in hnh v ti u nht v cng sc vn hnh (LEAST operational effort). Amazon S3 tch hp trc tip vi Amazon EventBridge  gi thng bo khi c file mi c ti ln (v d: s kin Object Created). Bn ch cn to mt Rule trn EventBridge  bt s kin ny v t Target l SageMaker Pipeline. H thng s t ng kch hot quy trnh hun luyn li (Retrain) ngay lp tc m khng cn vit bt k dng code no hay qun l h tng my ch. Ti sao khng chn A (S3 Lifecycle rule): S3 Lifecycle Rules ch dng  qun l vng i lu tr (v d: chuyn d liu sang lp lu tr r hn nh Glacier hoc xa t ng sau mt thi gian). N khng c chc nng kch hot cc tc v tnh ton hay khi chy quy trnh ML bn ngoi. Ti sao khng chn B (Lambda polling/scanning): Phng n ny yu cu bn phi vit code cho hm Lambda  qut S3 (polling) hoc x l s kin, sau  li phi vit thm logic  gi API kch hot Pipeline. Vic vit, test v bo tr code s tn cng sc vn hnh hn nhiu so vi gii php khng cn code (no-code) ca EventBridge. Ti sao khng chn D (Amazon MWAA): Amazon MWAA (Managed Airflow) l mt dch v iu phi workflow mnh m nhng phc tp v tn km (cn khi to mi trng, qun l DAGs). S dng MWAA ch  bt mt s kin n gin nh S3 upload l \"dng dao m tru  git g\" (over-engineering), khng tha mn tiu ch \"Least operational effort\"."
  },
  {
    "id": 145,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is developing a fraud detection model by using the Amazon SageMaker XGBoost algorithm. The model classifies transactions as either fraudulent or legitimate. During testing, the model excels at identifying fraud in the training dataset. However, the model is inefficient at identifying fraud in new and unseen transactions. What should the ML engineer do to improve the fraud detection for new transactions?",
    "options": [
      {
        "id": "A",
        "text": "Increase the learning rate."
      },
      {
        "id": "B",
        "text": "Remove some irrelevant features from the training dataset."
      },
      {
        "id": "C",
        "text": "Increase the value of the max_depth hyperparameter."
      },
      {
        "id": "D",
        "text": "Decrease the value of the max_depth hyperparameter."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Decrease max_depth): Hin tng m hnh hot ng cc tt trn tp d liu hun luyn (Training data) nhng li km trn d liu mi/cha thy (Unseen data) chnh l nh ngha kinh in ca Overfitting (Qu khp). iu ny c ngha l m hnh qu phc tp, n ang \"hc thuc lng\" cc nhiu (noise) ca d liu train thay v hc cc quy lut tng qut. Trong thut ton XGBoost, siu tham s max_depth kim sot  su ti a ca cy quyt nh. Cy cng su th cng nm bt c nhiu chi tit c th ca d liu train (bao gm c nhiu).  khc phc Overfitting, ta cn gim  phc tp ca m hnh bng cch gim max_depth. Cy nng hn (shallower trees) s buc m hnh phi hc cc mu tng qut hn, t  ci thin kh nng d on trn d liu mi. Ti sao khng chn A (Increase learning rate): Tng tc  hc (eta hoc learning_rate) thng lm cho m hnh cp nht trng s nhanh hn v mnh hn sau mi vng lp. Nu khng c kim sot k, iu ny thng dn n vic m hnh hi t qu nhanh vo cc im cc tiu cc b hoc lm tng kh nng Overfitting, khin tnh trng ti t hn. Ti sao khng chn B (Remove irrelevant features): Mc d loi b cc c trng khng lin quan (Feature Selection) l mt phng php tt  gim nhiu, nhng trong bi cnh tinh chnh siu tham s (Hyperparameter Tuning)  x l Overfitting trc tip trn m hnh cy, vic iu chnh max_depth l bin php k thut trc din v ph bin nht. Hn na,  bi khng cung cp thng tin khng nh c \"irrelevant features\" hay khng, trong khi triu chng Overfitting do model complexity l rt r rng. Ti sao khng chn C (Increase max_depth): Tng max_depth s lm cho cy tr nn su hn v phc tp hn. iu ny s lm tng kh nng \"hc thuc lng\" d liu train, khin vn  Overfitting tr nn nghim trng hn thay v gii quyt n."
  },
  {
    "id": 146,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using Amazon SageMaker to create ML models. The company's data scientists need fine-grained control of the ML workflows that they orchestrate. The data scientists also need the ability to visualize SageMaker jobs and workflows as a directed acyclic graph (DAG). The data scientists must keep a running history of model discovery experiments and must establish model governance for auditing and compliance verifications. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS CodePipeline and its integration with SageMaker Studio to manage the entire ML workflows. Use SageMaker ML Lineage Tracking for the running history of experiments and for auditing and compliance verifications."
      },
      {
        "id": "B",
        "text": "Use AWS CodePipeline and its integration with SageMaker Experiments to manage the entire ML workflows. Use SageMaker Experiments for the running history of experiments and for auditing and compliance verifications."
      },
      {
        "id": "C",
        "text": "Use SageMaker Pipelines and its integration with SageMaker Studio to manage the entire ML workflows. Use SageMaker ML Lineage Tracking for the running history of experiments and for auditing and compliance verifications."
      },
      {
        "id": "D",
        "text": "Use SageMaker Pipelines and its integration with SageMaker Experiments to manage the entire ML workflows. Use SageMaker Experiments for the running history of experiments and for auditing and compliance verifications."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Pipelines + Lineage Tracking): Cu hi c hai nhm yu cu chnh: . Workflow Orchestration & Visualization: Cn kim sot chi tit quy trnh (fine-grained control) v trc quan ha di dng biu  (DAG). SageMaker Pipelines l dch v CI/CD chuyn dng cho ML u tin, cho php nh ngha quy trnh bng code (SDK) v t ng hin th biu  trc quan (DAG Visualizer) ngay trong SageMaker Studio. . Audit & Compliance: Cn lu lch s th nghim v thit lp qun tr (governance) cho mc ch kim ton. SageMaker ML Lineage Tracking l tnh nng ct li c thit k chnh xc  truy vt ngun gc (lineage) ca model (t d liu th, code x l, n artifact model cui cng). N lu tr siu d liu (metadata) ca mi bc, cho php bn tr li cu hi \"Model ny c to ra t d liu no, bi ai, v tham s g?\" - y chnh l yu cu v auditing v compliance. Ti sao khng chn A & B (AWS CodePipeline): AWS CodePipeline l cng c CI/CD a nng cho DevOps (pht trin phn mm), khng c thit k chuyn bit cho cc c th ca ML (nh visualize DAG ca cc bc training/processing step, caching cc bc ML). Mc d c th dng, nhng n thiu tnh nng \"fine-grained control\" v tch hp su (native integration) vi h sinh thi SageMaker Studio  visualize DAG cho Data Scientist nh SageMaker Pipelines. Ti sao khng chn D (SageMaker Experiments cho governance): Mc d SageMaker Experiments gip t chc v theo di cc ln chy hun luyn (runs/trials), nhng mc ch chnh ca n l so snh hiu nng (metrics comparison).  p ng yu cu kht khe v Auditing v Compliance Verifications (nh truy vt ngun gc artifact), SageMaker ML Lineage Tracking l cng c chuyn su v chnh xc hn (n to ra cc mi quan h lin kt ContributedTo, Produced, DerivedFrom gia cc thc th). Cu tr li C chnh xc hn v mt thut ng k thut cho \"governance\"."
  },
  {
    "id": 147,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to reduce the cost of its containerized ML applications. The applications use ML models that run on Amazon EC2 instances, AWS Lambda functions, and an Amazon Elastic Container Service (Amazon ECS) cluster. The EC2 workloads and ECS workloads use Amazon Elastic Block Store (Amazon EBS) volumes to save predictions and artifacts. An ML engineer must identify resources that are being used inefficiently. The ML engineer also must generate recommendations to reduce the cost of these resources. Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create code to evaluate each instance's memory and compute usage."
      },
      {
        "id": "B",
        "text": "Add cost allocation tags to the resources. Activate the tags in AWS Billing and Cost Management."
      },
      {
        "id": "C",
        "text": "Check AWS CloudTrail event history for the creation of the resources."
      },
      {
        "id": "D",
        "text": "Run AWS Compute Optimizer."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (AWS Compute Optimizer): Bi ton yu cu \"xc nh ti nguyn s dng khng hiu qu\" (inefficient resources) v \"a ra khuyn ngh gim chi ph\" (generate recommendations) cho c EC2, Lambda, ECS, v EBS vi \"n lc pht trin t nht\" (LEAST development effort). AWS Compute Optimizer l dch v hon ton t ng, s dng Machine Learning  phn tch lch s s dng (CloudWatch Metrics) ca cc ti nguyn tnh ton (Compute resources). N t ng pht hin cc instance b d tha (over-provisioned) hoc thiu ht (under-provisioned) v a ra cc khuyn ngh c th (v d: \"i t m5.xlarge xung m5.large  tit kim $X/thng\"). N h tr chnh xc cc ti nguyn c nu trong  bi: EC2, Lambda, ECS services (trn Fargate), v EBS volumes. Bn ch cn bt (opt-in) dch v ny m khng cn vit bt k dng code no. Ti sao khng chn A (Create code to evaluate): Vic t vit code  thu thp metrics (CPU, RAM) t CloudWatch, phn tch v a ra quyt nh l vic lm th cng, tn nhiu thi gian v cng sc pht trin (High development effort), vi phm yu cu \"Least development effort\". Ti sao khng chn B (Cost allocation tags): Cost allocation tags gip bn phn loi ha n (v d: Chi ph ny thuc v Project A hay Project B)  bo co ti chnh. N ch cho bit bn  tiu bao nhiu tin, ch khng c kh nng phn tch hiu nng k thut  ni rng \"Ti nguyn ny ang b lng ph, hy gim size i\". Ti sao khng chn C (AWS CloudTrail): CloudTrail l dch v ghi log hot ng (Audit log)  bit \"Ai  to ra instance ny vo lc no\". N khng cha thng tin v mc  s dng ti nguyn (CPU % utilization, Memory usage) nn khng th dng  ti u ha chi ph hay hiu nng."
  },
  {
    "id": 148,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to create a central catalog for all the company's ML models. The models are in AWS accounts where the company developed the models initially. The models are hosted in Amazon Elastic Container Registry (Amazon ECR) repositories. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Configure ECR cross-account replication for each existing ECR repository. Ensure that each model is visible in each AWS account."
      },
      {
        "id": "B",
        "text": "Create a new AWS account with a new ECR repository as the central catalog. Configure ECR cross-account replication between the initial ECR repositories and the central catalog."
      },
      {
        "id": "C",
        "text": "Use the Amazon SageMaker Model Registry to create a model group for models hosted in Amazon ECR. Create a new AWS account. In the new account, use the SageMaker Model Registry as the central catalog. Attach a cross-account resource policy to each model group in the initial AWS accounts."
      },
      {
        "id": "D",
        "text": "Use an AWS Glue Data Catalog to store the models. Run an AWS Glue crawler to migrate the models from the ECR repositories to the Data Catalog. Configure cross- account access to the Data Catalog."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Model Registry):  bi yu cu to mt \"central catalog\" (danh mc tp trung) cho cc ML models. Khi nim \"Catalog\" trong ML khng ch n thun l ni cha file (docker image), m n cn qun l siu d liu (metadata), phin bn (versioning), trng thi ph duyt (approval status - Pending/Approved/Rejected), v lineage. Amazon SageMaker Model Registry l dch v chuyn bit c thit k ng cho mc ch ny. N cho php bn ng k cc model (bao gm c image nm trong ECR) vo cc \"Model Groups\". Tnh nng quan trng nht  y l Cross-account support: Bn c th cu hnh Resource Policy trn cc Model Group  cho php mt ti khon trung tm (Central Account) truy cp v qun l metadata ca cc model nm ri rc  cc ti khon khc. y l cch chun mc  xy dng kho qun l model tp trung (Model Governance) cp doanh nghip. Ti sao khng chn A (ECR Replication): ECR Cross-account replication ch n gin l copy cc Docker images t ti khon ny sang ti khon khc. N khng cung cp tnh nng \"Catalog\" (qun l phin bn model, trng thi ph duyt, metadata). Vic c hng ng image nm  khp ni khng gii quyt c bi ton qun tr tp trung. Ti sao khng chn B (New ECR account): Tng t nh A, vic dn tt c Docker images v mt ti khon ECR mi ch gii quyt vn  lu tr (Storage), khng gii quyt vn  qun l vng i model (Model Lifecycle Management). ECR l container registry, khng phi l Model Registry. Ti sao khng chn D (AWS Glue Data Catalog): AWS Glue Data Catalog l kho cha metadata cho d liu (bng, database, schema) dng cho ETL v Analytics (Athena, Redshift). N khng c thit k  qun l cc thc th ML Model (nh hyperparameters, metrics, inference images). S dng Glue Crawler  qut Docker images l khng kh thi v mt k thut."
  },
  {
    "id": 149,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has developed a new ML model. The company requires online model validation on 10% of the traffic before the company fully releases the model in production. The company uses an Amazon SageMaker endpoint behind an Application Load Balancer (ALB) to serve the model. Which solution will set up the required online validation with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Use production variants to add the new model to the existing SageMaker endpoint. Set the variant weight to 0.1 for the new model. Monitor the number of invocations by using Amazon CloudWatch."
      },
      {
        "id": "B",
        "text": "Use production variants to add the new model to the existing SageMaker endpoint. Set the variant weight to 1 for the new model. Monitor the number of invocations by using Amazon CloudWatch."
      },
      {
        "id": "C",
        "text": "Create a new SageMaker endpoint. Use production variants to add the new model to the new endpoint. Monitor the number of invocations by using Amazon CloudWatch."
      },
      {
        "id": "D",
        "text": "Configure the ALB to route 10% of the traffic to the new model at the existing SageMaker endpoint. Monitor the number of invocations by using AWS CloudTrail."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Production Variants with weight 0.1): Cu hi yu cu thc hin xc thc m hnh mi trn 10% lu lng truy cp (tng t Canary Deployment hoc A/B Testing) vi \"chi ph vn hnh thp nht\". SageMaker Production Variants l tnh nng c tch hp sn (native feature), cho php bn trin khai nhiu model (variants) ng sau cng mt Endpoint duy nht. Bn kim sot lu lng truy cp bng cch gn trng s (weights) cho tng variant.  nh tuyn 10% traffic vo model mi, bn ch cn cu hnh trng s (hoc t l) tng ng (v d: Model c = 0.9, Model mi = 0.1). SageMaker s t ng phn phi traffic ngu nhin theo t l ny m khng cn cu hnh thm bt k load balancer hay proxy no bn ngoi. y l cch n gin v chun mc nht. Ti sao khng chn B (Weight 1): Nu t trng s l 1 (trong khi model c cng c trng s >= 0), traffic s c chia theo t l tng ng ca tng trng s, nhng con s \"1\" khng m bo chnh xc 10% nu khng bit trng s ca model kia. Hn na, nu tng trng s ch c model ny l 1, n s nhn 100% traffic, vi phm yu cu  bi. Ti sao khng chn C (New Endpoint): To mt Endpoint hon ton mi ng ngha vi vic bn c hai URL endpoint khc nhau.  chia traffic (split traffic) gia hai URL ny, bn s phi cu hnh thm logic nh tuyn phc tp  tng ng dng (Application Layer) hoc Load Balancer, lm tng chi ph vn hnh so vi vic dng tnh nng c sn ca SageMaker. Ti sao khng chn D (ALB routing + CloudTrail): ALB thng nh tuyn da trn Path hoc Header. Mc d ALB c h tr weighted target groups, nhng vic cu hnh n  tr vo SageMaker Endpoint (thng c truy cp qua API invoke ca AWS ch khng phi HTTP target thng thng trong VPC theo cch truyn thng) phc tp hn. Quan trng hn, CloudTrail dng  audit API calls (qun l), khng phi  monitor s lng invocations (metrics) thi gian thc hiu qu nh CloudWatch."
  },
  {
    "id": 150,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to develop an ML model. The model must identify an item in an image and must provide the location of the item. Which Amazon SageMaker algorithm will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Image classification"
      },
      {
        "id": "B",
        "text": "XGBoost"
      },
      {
        "id": "C",
        "text": "Object detection"
      },
      {
        "id": "D",
        "text": "K-nearest neighbors (k-NN)"
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Object detection):  bi yu cu hai nhim v ng thi: xc nh vt th l g (\"identify an item\") v xc nh v tr ca n (\"provide the location\"). y chnh xc l nh ngha ca bi ton Object Detection (Pht hin vt th). Thut ton Object Detection (nh SSD hoc Faster R-CNN trong SageMaker) s u ra mt danh sch cc vt th c pht hin cng vi Bounding Box (hp bao quanh)  ch ra ta  v tr ca vt th  trong nh. Ti sao khng chn A (Image classification): Image Classification (Phn loi nh) ch tr li cu hi \"Trong nh ny c ci g?\" (v d: \"Con ch\"), nhng n gn nhn cho ton b bc nh v khng cung cp thng tin v v tr (ta  pixel) ca vt th  nm  u. Ti sao khng chn B (XGBoost): XGBoost l thut ton tng cng  dc (gradient boosting) chuyn dng cho d liu dng bng (tabular/structured data). N khng c thit k  x l d liu hnh nh (unstructured data) cho cc tc v th gic my tnh. Ti sao khng chn D (K-nearest neighbors - k-NN): k-NN l thut ton da trn khong cch, thng dng  tm cc im d liu tng ng (v d: tm cc nh ging nhau hoc phn loi da trn c trng). N khng c kh nng t nhin  v bounding box quanh mt vt th c th trong nh."
  },
  {
    "id": 151,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an Amazon S3 bucket that contains 1  of files from different sources. The S3 bucket contains the following file types in the same S3 folder: CSV, JSON, XLSX, and Apache Parquet. An ML engineer must implement a solution that uses AWS Glue DataBrew to process the data. The ML engineer also must store the final output in Amazon S3 so that AWS Glue can consume the output in the future. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use DataBrew to process the existing S3 folder. Store the output in Apache Parquet format."
      },
      {
        "id": "B",
        "text": "Use DataBrew to process the existing S3 folder. Store the output in AWS Glue Parquet format."
      },
      {
        "id": "C",
        "text": "Separate the data into a different folder for each file type. Use DataBrew to process each folder individually. Store the output in Apache Parquet format."
      },
      {
        "id": "D",
        "text": "Separate the data into a different folder for each file type. Use DataBrew to process each folder individually. Store the output in AWS Glue Parquet format."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Separate data + Process individually + Apache Parquet): y l gii php duy nht kh thi v mt k thut. . Hn ch ca DataBrew: Mt Dataset trong AWS Glue DataBrew c nh ngha t mt ngun d liu (S3 path). DataBrew yu cu tt c cc file trong ngun  phi c cng nh dng (format) v cu trc (schema) tng thch  n c th c v suy lun (infer) chnh xc. Nu bn tr DataBrew vo mt folder cha hn hp CSV, JSON, v Parquet, DataBrew s khng th x l ng thi v mi loi file cn mt parser khc nhau. Do , bt buc phi tch d liu ra cc folder ring bit theo nh dng. . Output Format: Apache Parquet l nh dng ct (columnar storage) tiu chun m, ti u ha cc tt cho hiu sut truy vn v tit kim chi ph lu tr. y l nh dng l tng (\"gold standard\")  AWS Glue v cc dch v phn tch khc tiu th sau ny. Ti sao khng chn A (Process mixed folder): DataBrew khng c kh nng t ng x l mt folder cha \"thp cm\" cc nh dng file khc nhau (mixed file types) trong cng mt Dataset. N s c gng p dng mt b quy tc c (v d: CSV parser) cho tt c cc file, dn n li khi gp file JSON hoc Parquet. Ti sao khng chn B v D (AWS Glue Parquet format): y l mt thut ng \"by\" (distractor). Trong th gii d liu ln, ch c nh dng chun l Apache Parquet. Khng c nh dng no tn l \"AWS Glue Parquet\". S xut hin ca thut ng sai lch ny lm cho cc p n B v D tr nn v gi tr."
  },
  {
    "id": 152,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A manufacturing company uses an ML model to determine whether products meet a standard for quality. The model produces an output of \"Passed\" or \"Failed.\" Robots separate the products into the two categories by using the model to analyze photos on the assembly line. Which metrics should the company use to evaluate the model's performance? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Precision and recall"
      },
      {
        "id": "B",
        "text": "Root mean square error (RMSE) and mean absolute percentage error (MAPE)"
      },
      {
        "id": "C",
        "text": "Accuracy and F1 score"
      },
      {
        "id": "D",
        "text": "Bilingual Evaluation Understudy (BLEU) score"
      },
      {
        "id": "E",
        "text": "Perplexity"
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Precision and recall) v C (Accuracy and F1 score): Bi ton xc nh sn phm \"Passed\" (t) hoc \"Failed\" (Hng) da trn hnh nh l mt bi ton Phn loi nh phn (Binary Classification) in hnh trong Computer Vision. . Accuracy ( chnh xc): o lng t l d on ng trn tng s mu. Hu ch khi d liu cn bng. . Precision ( chnh xc ca lp Positive) & Recall ( ph): Cc k quan trng trong sn xut.  Precision: Trong s cc sn phm model bo \"Hng\", bao nhiu phn trm thc s hng? (Trnh vt nhm hng tt).  Recall: Trong s tt c sn phm \"Hng\" thc t, model pht hin c bao nhiu? (Trnh  lt hng li ra th trng). . F1 Score: L trung bnh iu ha ca Precision v Recall, gip nh gi hiu nng tng th khi cn cn bng gia hai yu t trn hoc khi d liu b mt cn bng (Imbalanced Data - v d hng li rt t so vi hng tt). V cc p n cn li u thuc v cc lnh vc khc (Regression hoc NLP), nn A v C l hai nhm ch s duy nht ph hp. Ti sao khng chn B (RMSE v MAPE): Root Mean Square Error v Mean Absolute Percentage Error l cc ch s dng cho bi ton Hi quy (Regression) - tc l d on mt con s c th (v d: d on nhit , gi nh), khng dng cho bi ton phn loi (Classification). Ti sao khng chn D (BLEU score): BLEU (Bilingual Evaluation Understudy) l ch s tiu chun dng  nh gi cht lng ca cc h thng Dch my (Machine Translation) hoc sinh vn bn, bng cch so snh vn bn my to ra vi vn bn mu ca con ngi. N khng lin quan n x l nh. Ti sao khng chn E (Perplexity): Perplexity l ch s dng  nh gi cc M hnh ngn ng (Language Models) (v d: model d on t tip theo tt n u). N o lng  \"bi ri\" ca model trc d liu mi; gi tr cng thp cng tt. N khng p dng cho bi ton phn loi nh."
  },
  {
    "id": 153,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to encrypt all data in transit when an ML training job runs. The ML engineer must ensure that encryption in transit is applied to processes that Amazon SageMaker uses during the training job. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Encrypt communication between nodes for batch processing."
      },
      {
        "id": "B",
        "text": "Encrypt communication between nodes in a training cluster."
      },
      {
        "id": "C",
        "text": "Specify an AWS Key Management Service (AWS KMS) key during creation of the training job request."
      },
      {
        "id": "D",
        "text": "Specify an AWS Key Management Service (AWS KMS) key during creation of the SageMaker domain."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Encrypt communication between nodes): Trong qu trnh hun luyn phn tn (Distributed Training) s dng nhiu instance (nodes), cc node ny cn trao i d liu lin tc (nh tham s m hnh, gradients) vi nhau.  m bo an ton cho d liu ang di chuyn (in transit) gia cc container trn cc node khc nhau, Amazon SageMaker cung cp tnh nng Inter-container traffic encryption. Khi to training job, bn cn bt c (flag) EnableInterContainerTrafficEncryption thnh True. iu ny m bo mi giao tip mng gia cc node trong cm hun luyn u c m ha bng giao thc TLS 1.2. Ti sao khng chn A (Batch processing): \"Batch processing\" thng m ch tc v suy lun hng lot (Batch Transform) hoc x l d liu ETL, khng phi l thut ng chuyn mn dng  m t giao tip ni b trong cm \"Training Job\" (Hun luyn). Hn na, c ch m ha cho Batch Transform cng tng t nhng ng cnh cu hi ang nhn mnh vo \"training job\". Ti sao khng chn C (AWS KMS key for training job): Vic ch nh AWS KMS key (VolumeKmsKeyId) khi to training job ch yu  m ha D liu ti ch (Data At Rest) trn cc  a EBS c gn vo training instances v d liu u ra trn S3 (OutputDataConfig). N khng t ng bt tnh nng m ha ng truyn mng gia cc node. Ti sao khng chn D (AWS KMS key for SageMaker domain): Cu hnh KMS cho SageMaker Domain dng  m ha d liu lu tr trn  a EFS ca mi trng lm vic (SageMaker Studio/Notebooks). N hon ton khng lin quan n mi trng tnh ton tm thi (ephemeral cluster) c khi to ring bit  chy training job."
  },
  {
    "id": 154,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to use metrics to assess the quality of a time-series forecasting model. Which metrics apply to this model? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Recall"
      },
      {
        "id": "B",
        "text": "LogLoss"
      },
      {
        "id": "C",
        "text": "Root mean square error (RMSE)"
      },
      {
        "id": "D",
        "text": "InferenceLatency"
      },
      {
        "id": "E",
        "text": "Average weighted quantile loss (wQL)"
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (RMSE): D bo chui thi gian (Time-series forecasting) v bn cht l mt bi ton hi quy (Regression) vi yu t thi gian. RMSE (Root Mean Square Error) l thc o tiu chun  nh gi  lch gia gi tr d bo v gi tr thc t. N o lng  ln trung bnh ca sai s v c bit \"trng pht\" nng cc sai s ln (do bnh phng sai s), gip pht hin cc im d bo sai lch nghim trng. Ti sao chn E (Average weighted quantile loss - wQL): Trong cc dch v d bo hin i ca AWS (nh Amazon Forecast hay thut ton DeepAR), m hnh thng a ra d bo theo xc sut (Probabilistic Forecast) thay v mt im n l (Point Forecast). wQL c s dng  nh gi  chnh xc ca d bo ti cc phn v c th (v d: P10, P50, P90). N o lng mc  sai lch gia phn phi d on v thc t, gip nh gi ri ro tt hn (v d: d bo tn kho qu nhiu hay qu t). Ti sao khng chn A (Recall): Recall l ch s dng cho bi ton Phn loi (Classification)  o lng t l cc mu Positive thc t c d on ng. D bo chui thi gian l d on gi tr lin tc (s lng, doanh thu), khng phi phn loi nhn. Ti sao khng chn B (LogLoss): LogLoss (hay Cross-entropy Loss) l hm mt mt dng cho bi ton Phn loi, c bit l  nh gi  tin cy ca xc sut d on lp (class probability). N khng p dng cho d bo gi tr s thc. Ti sao khng chn D (InferenceLatency): InferenceLatency ( tr suy lun) l ch s v Hiu nng vn hnh (Operational Performance/Efficiency) - tc l model chy nhanh hay chm. Khi ni n \"quality of a model\" trong ng cnh Data Science, ngi ta thng m ch  chnh xc (Accuracy/Error metrics) ca d on, ch khng phi tc  phn hi."
  },
  {
    "id": 155,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company runs Amazon SageMaker ML models that use accelerated instances. The models require real-time responses. Each model has different scaling requirements. The company must not allow a cold start for the models. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a SageMaker Serverless Inference endpoint for each model. Use provisioned concurrency for the endpoints."
      },
      {
        "id": "B",
        "text": "Create a SageMaker Asynchronous Inference endpoint for each model. Create an auto scaling policy for each endpoint."
      },
      {
        "id": "C",
        "text": "Create a SageMaker endpoint. Create an inference component for each model. In the inference component settings, specify the newly created endpoint. Create an auto scaling policy for each inference component. Set the parameter for the minimum number of copies to at least 1."
      },
      {
        "id": "D",
        "text": "Create an Amazon S3 bucket. Store all the model artifacts in the S3 bucket. Create a SageMaker multi-model endpoint. Point the endpoint to the S3 bucket. Create an auto scaling policy for the endpoint. Set the parameter for the minimum number of copies to at least 1."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Inference Components): y l tnh nng mi v mnh m nht ca SageMaker Real-time Inference, c thit k c bit  gii quyt bi ton \"nhiu model trn mt endpoint\" nhng vn m bo s c lp v ti nguyn. . Accelerated Instances & Cost Efficiency: Inference Components cho php bn trin khai nhiu model ln cng mt endpoint (chia s instance GPU t tin)  ti u chi ph, thay v mi model mt endpoint ring. . Independent Scaling: im ct li l mi \"Inference Component\" c th c chnh sch Auto Scaling ring bit. Model A c th scale ln 5 copies khi ti cao, trong khi Model B vn gi 1 copy. . No Cold Start: Bng cch t tham s \"minimum number of copies\" (s bn sao ti thiu) l 1 (hoc ln hn), bn m bo model lun c ti sn trong b nh GPU/CPU v sn sng phc v request ngay lp tc, loi b hon ton vn  cold start ( tr khi khi ng). Ti sao khng chn A (Serverless Inference): Serverless Inference hin ti cha h tr GPU (accelerated instances). Ngoi ra, mc d c Provisioned Concurrency  gim cold start, nhng n khng ph hp vi cc workload yu cu GPU hiu nng cao nh  bi gi  (\"accelerated instances\"). Ti sao khng chn B (Asynchronous Inference): Asynchronous Inference c thit k cho cc request c thi gian x l lu (long-running) hoc payload ln, khng phi ti u cho \"Real-time responses\" (phn hi tc th) vi  tr thp nht. Hn na, Async Inference thng c c ch scale-to-zero, dn n cold start nu khng cu hnh k. Ti sao khng chn D (Multi-model Endpoint - MME): Multi-model Endpoint (MME) truyn thng hot ng theo c ch \"Lazy Loading\": model ch c ti t S3 vo b nh khi c request u tin gi n n. iu ny gy ra Cold Start r rt cho cc request u tin. Mc d MME tit kim chi ph, nhng n vi phm yu cu nghim ngt \"must not allow a cold start\". Ngoi ra, MME scale  cp  Endpoint (Instance level), kh kim sot scaling chi tit cho tng model rin..."
  },
  {
    "id": 156,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "Question 92",
    "options": [],
    "answer": "B",
    "explanation": "Ti sao chn B (MulticlassClassificationEvaluator): y l on m chnh xc  nh gi m hnh phn loi a lp (multi-class) trong Spark ML. . ng Class Evaluator: Bi ton nu r y l \"three-class decision tree classifier\" (phn loi 3 lp), do  bt buc phi s dng MulticlassClassificationEvaluator. . ng Cu hnh: Cc tham s predictionCol=\"prediction\" v labelCol=\"actual\" khp hon ton vi schema ca DataFrame preds_df  cho. Tham s metricName=\"accuracy\" l hp l  tnh  chnh xc. . ng Phng thc: Quan trng nht, on code gi phng thc .evaluate(preds_df) trn i tng evaluator. y l bc thc thi tnh ton metric da trn d liu v tr v kt qu s thc (Double) cho bin accuracy. Ti sao khng chn A (Summarizer): Summarizer trong Spark ML (pyspark.ml.stat) thng c s dng  tnh ton cc thng k m t (nh mean, variance, count) cho cc ct vector (Vector column summary statistics). N khng phi l class dng  nh gi hiu nng (evaluation metrics) ca m hnh phn loi. Ti sao khng chn C (Missing .evaluate()): Mc d Option C khi to ng i tng MulticlassClassificationEvaluator, nhng n thiu bc gi hm .evaluate(preds_df). Bin accuracy trong trng hp ny s ch cha i tng Evaluator (instance ca class), ch khng phi l gi tr  chnh xc (s thc) m  bi yu cu \"compute the accuracy\". Ti sao khng chn D (BinaryClassificationEvaluator): BinaryClassificationEvaluator ch dnh ring cho bi ton phn loi nh phn (2 lp).  bi nu r y l bi ton 3 lp (three-class), nn vic s dng Binary Evaluator l sai v mt k thut. Hn na, BinaryClassificationEvaluator mc nh h tr cc metric nh areaUnderROC hoc areaUnderPR, ch khng h tr accuracy mt cch trc tip nh Multiclass Evaluator."
  },
  {
    "id": 157,
    "type": "multiple_response",
    "required_answers": 3,
    "question": "A company runs training jobs on Amazon SageMaker by using a compute optimized instance. Demand for training runs will remain constant for the next 55 weeks. The instance needs to run for 35 hours each week. The company needs to reduce its model training costs. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use a serverless endpoint with a provisioned concurrency of 35 hours for each week. Run the training on the endpoint."
      },
      {
        "id": "B",
        "text": "Use SageMaker Edge Manager for the training. Specify the instance requirement in the edge device configuration. Run the training."
      },
      {
        "id": "C",
        "text": "Use the heterogeneous cluster feature of SageMaker Training. Configure the instance_type, instance_count, and instance_groups arguments to run training jobs."
      },
      {
        "id": "D",
        "text": "Opt in to a SageMaker Savings Plan with a 1-year term and an All Upfront payment. Run a SageMaker Training job on the instance."
      }
    ],
    "answer": "D,a",
    "explanation": "Ti sao chn D (SageMaker Savings Plan): Bi ton a ra mt nhu cu s dng ti nguyn rt r rng v n nh: chy training 35 gi/tun trong sut 55 tun (tng ng hn 1 nm mt cht). y l kch bn hon ho  s dng SageMaker Savings Plans. SageMaker Savings Plans cung cp mc gi chit khu ng k (ln n 64%) so vi gi On-Demand  i ly cam kt s dng mt lng ti nguyn nht nh (tnh theo $/gi) trong k hn 1 hoc 3 nm. Vi la chn \"1-year term\" v thanh ton \"All Upfront\" (tr trc ton b), cng ty s t c mc tit kim chi ph ti a cho khi lng cng vic  c d bo trc ny. Ti sao khng chn A (Serverless Endpoint): Serverless Endpoint l dch v dnh cho Suy lun (Inference), tc l trin khai model  d on. Bn khng th chy cc cng vic Hun luyn (Training jobs) trn mt Inference Endpoint. y l s nhm ln c bn v mc ch s dng dch v. Ti sao khng chn B (SageMaker Edge Manager): SageMaker Edge Manager c thit k  qun l, gim st v vn hnh cc model ML trn cc thit b bin (Edge devices) nh robot, camera thng minh, v.v. N khng phi l gii php  ti u ha chi ph cho vic hun luyn model trn m my (Cloud Training) bng EC2 instance. Ti sao khng chn C (Heterogeneous cluster): Tnh nng Heterogeneous cluster cho php chy mt training job trn nhiu loi instance khc nhau (v d: dng CPU instance cho x l d liu v GPU instance cho training)  ti u ha hiu nng. Tuy nhin,  bi ch  cp n vic s dng \"a compute optimized instance\" (mt loi instance) v tp trung vo kha cnh ti chnh (reduce costs) cho mt nhu cu n nh di hn. Vic thay i kin trc cluster phc tp hn v khng m bo gim chi ph trc tip bng vic cam kt Savings Plan."
  },
  {
    "id": 158,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company deployed an ML model that uses the XGBoost algorithm to predict product failures. The model is hosted on an Amazon SageMaker endpoint and is trained on normal operating data. An AWS Lambda function provides the predictions to the company's application. An ML engineer must implement a solution that uses incoming live data to detect decreased model accuracy over time. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon CloudWatch to create a dashboard that monitors real-time inference data and model predictions. Use the dashboard to detect drift."
      },
      {
        "id": "B",
        "text": "Modify the Lambda function to calculate model drift by using real-time inference data and model predictions. Program the Lambda function to send alerts."
      },
      {
        "id": "C",
        "text": "Schedule a monitoring job in SageMaker Model Monitor. Use the job to detect drift by analyzing the live data against a baseline of the training data statistics and constraints."
      },
      {
        "id": "D",
        "text": "Schedule a monitoring job in SageMaker Debugger. Use the job to detect drift by analyzing the live data against a baseline of the training data statistics and constraints."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Model Monitor): Cu hi yu cu pht hin s suy gim  chnh xc ca model theo thi gian (decreased model accuracy over time) da trn d liu thc t (incoming live data). y chnh l nh ngha ca vic gim st Model Quality Drift v Data Quality Drift. Amazon SageMaker Model Monitor l dch v chuyn dng c thit k  gii quyt chnh xc bi ton ny. Quy trnh chun nh sau: . To mt Baseline (ng c s) t thng k ca d liu hun luyn (Training data statistics & constraints). . Bt tnh nng Data Capture trn Endpoint  lu li d liu suy lun thc t. . Ln lch (Schedule) mt monitoring job nh k. Job ny s t ng so snh d liu thc t vi Baseline. Nu pht hin s sai lch vt qu ngng cho php (v d: phn phi d liu thay i), n s bo co drift v gi cnh bo qua CloudWatch. Ti sao khng chn A (CloudWatch Dashboard): Amazon CloudWatch Dashboards dng  trc quan ha cc s liu (metrics) nh CPU, Memory, Latency hoc cc custom metrics n gin. N khng c kh nng t ng thc hin cc php tnh thng k phc tp (nh KS test, Chi-square test)  so snh phn phi d liu (distribution comparison) nhm pht hin drift mt cch khoa hc. Ti sao khng chn B (Lambda function): Vic sa i Lambda function  t tnh ton drift l mt gii php th cng, phc tp v khng hiu qu (Self-managed solution). Bn s phi t vit code  lu tr trng thi, thc hin cc php ton thng k v qun l baseline, iu ny i ngc li vi thc tin tt nht l s dng cc dch v Managed Services c sn. Hn na, tnh ton drift thng l tc v batch (x l theo l) nh k, khng nn thc hin trong thi gian thc (real-time) bn trong Lambda v s lm tng  tr ca ng dng. Ti sao khng chn D (SageMaker Debugger): SageMaker Debugger l cng c dng  g li v phn tch model trong qu trnh hun luyn (Training phase), v d nh pht hin vanishing gradients, overfitting, hoc gim st vic s..."
  },
  {
    "id": 159,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an ML model that uses historical transaction data to predict customer behavior. An ML engineer is optimizing the model in Amazon SageMaker to enhance the model's predictive accuracy. The ML engineer must examine the input data and the resulting predictions to identify trends that could skew the model's performance across different demographics. Which solution will provide this level of analysis?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon CloudWatch to monitor network metrics and CPU metrics for resource optimization during model training."
      },
      {
        "id": "B",
        "text": "Create AWS Glue DataBrew recipes to correct the data based on statistics from the model output."
      },
      {
        "id": "C",
        "text": "Use SageMaker Clarify to evaluate the model and training data for underlying patterns that might affect accuracy."
      },
      {
        "id": "D",
        "text": "Create AWS Lambda functions to automate data pre-processing and to ensure consistent quality of input data for the model."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Clarify): Yu cu ct li ca  bi l \"examine the input data and the resulting predictions\" (kim tra d liu u vo v kt qu d on)  xc nh cc xu hng gy sai lch hiu nng trn \"different demographics\" (cc nhm nhn khu hc khc nhau). y chnh l bi ton pht hin Bias (Thin kin). Amazon SageMaker Clarify l cng c chuyn dng c tch hp trong SageMaker  thc hin nhim v ny. N c kh nng: . Pre-training bias metrics: Phn tch d liu trc khi hun luyn  tm s mt cn bng i din (v d: d liu v mt nhm tui qu t). . Post-training bias metrics: Phn tch kt qu d on sau khi hun luyn  xem model c i x bt cng vi cc nhm c th hay khng (v d: t l t chi vay vn cao bt thng  mt nhm dn tc). . Explainability: Gii thch l do ti sao model a ra d on  (Feature attribution). Ti sao khng chn A (Amazon CloudWatch): CloudWatch l dch v gim st h tng v hiu nng vn hnh (Operational monitoring). N theo di CPU, RAM, Network I/O  bit \"Server c khe khng\", ch khng th phn tch ni dung d liu  bit \"Model c cng bng khng\". Ti sao khng chn B (AWS Glue DataBrew): AWS Glue DataBrew l cng c visual data preparation dng  lm sch v chun ha d liu th (ETL). Mc d n c th hin th thng k d liu (Data profile), nhng n khng c kh nng so snh tng quan gia Input v Model Output  nh gi Bias ca thut ton ML nh Clarify. Ti sao khng chn D (AWS Lambda): AWS Lambda l dch v tnh ton phi my ch (Serverless Compute) dng  chy code ty .  dng Lambda gii quyt bi ton ny, bn s phi t vit ton b thut ton thng k phc tp  tnh ton bias t u (Self-managed code), rt tn km cng sc v khng tn dng c cc tnh nng c sn ca nn tng AI/ML."
  },
  {
    "id": 160,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company uses 10 Reserved Instances of accelerated instance types to serve the current version of an ML model. An ML engineer needs to deploy a new version of the model to an Amazon SageMaker real-time inference endpoint. The solution must use the original 10 instances to serve both versions of the model. The solution also must include one additional Reserved Instance that is available to use in the deployment process. The transition between versions must occur with no downtime or service interruptions. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Configure a blue/green deployment with all-at-once traffic shifting."
      },
      {
        "id": "B",
        "text": "Configure a blue/green deployment with canary traffic shifting and a size of 10%."
      },
      {
        "id": "C",
        "text": "Configure a shadow test with a traffic sampling percentage of 10%."
      },
      {
        "id": "D",
        "text": "Configure a rolling deployment with a rolling batch size of 1."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Rolling deployment): Bi ton t ra rng buc rt kht khe v ti nguyn: ang c 10 Reserved Instances (RIs) v ch c thm 1 RI d phng (Tng nng lc ti a l 11 instances ti mt thi im). Rolling Deployment (Trin khai cun chiu) l chin lc duy nht ph hp trong trng hp ny. Vi RollingBatchSize = 1, SageMaker s thc hin quy trnh sau: . Khi to 1 instance mi (s dng RI th 11) chy phin bn model mi. . Sau khi instance ny sn sng, n s nh tuyn traffic vo  v tt 1 instance c i (tr li 1 slot RI). . Lp li quy trnh ny 10 ln cho n khi ton b fleet c cp nht. Cch ny m bo s lng instance hot ng khng bao gi vt qu 11, tn dng ng s lng RIs  mua m vn m bo khng c thi gian cht (no downtime). Ti sao khng chn A v B (Blue/Green Deployment): Blue/Green Deployment (d l all-at-once hay canary) hot ng theo nguyn tc to ra mt mi trng mi (Green Fleet) hon ton c lp v song song vi mi trng c (Blue Fleet) trc khi chuyn traffic. Vi 10 instance ang chy, Blue/Green s yu cu khi to thm 10 instance mi na (Tng cng 20 instances)  sn sng chuyn i. Do cng ty ch c thm 1 RI, vic provision thm 10 instance s dn n vic phi dng On- Demand instance (tn km) hoc khng  quota/capacity, vi phm rng buc v vic s dng ti nguyn  nu. Ti sao khng chn C (Shadow test): Shadow Testing l k thut kim th, khng phi chin lc trin khai thay th (replacement strategy). N gi bn sao ca traffic sang mt \"Shadow variant\"  so snh hiu nng m khng tr kt qu cho user. Vic ny va khng hon thnh mc tiu \"deploy new version\"  phc v user, va tiu tn ti nguyn gp i (nu shadow 100% traffic) hoc phc tp ha h tng khng cn thit."
  },
  {
    "id": 161,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An IoT company uses Amazon SageMaker to train and test an XGBoost model for object detection. ML engineers need to monitor performance metrics when they train the model with variants in hyperparameters. The ML engineers also need to send Short Message Service (SMS) text messages after training is complete. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon CloudWatch to monitor performance metrics. Use Amazon Simple Queue Service (Amazon SQS) for message delivery."
      },
      {
        "id": "B",
        "text": "Use Amazon CloudWatch to monitor performance metrics. Use Amazon Simple Notification Service (Amazon SNS) for message delivery."
      },
      {
        "id": "C",
        "text": "Use AWS CloudTrail to monitor performance metrics. Use Amazon Simple Queue Service (Amazon SQS) for message delivery."
      },
      {
        "id": "D",
        "text": "Use AWS CloudTrail to monitor performance metrics. Use Amazon Simple Notification Service (Amazon SNS) for message delivery."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (CloudWatch + SNS): . Monitor performance metrics: Amazon CloudWatch l dch v gim st trung tm ca AWS. Khi Amazon SageMaker hun luyn model (nh XGBoost), n t ng y cc metrics v hiu nng (nh training error, validation accuracy) ln CloudWatch Metrics v ghi logs vo CloudWatch Logs. K s ML c th xem biu  trc quan ti y. . Send SMS: Amazon Simple Notification Service (Amazon SNS) l dch v thng bo dng Pub/Sub h tr y tin nhn n nhiu loi endpoint khc nhau, bao gm Email v SMS. y l gii php chun  gi thng bo vn bn ti ngi dng khi mt s kin (nh \"Training Completed\") xy ra. Ti sao khng chn A v C (Amazon SQS): Amazon SQS (Simple Queue Service) l dch v hng i (queue) dng  phn tch (decouple) cc thnh phn ng dng ( ng dng khc c v x l sau). SQS khng c chc nng gi tin nhn vn bn (SMS) trc tip n s in thoi ca ngi dng. Ti sao khng chn C v D (AWS CloudTrail): AWS CloudTrail l dch v dng  ghi li lch s gi API (Audit/Governance) nhm tr li cu hi \"Ai  lm g, lc no?\". N khng ghi nhn hay hin th cc ch s hiu nng (metrics) ca qu trnh hun luyn model (nh  chnh xc hay loss function)."
  },
  {
    "id": 162,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is working on an ML project that will include Amazon SageMaker notebook instances. An ML engineer must ensure that the SageMaker notebook instances do not allow root access. Which solution will prevent the deployment of notebook instances that allow root access?",
    "options": [
      {
        "id": "A",
        "text": "Use IAM condition keys to stop deployments of SageMaker notebook instances that allow root access."
      },
      {
        "id": "B",
        "text": "Use AWS Key Management Service (AWS KMS) keys to stop deployments of SageMaker notebook instances that allow root access."
      },
      {
        "id": "C",
        "text": "Monitor resource creation by using Amazon EventBridge events. Create an AWS Lambda function that deletes all deployed SageMaker notebook instances that allow root access."
      },
      {
        "id": "D",
        "text": "Monitor resource creation by using AWS CloudFormation events. Create an AWS Lambda function that deletes all deployed SageMaker notebook instances that allow root access."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (IAM condition keys): y l gii php phng nga (preventive) hiu qu nht. AWS IAM cung cp condition key c th l sagemaker:RootAccess. Bng cch to mt IAM policy vi quy tc Deny cho hnh ng sagemaker:CreateNotebookInstance nu sagemaker:RootAccess c t l Enabled, bn c th chn ng yu cu to instance ngay t bc gi API. Instance s khng bao gi c khi to, m bo tun th bo mt tuyt i. Ti sao khng chn B (AWS KMS): AWS KMS (Key Management Service) dng  qun l kha m ha d liu (Encryption). N khng c kh nng kim sot cu hnh h iu hnh (nh quyn Root) ca instance. Ti sao khng chn C (EventBridge + Lambda): y l gii php khc phc (reactive). Instance  c to ra v tn ti trong mt khong thi gian ngn vi quyn root trc khi EventBridge kch hot Lambda  xa n. Trong khong thi gian , l hng bo mt  tn ti.  bi yu cu \"prevent the deployment\" (ngn chn vic trin khai), tc l khng cho php n c sinh ra. Ti sao khng chn D (CloudFormation events + Lambda): Tng t nh phng n C, y l cch tip cn phn ng sau s vic. Ngoi ra, vic s dng CloudFormation events phc tp hn v khng chn c cc trng hp ngi dng to instance th cng qua Console hoc CLI m khng dng CloudFormation."
  },
  {
    "id": 163,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using Amazon SageMaker to develop ML models. The company stores sensitive training data in an Amazon S3 bucket. The model training must have network isolation from the internet. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Run the SageMaker training jobs in private subnets. Create a NAT gateway. Route traffic for training through the NAT gateway."
      },
      {
        "id": "B",
        "text": "Run the SageMaker training jobs in private subnets. Create an S3 gateway VPC endpoint. Route traffic for training through the S3 gateway VPC endpoint."
      },
      {
        "id": "C",
        "text": "Run the SageMaker training jobs in public subnets that have an attached security group. In the security group, use inbound rules to limit traffic from the internet. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE-KMS)."
      },
      {
        "id": "D",
        "text": "Encrypt traffic to Amazon S3 by using a bucket policy that includes a value of True for the aws:SecureTransport condition key. Use default at-rest encryption for Amazon S3. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE-KMS)."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Private subnets + S3 VPC Endpoint):  t c yu cu \"network isolation from the internet\" (c lp mng hon ton khi internet), cc SageMaker training jobs phi chy trong Private Subnet (khng c ng ra Internet Gateway). Tuy nhin, SageMaker vn cn truy cp d liu training nm trn S3. S3 Gateway VPC Endpoint l gii php chun  cho php cc ti nguyn trong Private Subnet truy cp S3 qua mng ni b ca AWS m khng cn i qua internet cng cng (public internet). y l kin trc ti u cho bo mt v c lp mng. Ti sao khng chn A (NAT gateway): NAT Gateway c thit k  cung cp kh nng truy cp internet chiu i (outbound) cho cc ti nguyn trong Private Subnet (v d:  ti gi tin, update phn mm). Vic s dng NAT Gateway ng ngha vi vic m kt ni ra internet, iu ny vi phm yu cu \"network isolation from the internet\" (c lp khi internet) ca  bi. Ti sao khng chn C (Public subnets): Vic chy training jobs trong Public Subnets yu cu ti nguyn phi c a ch Public IP v Internet Gateway  giao tip. D c dng Security Group  hn ch, v mt kin trc mng, ti nguyn ny vn nm trn mng cng cng, vi phm hon ton yu cu v c lp mng (network isolation). Ti sao khng chn D (Encryption/SecureTransport): p n ny ch tp trung vo bo mt d liu (m ha ng truyn v m ha lu tr) m khng gii quyt vn  v kin trc mng. Vic bt aws:SecureTransport (HTTPS) hay m ha d liu khng ngn chn vic training jobs kt ni ra internet hoc m bo traffic i qua ng truyn ring t."
  },
  {
    "id": 164,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs an AWS solution that will automatically create versions of ML models as the models are created. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Amazon Elastic Container Registry (Amazon ECR)"
      },
      {
        "id": "B",
        "text": "Model packages from Amazon SageMaker Marketplace"
      },
      {
        "id": "C",
        "text": "Amazon SageMaker ML Lineage Tracking"
      },
      {
        "id": "D",
        "text": "Amazon SageMaker Model Registry"
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (SageMaker Model Registry): y l dch v c thit k chuyn bit  qun l vng i ca m hnh. Khi bn to mt \"Model Group\" v ng k mt m hnh mi vo nhm , SageMaker Model Registry s t ng nh s phin bn (Version 1, Version 2,...) cho m hnh . N cung cp kho lu tr trung tm  qun l cc phin bn, trng thi ph duyt (Approved/Rejected) v metadata, p ng chnh xc yu cu  bi. Ti sao khng chn A (Amazon ECR): Amazon Elastic Container Registry (ECR) l ni lu tr cc Docker Container Images (thng cha code inference). Mc d container l mt phn ca qu trnh trin khai, nhng ECR khng qun l \"phin bn m hnh\" (model artifacts + metrics + status) theo ng cnh ML. N ch qun l phin bn ca image (tag). Ti sao khng chn B (SageMaker Marketplace): y l mt \"ch ng dng\"  tm kim, mua hoc bn cc m hnh v thut ton t bn th ba (third-party vendors), khng phi l cng c  qun l phin bn cho cc m hnh ni b ca cng ty. Ti sao khng chn C (SageMaker ML Lineage Tracking): Dch v ny tp trung vo vic truy vt ngun gc (provenance) v mi quan h gia cc thnh phn (D liu -> Training Job -> Model). Mc d n lu gi lch s, nhng chc nng chnh ca n l  audit (kim ton) v visualize quy trnh, khng phi l c ch chnh  ng k v nh s phin bn cho vic trin khai (deployment) nh Model Registry."
  },
  {
    "id": 165,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to use Retrieval Augmented Generation (RAG) to supplement an open source large language model (LLM) that runs on Amazon Bedrock. The company's data for RAG is a set of documents in an Amazon S3 bucket. The documents consist of .csv files and .docx files. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create a pipeline in Amazon SageMaker Pipelines to generate a new model. Call the new model from Amazon Bedrock to perform RAG queries."
      },
      {
        "id": "B",
        "text": "Convert the data into vectors. Store the data in an Amazon Neptune database. Connect the database to Amazon Bedrock. Call the Amazon Bedrock API to perform RAG queries."
      },
      {
        "id": "C",
        "text": "Fine-tune an existing LLM by using an AutoML job in Amazon SageMaker. Configure the S3 bucket as a data source for the AutoML job. Deploy the LLM to a SageMaker endpoint. Use the endpoint to perform RAG queries."
      },
      {
        "id": "D",
        "text": "Create a knowledge base for Amazon Bedrock. Configure a data source that references the S3 bucket. Use the Amazon Bedrock API to perform RAG queries."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Knowledge base for Amazon Bedrock): y l gii php \"Managed RAG\" (RAG c qun l hon ton) ca AWS. Tnh nng Knowledge Bases for Amazon Bedrock t ng ha ton b quy trnh RAG: t vic kt ni ngun d liu (S3), chia nh vn bn (chunking), to vector (embedding), lu tr vo vector database v truy xut (retrieval). V AWS x l mi cng on phc tp  backend, y l gii php c \"LEAST operational overhead\" (t gnh nng vn hnh nht). Ti sao khng chn A (SageMaker Pipelines/New model): Phng n ny  cp n vic to hoc train mt model mi. RAG (Retrieval Augmented Generation) l k thut b sung d liu ngoi vo prompt, khng phi l quy trnh training model mi. Hn na, vic qun l pipeline training rt tn km chi ph vn hnh. Ti sao khng chn B (Manual Vectorization + Neptune): Mc d kin trc ny kh thi v mt k thut, nhng n yu cu bn phi t vit code  chuyn i d liu sang vector, sau  t cu hnh v qun l c s d liu Amazon Neptune. Vic t qun l (self-managed) cc thnh phn ny to ra gnh nng vn hnh cao hn nhiu so vi gii php tch hp sn (Option D). Ti sao khng chn C (Fine-tune + AutoML): Fine-tuning (tinh chnh) khc hon ton vi RAG. Fine-tuning l dy model hc d liu mi  thay i trng s (weights), trong khi RAG l tra cu thng tin ti thi im chy (runtime). Ngoi ra, Fine-tuning yu cu chun b d liu phc tp v tn km ti nguyn tnh ton hn nhiu."
  },
  {
    "id": 166,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company plans to deploy an ML model for production inference on an Amazon SageMaker endpoint. The average inference payload size will vary from 100 MB to 300 MB. Inference requests must be processed in 60 minutes or less. Which SageMaker inference option will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Serverless inference"
      },
      {
        "id": "B",
        "text": "Asynchronous inference"
      },
      {
        "id": "C",
        "text": "Real-time inference"
      }
    ],
    "answer": "?",
    "explanation": "Ti sao chn B (Asynchronous inference): y l ch  suy lun c thit k c bit cho cc request c payload ln (ln n 1 GB) v thi gian x l lu (ln n 1 gi). o  bi yu cu x l payload t 100 MB - 300 MB (vt xa gii hn ca Real- time) v thi gian x l ln n 60 pht. Asynchronous Inference hot ng bng cch a request vo hng i (queue) v x l khng ng b, hon ton ph hp vi cc thng s ny. Ti sao khng chn A (Serverless inference): Ch  Serverless c gii hn payload rt thp (ti a 4 MB cho request body) v gii hn thi gian x l (timeout) ch l 60 giy. Khng th x l file 100 MB hay chy trong 60 pht. Ti sao khng chn C (Real-time inference): Ch  Real-time c ti u cho  tr thp (mili giy). N c gii hn payload l 6 MB v timeout mc nh l 60 giy. Nu gi 100 MB vo Real-time endpoint, request s b t chi ngay lp tc (413 Request Entity Too Large). Ti sao khng chn D (Batch transform): Mc d Batch Transform x l c d liu ln, nhng  bi yu cu trin khai trn mt SageMaker Endpoint (\"deploy... on an Amazon SageMaker endpoint\"). Batch Transform khng s dng endpoint thng trc m l chy cc jobs x l hng lot offline (khi to, x l, ri tt). Ch c Real-time, Serverless, v Asynchronous mi l cc li hnh trin khai Endpoint."
  },
  {
    "id": 167,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer notices class imbalance in an image classification training job. What should the ML engineer do to resolve this issue?",
    "options": [
      {
        "id": "A",
        "text": "Reduce the size of the dataset."
      },
      {
        "id": "B",
        "text": "Transform some of the images in the dataset."
      },
      {
        "id": "C",
        "text": "Apply random oversampling on the dataset."
      },
      {
        "id": "D",
        "text": "Apply random data splitting on the dataset."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Random oversampling): y l k thut kinh in  x l vn  mt cn bng d liu (Class Imbalance). Random Oversampling hot ng bng cch nhn bn ngu nhin cc mu d liu thuc lp thiu s (minority class) cho n khi s lng ca n cn bng vi lp a s (majority class). iu ny gip m hnh hc c cc c trng ca lp thiu s tt hn thay v b thin v v pha lp a s. Ti sao khng chn A (Reduce the size): Vic gim kch thc tp d liu mt cch chung chung (khng ni r l gim lp a s - Undersampling) s lm mt mt thng tin quan trng v c th dn n vic m hnh hot ng km (underfitting) m khng nht thit gii quyt c t l mt cn bng. Ti sao khng chn B (Transform images): \"Transform\" m ch k thut Data Augmentation (xoay, ct, lt nh). Mc d Augmentation thng c dng kt hp vi Oversampling  to ra cc bin th mi cho lp thiu s (trnh overfitting), nhng bn thn hnh ng \"bin i mt s nh\" khng trc tip gii quyt vn  v s lng/t l chnh lch gia cc lp nu khng c thc hin vi mc ch tng s lng mu thiu s c th. Option C l nh ngha trc tip ca gii php. Ti sao khng chn D (Random data splitting): y l bc chia d liu thnh cc tp Train/Validation/Test. Vic chia ngu nhin (Random splitting) thm ch cn c th lm vn  ti t hn nu lp thiu s qu t v khng xut hin trong tp Validation/Test.  x l imbalance khi chia d liu, ngi ta dng Stratified Splitting (chia theo t l tng), nhng n ch m bo phn phi u gia cc tp con ch khng sa li mt cn bng trong chnh d liu training."
  },
  {
    "id": 168,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company receives daily .csv files about customer interactions with its ML model. The company stores the files in Amazon S3 and uses the files to retrain the model. An ML engineer needs to implement a solution to mask credit card numbers in the files before the model is retrained. Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Create a discovery job in Amazon Macie. Configure the job to find and mask sensitive data."
      },
      {
        "id": "B",
        "text": "Create Apache Spark code to run on an AWS Glue job. Use the Sensitive Data Detection functionality in AWS Glue to find and mask sensitive data."
      },
      {
        "id": "C",
        "text": "Create Apache Spark code to run on an AWS Glue job. Program the code to perform a regex operation to find and mask sensitive data."
      },
      {
        "id": "D",
        "text": "Create Apache Spark code to run on an Amazon EC2 instance. Program the code to perform an operation to find and mask sensitive data."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (AWS Glue Sensitive Data Detection): AWS Glue l dch v ETL (Trch xut, Chuyn i, Ti) khng my ch (Serverless). Tnh nng \"Sensitive Data Detection\" c tch hp sn trong Glue cho php t ng nhn din cc loi d liu nhy cm (PII) nh s th tn dng v thc hin hnh ng thay th (masking/redaction) ngay trong lung x l d liu. V y l tnh nng c sn (built- in transform), bn ch cn cu hnh m khng cn vit code x l phc tp, p ng hon ho yu cu \"LEAST development effort\" (t n lc pht trin nht). Ti sao khng chn A (Amazon Macie): Amazon Macie l dch v bo mt v khm ph d liu (Discovery & Classification). Macie qut S3  pht hin v bo co d liu nhy cm, nhng n KHNG phi l cng c ETL  thc hin hnh ng bin i (masking/redacting) d liu trc tip trong file  phc v quy trnh training tip theo. Ti sao khng chn C (AWS Glue + Custom Regex): Mc d vn dng AWS Glue, nhng vic phi t vit code Apache Spark v duy tr cc biu thc chnh quy (Regex)  khp ng nh dng th tn dng i hi n lc pht trin v kim th cao hn nhiu so vi vic dng tnh nng c sn (Option B). Ngoi ra, Regex t vit d b sai st (false negatives) so vi th vin chun ca AWS. Ti sao khng chn D (EC2 + Custom Spark): y l gii php \"t qun l\" (Self- managed). Bn phi t ci t Spark, qun l h tng EC2 (patching, scaling) v t vit code x l. y l phng n tn km n lc pht trin v vn hnh nht (High operational overhead)."
  },
  {
    "id": 169,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A medical company is using AWS to build a tool to recommend treatments for patients. The company has obtained health records and self-reported textual information in English from patients. The company needs to use this information to gain insight about the patients. Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker to build a recurrent neural network (RNN) to summarize the data."
      },
      {
        "id": "B",
        "text": "Use Amazon Comprehend Medical to summarize the data."
      },
      {
        "id": "C",
        "text": "Use Amazon Kendra to create a quick-search tool to query the data."
      },
      {
        "id": "D",
        "text": "Use the Amazon SageMaker Sequence-to-Sequence (seq2seq) algorithm to create a text summary from the data."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Amazon Comprehend Medical): y l dch v AI chuyn dng (purpose-built) dnh cho lnh vc y t (Healthcare). N c kh nng t ng trch xut thng tin y khoa quan trng (nh tn thuc, bnh l, liu lng, quy trnh iu tr) t vn bn phi cu trc (ghi ch bnh n) m khng cn train model. V  bi yu cu \"gain insight\" t h s sc khe vi \"LEAST development effort\" (t n lc pht trin nht), vic gi mt API c sn chuyn cho y t l ti u nht. Ti sao khng chn A (RNN with SageMaker): Vic xy dng mt mng n-ron hi quy (RNN) t u i hi chuyn mn su v Data Science, chun b d liu ln, training, tuning v deployment. y l phng n tn nhiu cng sc nht (High effort). Ti sao khng chn C (Amazon Kendra): Amazon Kendra l dch v tm kim doanh nghip (Enterprise Search), gip tm ti liu da trn cu hi. Mc d n h tr tm kim, nhng nhim v chnh  y l \"gain insight\" (phn tch/hiu su) ni dung y t (trch xut thc th, mi quan h) ch khng phi ch l tm ra file ti liu no cha t kha. Ti sao khng chn D (Seq2seq algorithm): Tng t nh Option A, Seq2seq (Sequence-to-Sequence) l thut ton tm tt vn bn chung chung. Bn phi t x l d liu, train model v fine-tune  n hiu ng cnh y khoa. Effort cao hn rt nhiu so vi dng dch v Managed AI nh Comprehend Medical."
  },
  {
    "id": 170,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to extract entities from a PDF document to build a classifier model. Which solution will extract and store the entities in the LEAST amount of time?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Comprehend to extract the entities. Store the output in Amazon S3."
      },
      {
        "id": "B",
        "text": "Use an open source AI optical character recognition (OCR) tool on Amazon SageMaker to extract the entities. Store the output in Amazon S3."
      },
      {
        "id": "C",
        "text": "Use Amazon Textract to extract the entities. Use Amazon Comprehend to convert the entities to text. Store the output in Amazon S3."
      },
      {
        "id": "D",
        "text": "Use Amazon Textract integrated with Amazon Augmented AI (Amazon A2I) to extract the entities. Store the output in Amazon S3."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Amazon Comprehend direct processing): Amazon Comprehend hin  h tr tnh nng Intelligent Document Processing (IDP), cho php x l trc tip cc file PDF, Word v hnh nh. Bn khng cn phi trch xut vn bn (OCR) th cng trc. Ch cn gi API ca Comprehend vi input l file PDF, dch v s t ng thc hin OCR (ngm nh) v trch xut thc th (Entities) trong mt bc duy nht. y l gii php ti u nht v thi gian trin khai v  phc tp kin trc. Ti sao khng chn C (Textract + Comprehend): Mc d y l mt pattern hp l (dng Textract  OCR ri a text vo Comprehend), nhng n yu cu bn phi qun l hai dch v ring bit v vit code  ni (glue code) output ca Textract vo input ca Comprehend. iu ny tn nhiu thi gian thit lp hn so vi tnh nng native ca Option A. Ngoi ra, m t \"Comprehend to convert the entities to text\" trong p n ny b sai v mt k thut (Comprehend nhn Text v tr v Entities, khng phi ngc li). Ti sao khng chn B (Open source OCR on SageMaker): Vic t trin khai v qun l mt tool OCR m ngun m trn SageMaker i hi nhiu cng sc ci t, cu hnh mi trng v vn hnh (Self-managed). N khng th nhanh v n gin bng vic s dng dch v Managed Service c sn. Ti sao khng chn D (Textract + A2I): Amazon A2I (Augmented AI) c dng  thm quy trnh con ngi r sot (Human Loop) vo workflow. Vic thm con ngi vo quy trnh s lm tng ng k thi gian x l, tri ngc hon ton vi yu cu \"Least amount of time\"."
  },
  {
    "id": 171,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to merge and transform data from two sources to retrain an existing ML model. One data source consists of .csv files that are stored in an Amazon S3 bucket. Each .csv file consists of millions of records. The other data source is an Amazon Aurora DB cluster. The result of the merge process must be written to a second S3 bucket. The ML engineer needs to perform this merge-and-transform task every week. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create a transient Amazon EMR cluster every week. Use the cluster to run an Apache Spark job to merge and transform the data."
      },
      {
        "id": "B",
        "text": "Create a weekly AWS Glue job that uses the Apache Spark engine. Use DynamicFrame native operations to merge and transform the data."
      },
      {
        "id": "C",
        "text": "Create an AWS Lambda function that runs Apache Spark code every week to merge and transform the data. Configure the Lambda function to connect to the initial S3 bucket and the DB cluster."
      },
      {
        "id": "D",
        "text": "Create an AWS Batch job that runs Apache Spark code on Amazon EC2 instances every week. Configure the Spark code to save the data from the EC2 instances to the second S3 bucket."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (AWS Glue): AWS Glue l dch v ETL (Extract, Transform, Load) c qun l hon ton (Serverless). N c thit k chuyn bit  x l khi lng d liu ln (millions of records) vi engine Apache Spark m khng cn bn phi qun l h tng my ch. o Native Integrations: Glue c sn kt ni (Connectors) ti S3 v Amazon Aurora (JDBC), gip vic c/ghi d liu t hai ngun ny cc k n gin. o DynamicFrame: L cu trc d liu ti u ring ca Glue, gip x l d liu hn hp v bin i schema linh hot hn DataFrame chun ca Spark. o Operational Overhead: V l serverless, bn ch cn vit script v t lch (schedule), AWS lo ton b vic cp pht ti nguyn, scaling v patching. y l la chn c gnh nng vn hnh thp nht. Ti sao khng chn A (Amazon EMR): Mc d EMR chy tt Spark, nhng bn vn phi cu hnh cm (cluster configuration), chn loi instance, qun l bootstrap actions v phin bn phn mm. D l cm tm thi (transient), cng sc thit lp v qun l (manage infrastructure) vn cao hn nhiu so vi Glue. Ti sao khng chn C (AWS Lambda): AWS Lambda c gii hn thi gian chy ti a l 15 pht v gii hn v b nh/lu tr tm (ephemeral storage). Vi u vo l \"millions of records\", vic x l d liu v chy Spark (vn rt nng) trn Lambda l khng kh thi v rt d b timeout hoc li out-of-memory. Ti sao khng chn D (AWS Batch): AWS Batch yu cu bn phi ng gi code vo Docker container, thit lp mi trng tnh ton (Compute Environments) v hng i (Job Queues). Bn phi qun l nh ngha cng vic (Job Definitions) v i khi c cu hnh EC2 bn di. Mc  vn hnh cao hn hn so vi Glue."
  },
  {
    "id": 172,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer has deployed an Amazon SageMaker model to a serverless endpoint in production. The model is invoked by the InvokeEndpoint API operation. The model's latency in production is higher than the baseline latency in the test environment. The ML engineer thinks that the increase in latency is because of model startup time. What should the ML engineer do to confirm or deny this hypothesis?",
    "options": [
      {
        "id": "A",
        "text": "Schedule a SageMaker Model Monitor job. Observe metrics about model quality."
      },
      {
        "id": "B",
        "text": "Schedule a SageMaker Model Monitor job with Amazon CloudWatch metrics enabled."
      },
      {
        "id": "C",
        "text": "Enable Amazon CloudWatch metrics. Observe the ModelSetupTime metric in the SageMaker namespace."
      },
      {
        "id": "D",
        "text": "Enable Amazon CloudWatch metrics. Observe the ModelLoadingWaitTime metric in the SageMaker namespace."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (ModelSetupTime): o i vi SageMaker Serverless Inference, metric ModelSetupTime c thit k c bit  o thi gian \"Cold Start\" (khi ng ngui). o Khong thi gian ny bao gm vic AWS cp pht ti nguyn tnh ton (compute resources), ti model artifact t S3, v khi ng container. o Khi latency tng cao t ngt trong mi trng production (ni traffic c th khng u n gy ra cold start), vic quan st ModelSetupTime s xc nhn trc tip gi thuyt \"model startup time\" c phi l nguyn nhn hay khng. Ti sao khng chn D (ModelLoadingWaitTime): o Metric ModelLoadingWaitTime l metric dnh ring cho Multi-Model Endpoints (MME) chy trn cc instance thng thng (Provisioned instances). N o thi gian mt request phi ch  model c ti vo b nh khi model  cha c sn trn instance. o Mc d Serverless Inference cng c c ch ti model tng t, nhng metric chnh thc c AWS cng b cho Serverless Endpoint  o ton b qu trnh khi to l ModelSetupTime. Ti sao khng chn A (Model Monitor - Model Quality): o SageMaker Model Monitor dng  gim st cht lng m hnh (Model Quality) hoc tri dt d liu (Data Drift) v mt thng k v ton hc. N khng dng  o lng cc ch s hiu nng h thng (latency, CPU, memory). Ti sao khng chn B (Model Monitor + CloudWatch): o Tng t nh A, vic bt Model Monitor khng gii quyt vn  debug v latency h tng. CloudWatch metrics (nh  p n C)  c sn cho Endpoint m khng cn cu hnh thm job Model Monitor phc tp."
  },
  {
    "id": 173,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to ensure that a dataset complies with regulations for personally identifiable information (PII). The ML engineer will use the data to train an ML model on Amazon SageMaker instances. SageMaker must not use any of the PII. Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": "A",
        "text": "Use the Amazon Comprehend DetectPiiEntities API call to redact the PII from the data. Store the data in an Amazon S3 bucket. Access the S3 bucket from the SageMaker instances for model training."
      },
      {
        "id": "B",
        "text": "Use the Amazon Comprehend DetectPiiEntities API call to redact the PII from the data. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system to the SageMaker instances for model training."
      },
      {
        "id": "C",
        "text": "Use AWS Glue DataBrew to cleanse the dataset of PII. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system to the SageMaker instances for model training."
      },
      {
        "id": "D",
        "text": "Use Amazon Macie for automatic discovery of PII in the data. Remove the PII. Store the data in an Amazon S3 bucket. Mount the S3 bucket to the SageMaker instances for model training."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Comprehend + S3): o Amazon Comprehend DetectPiiEntities: L dch v chuyn bit s dng NLP  pht hin v xc nh v tr cc thc th PII (tn, a ch, s th...) trong vn bn. N cung cp thng tin chnh xc  thc hin vic che giu (redaction) d liu trc khi training. o Amazon S3: L chun lu tr (de facto standard) cho d liu training ca SageMaker. S3 tch hp natively vi SageMaker (qua File Mode hoc Pipe Mode), r hn EFS v d dng qun l vng i d liu. o Operational Efficiency: Vic kt hp kh nng pht hin mnh m ca Comprehend v lu tr chun S3 mang li quy trnh mt m v t tn cng vn hnh nht so vi vic phi qun l mount im EFS hay dng Macie (ch discovery). Ti sao khng chn B (Comprehend + EFS): o Mc d phn x l PII ging A, nhng vic s dng Amazon EFS lm kho lu tr chnh cho training data thng phc tp hn S3. EFS cn cu hnh VPC, Mount Targets, v gn (mount) vo instance. Tr khi cn chia s file h thng trc tip hoc hun luyn phn tn c th, S3 lun l la chn mc nh v hiu qu hn cho SageMaker. Ti sao khng chn C (Glue DataBrew + EFS): o Glue DataBrew l cng c trc quan  lm sch d liu, c th x l PII. Tuy nhin, vic kt hp vi EFS li lm gim tnh hiu qu vn hnh (nh l do  B). Ngoi ra, i vi d liu vn bn phi cu trc cn NLP  nhn din ng cnh PII, Comprehend thng vt tri hn cc quy tc transformation c nh ca DataBrew. Ti sao khng chn D (Macie + S3): o Amazon Macie ch yu l cng c bo mt v tun th (Security & Compliance) dng  qut v bo co d liu nhy cm ang nm yn trong S3. N khng phi l cng c tin x l (preprocessing tool)  redact (xa/che) d liu v to ra dataset sch mi cho training pipeline mt cch trc tip nh Comprehend. Macie thin v \"pht hin ri ro\" hn l \"chun b d liu\"."
  },
  {
    "id": 174,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company must install a custom script on any newly created Amazon SageMaker notebook instances. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create a lifecycle configuration script to install the custom script when a new SageMaker notebook is created. Attach the lifecycle configuration to every new SageMaker notebook as part of the creation steps."
      },
      {
        "id": "B",
        "text": "Create a custom Amazon Elastic Container Registry (Amazon ECR) image that contains the custom script. Push the ECR image to a Docker registry. Attach the Docker image to a SageMaker Studio domain. Select the kernel to run as part of the SageMaker notebook."
      },
      {
        "id": "C",
        "text": "Create a custom package index repository. Use AWS CodeArtifact to manage the installation of the custom script. Set up AWS PrivateLink endpoints to connect CodeArtifact to the SageMaker instance. Install the script."
      },
      {
        "id": "D",
        "text": "Store the custom script in Amazon S3. Create an AWS Lambda function to install the custom script on new SageMaker notebooks. Configure Amazon EventBridge to invoke the Lambda function when a new SageMaker notebook is initialized."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Lifecycle configuration): y l tnh nng nguyn bn (native) ca Amazon SageMaker Notebook Instances, c thit k chuyn bit  t ng ha vic chy cc script ty chnh (shell scripts) ti hai thi im: khi to mi (OnCreate) hoc khi khi ng li (OnStart). Bn ch cn upload script mt ln v chn n khi to notebook. y l gii php n gin nht, khng cn qun l thm h tng hay dch v bn ngoi, p ng hon ho tiu ch \"LEAST operational overhead\". Ti sao khng chn B (Custom ECR image): Vic to v duy tr mt Docker Image ty chnh (build, push ln ECR, versioning) ch  chy mt script ci t l qu phc tp v tn cng sc (Overkill). Gii php ny thng dng khi bn cn thay i su vo mi trng (OS, drivers) hoc dng cho SageMaker Studio/Training Jobs, khng phi l cch ti u cho vic ci script n gin trn Notebook Instance. Ti sao khng chn C (AWS CodeArtifact): CodeArtifact l dch v qun l gi ph thuc (artifact/package repository nh PyPI, Maven private). N dng  lu tr th vin, khng phi l cng c  thc thi script cu hnh h thng (bootstrapping) khi khi to my o. Ti sao khng chn D (Lambda + EventBridge): y l gii php \"t ch\" (custom orchestration). Bn phi thit lp EventBridge  bt s kin to notebook, kch hot Lambda, v Lambda phi tm cch kt ni vo Notebook (thng qua SSM)  chy lnh. Kin trc ny qu phc tp, d li v kh bo tr so vi tnh nng Lifecycle Config c sn."
  },
  {
    "id": 175,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is building a real-time data processing pipeline for an ecommerce application. The application generates a high volume of clickstream data that must be ingested, processed, and visualized in near real time. The company needs a solution that supports SQL for data processing and Jupyter notebooks for interactive analysis. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Data Firehose to ingest the data. Create an AWS Lambda function to process the data. Store the processed data in Amazon S3. Use Amazon QuickSight to visualize the data."
      },
      {
        "id": "B",
        "text": "Use Amazon Kinesis Data Streams to ingest the data. Use Amazon Data Firehose to transform the data. Use Amazon Athena to process the data. Use Amazon QuickSight to visualize the data."
      },
      {
        "id": "C",
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to ingest the data. Use AWS Glue with PySpark to process the data. Store the processed data in Amazon S3. Use Amazon QuickSight to visualize the data."
      },
      {
        "id": "D",
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to ingest the data. Use Amazon Managed Service for Apache Flink to process the data. Use the built-in Flink dashboard to visualize the data."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Amazon MSK + Managed Flink): o Real-time & High Volume: Amazon MSK (Kafka) l tiu chun cng nghip cho vic ingesting (np) lng d liu clickstream khng l vi  tr thp. o SQL & Interactive Analysis: Amazon Managed Service for Apache Flink (trc y l Kinesis Data Analytics) cung cp tnh nng \"Studio\". Tnh nng ny cho php bn chy SQL queries trc tip trn lung d liu (streaming data) theo thi gian thc. o Notebooks: Flink Studio cung cp giao din Notebook (da trn Apache Zeppelin, nhng thng c nh ng vi tri nghim \"interactive notebook\" trong cc bi thi)  Data Scientist/Engineer c th vit code, chy SQL v xem kt qu trc quan (visualize) ngay lp tc trn dng d liu ang chy m khng cn ch lu xung a. y l gii php ti u nht cho yu cu \"Interactive analysis\" trn d liu Streaming. Ti sao khng chn A (Firehose + Lambda): AWS Lambda l dch v tnh ton da trn s kin (Event-driven compute), s dng code (Python, Node.js...) ch khng phi SQL native cho vic x l d liu (mc d c th vit code  chy SQL, nhng khng phi l cng c \"Supports SQL\" theo ngha ca  bi). Hn na, Lambda khng cung cp giao din \"Jupyter notebooks\"  phn tch tng tc. Ti sao khng chn B (Athena): Amazon Athena l cng c truy vn tng tc cho d liu  lu tr (Data at Rest) trn S3. N khng phi l cng c x l lung thi gian thc (Real-time Stream Processing).  tr t lc d liu vo Kinesis -> Firehose -> S3 -> Athena query l qu cao so vi yu cu \"Real-time processing\". Ti sao khng chn C (Glue + PySpark): Mc d AWS Glue c h tr Streaming ETL v Jupyter Notebooks, nhng Glue thng hot ng theo c ch Micro-batch (ca Spark Streaming) nn  tr thng cao hn Flink (Native Streaming). Ngoi ra, Glue thng thin v cc job ETL (Trch xut - Bin i - Ti) nng n hn l vic \"Interactive Analysis\" (Phn tch tng tc) nhanh chng trn lung d liu nh Flink Studio."
  },
  {
    "id": 176,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A medical company needs to store clinical data. The data includes personally identifiable information (PII) and protected health information (PHI). An ML engineer needs to implement a solution to ensure that the PII and PHI are not used to train ML models. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Store the clinical data in Amazon S3 buckets. Use AWS Glue DataBrew to mask the PII and PHI before the data is used for model training."
      },
      {
        "id": "B",
        "text": "Upload the clinical data to an Amazon Redshift database. Use built-in SQL stored procedures to automatically classify and mask the PII and PHI before the data is used for model training."
      },
      {
        "id": "C",
        "text": "Use Amazon Comprehend to detect and mask the PII before the data is used for model training. Use Amazon Comprehend Medical to detect and mask the PHI before the data is used for model training."
      },
      {
        "id": "D",
        "text": "Create an AWS Lambda function to encrypt the PII and PHI. Program the Lambda function to save the encrypted data to an Amazon S3 bucket for model training."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Comprehend + Comprehend Medical): o y l s kt hp ca hai dch v NLP (X l ngn ng t nhin) chuyn dng nht ca AWS:  Amazon Comprehend: Gii vic pht hin v lm n cc thng tin PII ph thng (tn, a ch, email).  Amazon Comprehend Medical: c hun luyn c bit trn d liu y t  nhn din chnh xc PHI (Protected Health Information) nh tn thuc, bnh l, liu lng, v mi lin h vi bnh nhn trong cc vn bn lm sng (clinical text). o Vic s dng ng cng c chuyn ngnh (Domain-specific) gip m bo  chnh xc cao nht trong vic loi b d liu nhy cm m khng cn t xy dng model hay rule phc tp. Ti sao khng chn A (AWS Glue DataBrew): Glue DataBrew l cng c chun b d liu trc quan, c th nhn din PII c bn da trn mu (pattern/regex). Tuy nhin, n thiu kh nng hiu ng cnh y khoa su sc nh Comprehend Medical  pht hin PHI trong cc ghi ch lm sng phi cu trc. Ti sao khng chn B (Amazon Redshift): Amazon Redshift l kho d liu (Data Warehouse) dng  phn tch SQL. N khng c kh nng NLP tch hp sn  \"c hiu\" vn bn v t ng phn loi/masking d liu PHI/PII phc tp. Vic vit th tc SQL (Stored Procedures)  lm vic ny l khng kh thi v km hiu qu. Ti sao khng chn D (Lambda + Encryption): o Th nht, \"M ha\" (Encryption) khc vi \"Lm n/Che\" (Masking/Redaction). D liu m ha thng khng th dng  training (tr khi gii m), trong khi masking gi li cu trc cu  model c th hc ng cnh. o Th hai, vic t vit code Lambda  pht hin PHI/PII l ri ro cao v tn km thi gian (High operational overhead) so vi vic gi API c sn."
  },
  {
    "id": 177,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is developing a classification model. The ML engineer needs to use custom libraries in processing jobs, training jobs, and pipelines in Amazon SageMaker. Which solution will provide this functionality with the LEAST implementation effort?",
    "options": [
      {
        "id": "A",
        "text": "Manually install the libraries in the SageMaker containers."
      },
      {
        "id": "B",
        "text": "Build a custom Docker container that includes the required libraries. Host the container in Amazon Elastic Container Registry (Amazon ECR). Use the ECR image in the SageMaker jobs and pipelines."
      },
      {
        "id": "C",
        "text": "Create a SageMaker notebook instance to host the jobs. Create an AWS Lambda function to install the libraries on the notebook instance when the notebook instance starts. Configure the SageMaker jobs and pipelines to run on the notebook instance."
      },
      {
        "id": "D",
        "text": "Run code for the libraries externally on Amazon EC2 instances. Store the results in Amazon S3. Import the results into the SageMaker jobs and pipelines."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Custom Docker Container): y l phng php chun (Best Practice)  qun l cc th vin ty chnh trong mi trng ML cng nghip (ML Pipelines). o Tnh nht qun (Consistency): Bng cch ng gi th vin vo Docker Image, bn m bo mi trng chy Processing, Training v Pipeline l hon ton ging nhau, trnh li \"works on my machine\". o Hiu nng: Th vin c ci sn trong image, khng tn thi gian ci t li mi khi job chy (khc vi vic pip install ti runtime). o Kh nng m rng: D dng chia s v ti s dng image ny qua ECR cho nhiu project khc nhau. Ti sao khng chn A (Manually install): \"Ci t th cng\" (thng m ch vic chy lnh pip install ngay trong code training hoc script entrypoint) c nhiu nhc im: o Tng thi gian khi ng job (Latency) v phi ti v ci th vin mi ln chy. o Yu cu kt ni Internet (ri ro bo mt). o D gp li version xung t ti thi im chy (Runtime errors). Ti sao khng chn C (Notebook Instance + Lambda): y l hiu sai c bn v kin trc SageMaker. Cc SageMaker Jobs (Training, Processing) chy trn cc cm my ch tnh ton (compute clusters) tm thi, tch bit hon ton vi Notebook Instance. Vic ci th vin ln Notebook Instance khng h nh hng hay cung cp th vin cho mi trng chy Job. Ti sao khng chn D (External EC2): Vic chy code trn EC2 th cng l i ngc li li ch ca SageMaker (Managed Service). Bn s phi t qun l vic bt/tt my, scaling v chuyn d liu, gy tn km cng sc vn hnh (High operational overhead) hn rt nhiu."
  },
  {
    "id": 178,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer is deploying a trained model to an Amazon SageMaker endpoint. The ML engineer needs to receive alerts when data quality issues occur in production. Which solution will meet this requirement?",
    "options": [
      {
        "id": "A",
        "text": "Configure an Amazon CloudWatch metric alarm and a corresponding action to send an Amazon Simple Notification Service (Amazon SNS) notification."
      },
      {
        "id": "B",
        "text": "Integrate the SageMaker endpoint with a SageMaker Clarify processing job. Configure an Amazon CloudWatch alarm to provide alerts."
      },
      {
        "id": "C",
        "text": "Configure a monitoring job in SageMaker Model Monitor. Integrate Model Monitor with Amazon CloudWatch to provide alerts."
      },
      {
        "id": "D",
        "text": "Configure a data flow in SageMaker Data Wrangler. Integrate Data Wrangler with Amazon CloudWatch to provide alerts."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Model Monitor): y l dch v chuyn dng  gim st cht lng d liu (Data Quality) v s tri dt (Data Drift/Concept Drift) cho cc m hnh  trin khai trn Production. o C ch: Model Monitor t ng so snh d liu u vo thc t (captured data) vi mt tp d liu c s (baseline statistics). Nu pht hin s sai lch (v d: t l null cao, sai kiu d liu, thay i phn phi), n s y cc metrics vi phm ln Amazon CloudWatch. o Alerting: T CloudWatch, bn c th cu hnh Alarm  gi thng bo (alerts) khi cc metrics ny vt ngng. Ti sao khng chn A (CloudWatch metric alarm only): CloudWatch Alarm cn mt ngun metrics u vo. Bn thn CloudWatch ch o cc ch s h tng (CPU, Memory, Latency) ch khng th t \"c\" ni dung d liu ML  bit cht lng tt hay xu. N cn Model Monitor (Option C) lm bc trung gian  phn tch d liu v gi metrics. Ti sao khng chn B (SageMaker Clarify): SageMaker Clarify tp trung vo vic pht hin thin kin (Bias) v gii thch m hnh (Explainability). Mc d Model Monitor c s dng Clarify  backend cho tnh nng Bias Drift, nhng  gim st chung v \"Data Quality\" (nh schema validation, completeness), thut ng chnh xc v dch v bao trm nht l Model Monitor. Ti sao khng chn D (SageMaker Data Wrangler): Data Wrangler l cng c low- code  chun b v lm sch d liu (Data Prep) trc khi training. N khng c chc nng gim st (Monitoring) cho Endpoint ang chy."
  },
  {
    "id": 179,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to use Amazon SageMaker to train a model on more than 300 GB of data. The training data is composed of files that are 200 MB in size. The data is stored in Amazon S3 Standard storage and feeds a dashboard tool. Which SageMaker training ingestion mechanism is the MOST cost-effective solution for this scenario?",
    "options": [
      {
        "id": "A",
        "text": "Amazon Elastic File System (Amazon EFS) file system"
      },
      {
        "id": "B",
        "text": "Amazon FSx for Lustre file system"
      },
      {
        "id": "C",
        "text": "Amazon S3 in fast file mode while using S3 Express One Zone"
      },
      {
        "id": "D",
        "text": "Amazon S3 in fast file mode without using S3 Express One Zone"
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (S3 Fast File Mode with S3 Standard): o Ti u chi ph: D liu hin ang nm trn S3 Standard (r nht cho lu tr nng). Fast File Mode (FFM) cho php container training truy cp d liu trc tip di dng file h thng (POSIX) m khng cn ti ton b d liu v  a cc b (EBS) ca instance. iu ny gip loi b thi gian ch ti d liu (startup time) v tit kim chi ph dung lng EBS. o Ph hp vi c th file: Vi kch thc file trung bnh ~200 MB, FFM hot ng rt hiu qu (v t tn chi ph overhead cho metadata requests so vi hng triu file nh). o Khng tn ph h tng ph: Khng cn khi to v tr tin cho file system ph tr nh FSx hay EFS. Ti sao khng chn A (Amazon EFS): Amazon EFS c chi ph lu tr cao hn nhiu so vi S3 Standard (khong gp 10 ln). Ngoi ra, EFS thng c dng khi cn chia s trng thi hoc code gia cc instance, khng phi l la chn ti u v gi cho vic stream d liu training tnh c ln (throughput cost cao). Ti sao khng chn B (Amazon FSx for Lustre): FSx for Lustre cung cp hiu nng cc cao (throughput ln, latency thp), gip tng tc training. Tuy nhin, n yu cu bn phi tr tin cho mt h thng file ring bit (dung lng + throughput). Vi yu cu \"MOST cost-effective\" (tit kim nht) v kch thc 300GB, chi ph tng thm ca FSx thng khng b p c bng vic gim thi gian training so vi gii php \"free\" l FFM. Ti sao khng chn C (S3 Express One Zone): S3 Express One Zone l lp lu tr hiu nng cao (single-AZ) vi  tr cc thp, nhng gi lu tr ca n t hn nhiu so vi S3 Standard (khong gp 7 ln). Vic di chuyn d liu sang lp ny ch  training s lm tng ng k chi ph lu tr m khng cn thit cho kch bn ny."
  },
  {
    "id": 180,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company has an ML model that is deployed to an Amazon SageMaker endpoint for real- time inference. The company needs to deploy a new model. The company must compare the new model's performance to the currently deployed model's performance before shifting all traffic to the new model. Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the new model to a separate endpoint. Manually split traffic between the two endpoints."
      },
      {
        "id": "B",
        "text": "Deploy the new model to a separate endpoint. Use Amazon CloudFront to distribute traffic between the two endpoints."
      },
      {
        "id": "C",
        "text": "Deploy the new model as a shadow variant on the same endpoint as the current model. Route a portion of live traffic to the shadow model for evaluation."
      },
      {
        "id": "D",
        "text": "Use AWS Lambda functions with custom logic to route traffic between the current model and the new model."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Shadow Variant): o Shadow Deployment (Shadow Mode) l tnh nng c SageMaker h tr sn, cho php bn trin khai m hnh mi (Shadow model) song song vi m hnh hin ti (Production model) trn cng mt Endpoint. o N s t ng sao chp (mirror) traffic thc t gi ti Production model sang Shadow model  chy th nghim. Kt qu tr v cho user vn l ca Production model, nhng bn s thu thp c metrics (latency, error rate, prediction) ca Shadow model  so snh hiu nng. o y l gii php c \"LEAST operational effort\" v SageMaker lo ton b vic iu phi traffic, logging v so snh m khng nh hng n ngi dng cui. Ti sao khng chn A (Separate endpoint + Manual split): Vic trin khai ra endpoint ring v t chia traffic th cng ( pha client/application code) i hi bn phi sa i code ng dng, qun l logic cn bng ti, v t thu thp/so snh metrics t 2 endpoint khc nhau. Ri ro v cng sc vn hnh cao hn nhiu. Ti sao khng chn B (CloudFront): Amazon CloudFront l CDN (Content Delivery Network), thng dng  phn phi ni dung tnh hoc cache API. Mc d CloudFront Functions/Lambda@Edge c th chia traffic, nhng vic thit lp n  routing request ti 2 SageMaker Endpoints (vn nm trong VPC hoc yu cu SigV4 signing phc tp) l gii php rt cng knh (High complexity) v khng phi l pattern chun cho ML Model A/B Testing. Ti sao khng chn D (Lambda routing): Tng t nh B, vic dng mt hm Lambda ng trc  lm \"Router\" i hi bn phi vit code custom logic  chia traffic, x l retry, timeout v monitor. y l vic \"reinventing the wheel\" (pht minh li bnh xe) khi SageMaker  c sn tnh nng ny."
  },
  {
    "id": 181,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company runs an ML model on Amazon SageMaker. The company uses an automatic process that makes API calls to create training jobs for the model. The company has new compliance rules that prohibit the collection of aggregated metadata from training jobs. Which solution will prevent SageMaker from collecting metadata from the training jobs?",
    "options": [
      {
        "id": "A",
        "text": "Opt out of metadata tracking for any training job that is submitted."
      },
      {
        "id": "B",
        "text": "Ensure that training jobs are running in a private subnet in a custom VPC."
      },
      {
        "id": "C",
        "text": "Encrypt the training data with an AWS Key Management Service (AWS KMS) customer managed key."
      },
      {
        "id": "D",
        "text": "Reconfigure the training jobs to use only AWS Nitro instances."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Opt out of metadata tracking): o AWS thu thp siu d liu (metadata) tng hp t cc dch v AI/ML (nh SageMaker)  ci thin cht lng dch v.  tun th quy tc \"prohibit the collection of aggregated metadata\", AWS cung cp c ch Opt-out (thng qua AWS Organizations AI services opt-out policies). o Khi bn chn \"Opt out\", AWS s ngng lu tr hoc s dng d liu o to v metadata ca bn cho mc ch ci thin sn phm ca h. y l gii php trc tip nht  p ng yu cu v \"compliance rules\" lin quan n vic thu thp metadata. Ti sao khng chn B (Private subnet): Chy trong Private Subnet gip c lp kt ni mng (Network Isolation)  job khng truy cp Internet. Tuy nhin, n khng ngn cn SageMaker Control Plane (mt phng qun l ca AWS) thu thp cc metrics v metadata (nh trng thi job, thi gian chy, li)  bo co li cho bn trong Console hoc CloudWatch. Ti sao khng chn C (KMS Encryption): M ha d liu (Encryption) bo v tnh b mt ca ni dung d liu (Training Data), nhng khng che giu metadata (nh tn job, tham s hyperparameter, loi instance, thi gian chy). AWS vn c th nhn thy cc thng tin ny  qun l ti nguyn. Ti sao khng chn D (Nitro instances): AWS Nitro Instances cung cp s c lp phn cng v bo mt tt hn (Enclaves), nhng chng khng c tnh nng t ng chn vic gi metadata vn hnh v cho AWS Control Plane."
  },
  {
    "id": 182,
    "type": "matching",
    "required_answers": 1,
    "question": "Topic #: 1 Context: An ecommerce company is using Amazon SageMaker Clarify Foundation Model Evaluations (FMEval) to evaluate ML models. Task: Select the correct model evaluation task from the following list for each ecommerce use case. Each model evaluation task should be selected one time. Options: * Classification evaluation * Open-ended generation * Question answering * Text summarization Use Cases (Inputs): 1. Categorize customer reviews as positive, neutral, or negative sentiment. 2. Create concise product descriptions from complete manufacturer details. 3. Recommend products based on a user's browsing history. 4. Respond to specific customer queries about product features. Answer : Di y l cp ghp ni chnh xc (Mapping): 1. Categorize customer reviews as positive, neutral, or negative sentiment.  [Classification evaluation] 2. Create concise product descriptions from complete manufacturer details.  [Text summarization] 3. Recommend products based on a user's browsing history.  [Open-ended generation] 4. Respond to specific customer queries about product features.  [Question answering] Explain: * Ti sao chn Classification evaluation cho \"Categorize customer reviews...\": Yu cu  y l gn nhn (labeling) cho vn bn vo cc nhm ri rc  nh trc (Positive, Neutral, Negative). Trong thut ng Machine Learning v NLP, vic xc nh cm xc (Sentiment",
    "options": [],
    "answer": "D",
    "explanation": "Ti sao chn Classification evaluation cho \"Categorize customer reviews...\": Yu cu  y l gn nhn (labeling) cho vn bn vo cc nhm ri rc  nh trc (Positive, Neutral, Negative). Trong thut ng Machine Learning v NLP, vic xc nh cm xc (Sentiment Analysis) chnh l bi ton Classification (Phn loi vn bn). FMEval nh gi task ny da trn  chnh xc ca vic model gn ng nhn. Ti sao chn Text summarization cho \"Create concise product descriptions...\": T kha \"concise\" (ngn gn) v \"from complete details\" (t chi tit y ) m t chnh xc qu trnh tm tt vn bn. Task Text summarization o lng kh nng ca model trong vic cht lc thng tin quan trng t vn bn di thnh vn bn ngn hn m vn gi c  ngha ct li. Ti sao chn Question answering cho \"Respond to specific customer queries...\": Hnh ng \"Respond to queries\" (Tr li truy vn) v tnh nng sn phm l nh ngha c bn ca bi ton Question Answering (QA). Trong FMEval, task ny nh gi kh nng model tr li chnh xc cu hi da trn kin thc  hc hoc ng cnh c cung cp (RAG). Ti sao chn Open-ended generation cho \"Recommend products...\": y l la chn cui cng v cng l task tru tng nht. Trong ng cnh ca Foundation Models (GenAI), vic a ra gi  (recommendation) da trn lch s duyt web thng c coi l mt bi ton sinh vn bn t do (text generation) da trn ng cnh u vo (context). Khng ging nh classification (chn 1 nhn) hay QA (tr li fact), vic gi  i hi model sng to ra mt danh sch hoc on vn bn thuyt phc ngi dng, do  n ph hp nht vi nhm Open-ended generation."
  },
  {
    "id": 183,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to launch a new internal generative AI interface to answer user questions. The interface will be based on a popular open source large language model (LLM). Which combination of steps will deploy the interface with the LEAST operational overhead? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker JumpStart to deploy the LLM."
      },
      {
        "id": "B",
        "text": "Download the LLM as a .zip file. Deploy the LLM on a GPU-based Amazon EC2 instance."
      },
      {
        "id": "C",
        "text": "Create a frontend HTML interface that uses an Amazon API Gateway WebSocket API with AWS Lambda functions to handle the user interaction."
      },
      {
        "id": "D",
        "text": "Use Amazon QuickSight to create a UI to handle the user interaction."
      },
      {
        "id": "E",
        "text": "Use Amazon Lex to create a UI to handle the user interaction."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (SageMaker JumpStart):  trin khai mt m hnh m ngun m (Open Source LLM) ph bin vi chi ph vn hnh thp nht, SageMaker JumpStart l gii php ti u. N cung cp mt \"Model Zoo\" vi cc m hnh  c ng gi sn (nh Llama, Falcon, Mistral). Bn c th trin khai chng ln SageMaker Endpoint ch bng vi c click chut (hoc vi dng code) m khng cn t xy dng Docker container hay lo lng v vic ti u ha model server. Ti sao chn E (Amazon Lex):  to giao din tng tc ngi dng (UI/Conversational Interface) vi t n lc nht, Amazon Lex l dch v c qun l hon ton (fully managed) dnh cho chatbot. Thay v phi t code v duy tr h thng WebSocket, API Gateway v Frontend (nh Option C), Amazon Lex cung cp sn khung sn cho hi thoi, qun l phin (session management) v tch hp d dng vi cc knh chat hoc web client (thng qua Amazon Lex Web UI). Bn ch cn cu hnh Lex  gi Lambda, v Lambda s gi SageMaker Endpoint  ly cu tr li t LLM. Ti sao khng chn B (EC2 manual deployment): Vic t ti model v ci t trn EC2 yu cu bn phi qun l h iu hnh, driver GPU (CUDA), bn v bo mt v t xy dng c ch auto-scaling. y l phng n c gnh nng vn hnh (Operational Overhead) cao nht (Self-managed). Ti sao khng chn C (Custom Frontend + API Gateway + Lambda): Mc d y l kin trc Serverless ph bin, nhng vic phi t xy dng (build) v duy tr m ngun cho Frontend HTML, cu hnh WebSocket API v logic Lambda  x l kt ni i hi n lc pht trin v vn hnh ln hn nhiu so vi vic s dng dch v Chatbot c sn nh Lex. Ti sao khng chn D (Amazon QuickSight): Amazon QuickSight l cng c Business Intelligence (BI) dng  v biu  v phn tch d liu. Mc d c tnh nng Generative Q, nhng n khng phi l nn tng  host mt giao din chat ty chnh cho mt model LLM m ngun m bt k."
  },
  {
    "id": 184,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to build a real-time analytics application that uses streaming data from social media. An ML engineer must implement a solution that ingests and transforms 5 GB of data each minute. The solution also must load the data into a data store that supports fast queries for the real-time analytics. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon EventBridge to ingest the social media data. Use AWS Glue to transform the data. Store the transformed data in Amazon ElastiCache (Memcached)."
      },
      {
        "id": "B",
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to ingest the social media data. Use AWS Lambda to transform the data. Store the transformed data in Amazon S3."
      },
      {
        "id": "C",
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to ingest the social media data. Use Amazon EMR to transform the data. Store the transformed data in Amazon RDS."
      },
      {
        "id": "D",
        "text": "Use Amazon Kinesis Data Streams to ingest the social media data. Use Amazon Managed Service for Apache Flink to transform the data. Store the transformed data in Amazon DynamoDB."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (Kinesis Data Streams + Managed Flink + DynamoDB): y l kin trc tiu chun (Standard Reference Architecture) cho cc ng dng phn tch d liu thi gian thc (Real-time Analytics) vi lu lng ln trn AWS. o Ingestion: Amazon Kinesis Data Streams c thit k chuyn bit  x l d liu streaming vi thng lng cao (high throughput). Vi khi lng 5 GB/pht (~85 MB/s), Kinesis c th d dng m rng bng cch tng s lng Shard. o Transformation: Amazon Managed Service for Apache Flink l dch v x l lung (stream processing) c  tr cc thp (sub-second latency), h tr stateful processing (x l c trng thi)  thc hin cc php bin i phc tp trn ca s thi gian (windowing). o Storage: Amazon DynamoDB l c s d liu NoSQL c kh nng chu ti ghi cc ln (high write throughput) v tr v kt qu truy vn vi  tr mili giy (millisecond latency), p ng hon ho yu cu \"fast queries for real- time analytics\". Ti sao khng chn A (EventBridge + Glue + ElastiCache): Amazon EventBridge l dch v nh tuyn s kin (Event Bus) cho kin trc hng s kin, khng phi l ng ng dn d liu (Data Pipeline) cho khi lng d liu streaming ln (5 GB/pht). Ngoi ra, ElastiCache (Memcached) l b nh m (caching) tm thi, khng phi l ni lu tr bn vng chnh (primary data store) cho ng dng analytics. Ti sao khng chn B (SQS + Lambda + S3): Amazon S3 l kho lu tr i tng (Object Storage), thng dng cho Data Lake. Mc d S3 bn vng, nhng  tr khi truy vn d liu t S3 (v d dng Athena) thng tnh bng giy hoc pht, khng p ng c yu cu \"fast queries for real-time analytics\" (thng cn ms). Ngoi ra, m hnh SQS + Lambda kh x l cc bi ton phn tch lung phc tp (nh sliding windows) so vi Flink. Ti sao khng chn C (SNS + EMR + RDS): Amazon SNS l dch v thng bo (Pub/Sub notifications), gii hn kch thc payload (256KB) v khng c kh nng lu gi/pht li lung d liu (retentio..."
  },
  {
    "id": 185,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company stores training data as a .csv file in an Amazon S3 bucket. The company must encrypt the data and must control which applications have access to the encryption key. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a new SSH access key. Use the AWS Encryption CLI with a reference to the new access key to encrypt the file."
      },
      {
        "id": "B",
        "text": "Create a new API key by using the Amazon API Gateway CreateApiKey API operation. Use the AWS CLI with a reference to the new API key to encrypt the file."
      },
      {
        "id": "C",
        "text": "Create a new IAM role. Attach a policy that allows the AWS Key Management Service (AWS KMS) GenerateDataKey action. Use the role to encrypt the file."
      },
      {
        "id": "D",
        "text": "Create a new AWS Key Management Service (AWS KMS) key. Use the AWS Encryption CLI with a reference to the new KMS key to encrypt the file."
      }
    ],
    "answer": "D",
    "explanation": "Ti sao chn D (AWS KMS + Encryption CLI): o AWS KMS (Key Management Service): L dch v chuyn bit  to v qun l cc kha m ha. Tnh nng quan trng nht ca KMS l Key Policy (Chnh sch kha), cho php bn nh ngha chnh xc \"ai\" (User, Role, Application) c php s dng kha ny  m ha hoc gii m. iu ny p ng trc tip yu cu \"control which applications have access to the encryption key\". o AWS Encryption CLI: L cng c dng lnh gip m ha d liu (Client-side encryption) mt cch an ton bng cch tch hp vi KMS. Ti sao khng chn A (SSH access key): SSH Keys c dng  xc thc quyn truy cp vo my ch (nh EC2) qua giao thc SSH. Chng khng phi l kha mt m dng  m ha d liu (Data Encryption). Ti sao khng chn B (API Gateway API Key): API Keys trong API Gateway c dng  nh danh v kim sot hn ngch (throttling/quota) cho ngi gi API. Chng khng c chc nng m ha d liu. Ti sao khng chn C (IAM Role only): Mc d IAM Role gip qun l quyn truy cp, nhng bn thn Role khng phi l \"Kha m ha\". Bn cn phi to ra mt kha (Key) trong KMS trc, sau  mi dng IAM Role  cp quyn s dng kha . Vic ch to Role m khng c Key th khng th thc hin hnh ng m ha."
  },
  {
    "id": 186,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company needs to perform feature engineering, aggregation, and data preparation. After the features are produced, the company must implement a solution on AWS to process and store the features. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon SageMaker Feature Processing to process and ingest the data. Use SageMaker Feature Store to manage and store the features."
      },
      {
        "id": "B",
        "text": "Use Amazon SageMaker Model Monitor to automatically ingest and transform the data. Create an Amazon S3 bucket to store the features in JSON format."
      },
      {
        "id": "C",
        "text": "Use Amazon Managed Service for Apache Flink to transform the data and to ingest the data directly into Amazon SageMaker Feature Store. Use Feature Store to manage and store the features."
      },
      {
        "id": "D",
        "text": "Use an Amazon SageMaker batch transform job to analyze, transform, and ingest the data. Create an Amazon DynamoDB table to store the features."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (SageMaker Feature Processing + Feature Store): o SageMaker Feature Store l kho lu tr trung tm chuyn dng (centralized repository)  lu tr, truy xut v chia s cc c trng (features) phc v cho ML. N h tr c Online Store ( tr thp cho inference) v Offline Store (cho training). o SageMaker Feature Processing l mt kh nng tch hp sn ca Feature Store, cho php bn nh ngha cc quy trnh chuyn i d liu (thng s dng Apache Spark/PySpark)  thc hin feature engineering v aggregation. N t ng ti (ingest) d liu  x l vo Feature Store. y l gii php trn gi (end-to-end) chun nht trong h sinh thi SageMaker cho bi ton ny. Ti sao khng chn B (Model Monitor + S3): o SageMaker Model Monitor c dng  gim st m hnh  trin khai (deployed models) nhm pht hin data drift hoc model drift. N khng phi l cng c ETL  thc hin feature engineering hay aggregation cho d liu th ban u. o Lu tr JSON trn S3 thiu cc tnh nng qun l c trng chuyn su (nh time travel, feature serving low-latency) m Feature Store cung cp. Ti sao khng chn C (Managed Flink): o Mc d Amazon Managed Service for Apache Flink c th x l d liu streaming v y vo Feature Store, nhng n thin v x l lung thi gian thc (Real-time Streaming).  bi  cp n quy trnh \"feature engineering, aggregation, and data preparation\" mt cch tng qut (thng l batch processing). SageMaker Feature Processing c thit k chuyn bit  gn lin quy trnh bin i ny vi Feature Store management, l p n chnh xc hn v mt tnh nng \"native\". Ti sao khng chn D (Batch Transform + DynamoDB): o SageMaker Batch Transform ch yu c dng  chy suy lun (inference) offline trn hng lot d liu bng mt model  train, khng phi l cng c chnh  lm ETL/Feature Engineering. o Dng DynamoDB trc tip i hi bn t xy dng li cc tnh nng qun l feature (metadata, versioning) m SageMaker Featu..."
  },
  {
    "id": 187,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is developing a new online application to gather information from customers. An ML engineer has developed a new ML model that will determine a score for each customer. The model will use the score to determine which product to display to the customer. The ML engineer needs to minimize response-time latency for the model. How should the ML engineer deploy the application in Amazon SageMaker to meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Configure batch transform."
      },
      {
        "id": "B",
        "text": "Configure a real-time inference endpoint."
      },
      {
        "id": "C",
        "text": "Configure a serverless inference endpoint."
      },
      {
        "id": "D",
        "text": "Configure an asynchronous inference endpoint."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Real-time inference endpoint): o y l ty chn trin khai SageMaker ti u nht cho cc ng dng yu cu  tr cc thp (millisecond latency) v phn hi tc th (synchronous). o Trong kch bn \"online application\", khi khch hng ang ch  xem sn phm no c hin th, m hnh cn tr v kt qu ngay lp tc (sub- second). Real-time Endpoint duy tr cc instance lun chy sn (persistent/warm instances), m bo khng c  tr khi ng (cold start), p ng hon ho yu cu \"minimize response-time latency\". Ti sao khng chn A (Batch transform): Batch Transform dng  x l offline hng lot d liu ln (v d: chy d on cho ton b khch hng vo ban m). N khng h tr tng tc thi gian thc vi ngi dng ang online. Ti sao khng chn C (Serverless inference): Serverless Inference c th gp vn  \"Cold Start\" (thi gian khi ng ngui) nu endpoint khng c gi trong mt khong thi gian, gy ra  tr cao t ngt cho request u tin. Do , n khng phi l la chn tt nht nu mc tiu ti thng l \"minimize response-time latency\" mt cch n nh. Ti sao khng chn D (Asynchronous inference): Asynchronous Inference a request vo hng i (queue)  x l sau. N ph hp cho cc tc v mt nhiu thi gian (vi pht) hoc d liu ln, nhng khng ph hp cho ng dng online cn hin th kt qu ngay lp tc cho ngi dng."
  },
  {
    "id": 188,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using Amazon EMR. The company has a large dataset in Amazon S3 that needs to be ingested into Amazon SageMaker Feature Store. The dataset contains historical data and real-time streaming data. The company must ensure that the Feature Store online store is updated with the most recent data as soon as the data becomes available. The company also must maintain a complete Feature Store offline store for batch processing. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Use the PutRecord API in Feature Store Runtime to ingest all the data into the online store."
      },
      {
        "id": "B",
        "text": "Use the PutRecord API in Feature Store Runtime to ingest all the data into the offline store."
      },
      {
        "id": "C",
        "text": "Use the Feature Store Spark connector to ingest the data as Spark DataFrames with the online store and offline store enabled."
      },
      {
        "id": "D",
        "text": "Use the Feature Store Spark connector to ingest the data as Spark DataFrames with only the online store enabled."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (Spark connector + Both stores): o Amazon SageMaker Feature Store Spark Connector: y l cng c c ti u ha  lm vic vi Amazon EMR v cc job x l d liu Spark. N cho php np (ingest) khi lng d liu ln t Spark DataFrames vo Feature Store mt cch hiu qu m khng cn vit vng lp gi API th cng. o Requirements:  bi yu cu cp nht \"online store\" ngay lp tc (real- time availability) V duy tr \"offline store\" cho batch processing. Khi s dng Spark Connector v bt c (flag) cho c hai store (OnlineStore=True, OfflineStore=True), Feature Store s t ng ng b d liu vo c hai ni: Online (DynamoDB) cho  tr thp v Offline (S3) cho lu tr lch s. y l gii php \"mt mi tn trng hai ch\" chun xc nht cho ng cnh EMR. Ti sao khng chn A (PutRecord API to Online): Dng PutRecord API n l trong vng lp (loop)  np mt \"large dataset\" t EMR l khng hiu qu v mt hiu nng (throughput thp, latency cao do round-trip network). Ngoi ra, nu ch ingest vo Online store m khng cu hnh t ng sync sang Offline store (hoc khng enable Offline store ngay t u), bn c th mt d liu lch s hoc phi thc hin thm bc copy phc tp. Ti sao khng chn B (PutRecord API to Offline): PutRecord khng h tr ghi trc tip vo Offline store theo cch update real-time  phc v inference ngay lp tc. Offline Store thng c  tr (latency) khi d liu xut hin trn S3 (do buffering). Hn na, yu cu quan trng l \"online store is updated... as soon as available\", vic ghi vo Offline store vi phm iu ny. Ti sao khng chn D (Spark connector + Only Online): Nu ch bt Online store, bn s mt tnh nng lu tr lch s (Historical data retention) trn S3 phc v cho training v batch processing sau ny. iu ny vi phm yu cu \"maintain a complete Feature Store offline store\"."
  },
  {
    "id": 189,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to deploy four ML models in an Amazon SageMaker inference pipeline. The models were built with different frameworks. The ML engineer also needs to give clients the ability to use the invoke_endpoint call to perform inference for each model. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": "A",
        "text": "Create a SageMaker multi-model endpoint."
      },
      {
        "id": "B",
        "text": "Create a SageMaker multi-container endpoint."
      },
      {
        "id": "C",
        "text": "Create multiple SageMaker single-model endpoints."
      },
      {
        "id": "D",
        "text": "Run a SparkML job to generate multiple endpoints."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Multi-container endpoint): o Different Frameworks:  bi nu r cc m hnh c xy dng bng \"different frameworks\" (v d: mt ci l PyTorch, mt ci l TensorFlow). iu ny ng ngha vi vic chng cn cc mi trng runtime v th vin dependencies khc nhau, tc l cn cc Container Image khc nhau. SageMaker Multi-Container Endpoint (MCE) cho php bn trin khai ti a 15 container khc nhau trn cng mt Endpoint (v cng mt instance EC2). o Inference Pipeline: Tnh nng MCE chnh l nn tng  xy dng Serial Inference Pipeline (chui x l tun t: Tin x l -> Model A -> Hu x l). o Cost-effective: Thay v dng 4 endpoint ring bit (tn tin cho 4 instance chy lin tc), bn gom c 4 container ny vo 1 endpoint duy nht, chy trn 1 instance (hoc 1 cluster nh), gip tit kim chi ph h tng ng k. o Direct Invocation: Vi Multi-Container Endpoint, khch hng c th invoke ton b pipeline hoc (ty cu hnh) gi trc tip n mt container c th trong nhm bng cch s dng header TargetContainerHostname trong API invoke_endpoint. Ti sao khng chn A (Multi-model endpoint - MME): o MME c thit k  host hng ngn m hnh chia s chung mt Container (v d: 1000 model XGBoost chy trn 1 container XGBoost). o V cc model trong  bi dng \"different frameworks\", chng khng th d dng chia s chung mt container (tr khi dng Triton Inference Server cu hnh phc tp). MME khng gii quyt vn  \"khc bit mi trng/framework\" tt bng Multi-Container Endpoint. Ti sao khng chn C (Multiple single-model endpoints): o y l gii php tn km nht. Bn phi tr tin cho t nht 4 instance chy 24/7 (mi endpoint cn t nht 1 instance). So vi vic gp li (Option B), chi ph s cao gp 4 ln. Ti sao khng chn D (SparkML job): o SparkML job dng  x l d liu (batch processing) hoc training, khng phi l c ch  to v hosting Real-time Inference Endpoint."
  },
  {
    "id": 190,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer wants an Amazon SageMaker notebook to automatically stop running after 1 hour of idle time. How can the ML engineer accomplish this goal?",
    "options": [
      {
        "id": "A",
        "text": "Create a lifecycle configuration in SageMaker. Copy the auto-stop-idle script from GitHub to the Start Notebook section."
      },
      {
        "id": "B",
        "text": "Create a lifecycle configuration in SageMaker. Copy the auto-stop-idle script from GitHub to the Create Notebook section."
      },
      {
        "id": "C",
        "text": "Track the notebook's CPU metric by using Amazon CloudWatch Logs. Invoke an AWS Lambda function from CloudWatch Logs to shut down the notebook instance if CPU utilization becomes zero."
      },
      {
        "id": "D",
        "text": "Track the notebook's memory metric by using Amazon CloudWatch Logs. Invoke an AWS Lambda function from CloudWatch Logs to shut down the notebook instance if memory utilization becomes zero."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (Lifecycle config - Start Notebook): o C ch hot ng:  t ng tt Notebook khi khng s dng (idle), AWS cung cp mt script mu (auto-stop-idle) chy mt tin trnh nn (cron job)  lin tc kim tra trng thi ca Jupyter Kernel. o V tr cu hnh: Tin trnh gim st ny cn c kch hot mi ln Notebook khi ng (k c sau khi Stop/Start li). Do , script phi c t trong phn \"Start Notebook\" (On-Start) ca Lifecycle Configuration. Ti sao khng chn B (Create Notebook section): Script trong phn \"Create Notebook\" (On-Create) ch chy duy nht mt ln khi bn to mi instance ln u tin. Nu bn tt (Stop) instance v bt li vo ngy hm sau, script gim st s khng c chy li, dn n tnh nng t ng tt b v hiu ha. Ti sao khng chn C (CloudWatch CPU metric): o V mt k thut, CPU Utilization ca mt instance ang chy khng bao gi bng 0 (do cc tin trnh h iu hnh nn). Vic t ngng trigger da trn CPU thng khng chnh xc bng vic kim tra trc tip trng thi kt ni ca Jupyter Kernel (nh script  p n A lm). o Trin khai gii php ny phc tp hn nhiu (cn Lambda, Alarm, IAM) so vi dng Lifecycle Script c sn. Ti sao khng chn D (CloudWatch Memory metric): Tng t nh C, Memory Utilization khng bao gi v 0 khi my ang chy. Dng RAM  xc nh trng thi \"idle\" l sai v mt nguyn l qun tr h thng."
  },
  {
    "id": 191,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to provide services to help other businesses label images. The company wants its labeling specialists to complete human labeling tasks on AWS. How should the company register the labeling specialists to receive tasks on AWS?",
    "options": [
      {
        "id": "A",
        "text": "Use AWS Data Exchange."
      },
      {
        "id": "B",
        "text": "Create and use an internal workforce in Amazon SageMaker Ground Truth."
      },
      {
        "id": "C",
        "text": "Create and use Amazon Mechanical Turk entities in an Amazon SageMaker human loop."
      },
      {
        "id": "D",
        "text": "Use the Amazon Mechanical Turk website."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Internal workforce): o Amazon SageMaker Ground Truth cung cp 3 loi lc lng lao ng (workforce): . Public (Amazon Mechanical Turk): Cng ng n danh ton cu. . Vendor: Cc cng ty cung cp dch v gn nhn chuyn nghip (bn th 3). . Private (Internal): Nhn vin hoc chuyn gia ring ca chnh cng ty bn. o  bi nu r cng ty mun s dng \"its labeling specialists\" (chuyn gia ca chnh h)  thc hin tc v. Do , vic to mt Internal Workforce (Private Team) l gii php chnh xc. Bn c th qun l danh sch ngi dng ny thng qua Amazon Cognito hoc OIDC IDP ca ring cng ty. Ti sao khng chn A (AWS Data Exchange): Dch v ny dng  tm kim, ng k v trao i cc b d liu (datasets) t bn th ba trn nn tng AWS. N khng c chc nng qun l quy trnh gn nhn hay qun l nhn s gn nhn. Ti sao khng chn C (Amazon Mechanical Turk entities): y l ty chn Public Workforce. Nu chn ty chn ny, task ca bn s c gi cho cng ng Workers n danh trn ton th gii (Crowdsourcing), khng phi cho i ng chuyn gia ni b ca cng ty. Ti sao khng chn D (Amazon Mechanical Turk website): y l trang web dnh cho Requesters (ngi thu) v Workers (ngi lm) ca h thng Mechanical Turk cng cng. N khng cung cp giao din  tch hp v qun l i ng ring (Private Team) trong quy trnh training ca SageMaker Ground Truth."
  },
  {
    "id": 192,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company wants to use Amazon SageMaker to host an ML model that runs on CPU for real-time predictions. The model will have intermittent traffic during business hours and will have periods of no traffic after business hours. The company needs a solution that will serve inference requests in the most cost-effective manner. Which hosting option will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the model to a SageMaker real-time endpoint. Add a schedule-based auto scaling policy to handle traffic surges during business hours."
      },
      {
        "id": "B",
        "text": "Deploy the model to a SageMaker Serverless Inference endpoint. Configure increased provisioned concurrency during business hours."
      },
      {
        "id": "C",
        "text": "Deploy the model to a SageMaker Asynchronous Inference endpoint. Configure an auto scaling policy that scales in to zero outside business hours."
      },
      {
        "id": "D",
        "text": "Deploy the model to a SageMaker real-time endpoint. Create a scheduled AWS Lambda function that activates the endpoint during business hours only."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Serverless Inference): o Cost-effective for Intermittent Traffic: SageMaker Serverless Inference l gii php l tng cho cc m hnh c lu lng truy cp khng u n (intermittent) hoc c khong thi gian khng hot ng (idle). Bn ch phi tr tin cho thi gian tnh ton (compute duration) khi c request x l v dung lng b nh s dng. Khi khng c traffic (sau gi lm vic), chi ph l bng 0 v n t ng scale v 0. o Provisioned Concurrency:  m bo  tr thp (real-time) trong gi cao im (\"business hours\"), bn c th cu hnh Provisioned Concurrency (Kh nng ng thi c cung cp trc)  gi cho cc hm \"m\" (warm), trnh hin tng Cold Start. y l s cn bng tt nht gia chi ph v hiu nng. Ti sao khng chn A (Real-time endpoint): SageMaker Real-time Endpoint s dng cc instance EC2 dnh ring (provisioned instances). Bn phi tr tin cho thi gian instance bt, bt k c traffic hay khng. Vi lu lng \"intermittent\" (lc c lc khng), bn s lng ph tin cho cc khong thi gian instance chy nhn ri (idle time). Ti sao khng chn C (Asynchronous Inference): Async Inference c thit k cho cc tc v x l lu (long-running) hoc payload ln (ln n 1GB), v m hnh hot ng l khng ng b (tr v Job ID  poll kt qu sau). N khng ph hp vi yu cu \"real-time predictions\" (phn hi tc th) nh Serverless hoc Real-time Endpoint. Ti sao khng chn D (Lambda activation): Vic dng Lambda  \"bt/tt\" endpoint thc cht l to v xa endpoint hng ngy. Qu trnh to endpoint mi c th mt vi pht n hng chc pht, gy gin on dch v vo u gi lm vic. Ngoi ra, trong gi lm vic, nu dng Real-time endpoint th vn b vn  chi ph cho thi gian nhn ri nh Option A."
  },
  {
    "id": 193,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to train a supervised deep learning model. The available dataset is a large number of unlabeled images that only employees should access. The ML engineer needs to implement a solution that labels the dataset with the highest possible accuracy. Which combination of steps should the ML engineer take to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Use Amazon Rekognition to automatically label the dataset."
      },
      {
        "id": "B",
        "text": "Train the deep learning model directly on the raw data. Let the model infer the labels by itself."
      },
      {
        "id": "C",
        "text": "Use Amazon SageMaker Ground Truth to create an annotation job that specifies the labeling task and requirements."
      },
      {
        "id": "D",
        "text": "Set up workforce teams to access a private workforce to run and review the annotation job created by Amazon SageMaker Ground Truth."
      },
      {
        "id": "E",
        "text": "Use Amazon Mechanical Turk to complete the annotation job created by Amazon SageMaker Ground Truth."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Ground Truth): o  hun luyn mt m hnh hc su c gim st (supervised deep learning model), iu kin tin quyt l phi c d liu c gn nhn (labeled data). D liu hin ti l \"unlabeled images\" (nh cha gn nhn). o Amazon SageMaker Ground Truth l dch v chuyn bit gip xy dng cc b d liu hun luyn chnh xc cao bng cch kt hp sc ngi (human labelers) v my hc (auto-labeling). N cung cp giao din v quy trnh qun l job gn nhn mt cch h thng. Ti sao chn D (Private Workforce): o Yu cu bo mt:  bi nu r \"dataset... only employees should access\" (ch nhn vin mi c truy cp). iu ny loi b hon ton kh nng s dng lc lng lao ng cng cng (Public Workforce) nh Amazon Mechanical Turk hay cc nh cung cp bn th ba (Vendor Workforce). o Private Workforce cho php bn nh ngha mt nhm ngi dng c th (nhn vin ni b) v cp quyn truy cp an ton cho h  thc hin task gn nhn m khng  l d liu ra bn ngoi. o  chnh xc: Yu cu \"highest possible accuracy\" thng t c tt nht khi s dng chuyn gia ni b (Subject Matter Experts) hiu r nghip v  gn nhn v review th cng (human-in-the-loop). Ti sao khng chn A (Amazon Rekognition): o Amazon Rekognition tr v cc nhn tng qut (generic labels) da trn m hnh  hun luyn sn ca AWS. Cc nhn ny c th khng khp vi danh mc (classes) c th m bi ton ca bn yu cu. Ngoi ra,  chnh xc cho cc d liu c th (domain-specific) s khng cao bng vic con ngi gn nhn trc tip. Ti sao khng chn B (Train on raw data): o  bi yu cu hun luyn m hnh Supervised (C gim st). nh ngha ca Supervised Learning l m hnh hc t cc cp (Input, Label). Vic dng d liu th cha gn nhn (Raw data)  training trc tip thuc v lnh vc Unsupervised Learning (Hc khng gim st), sai vi yu cu  bi. Ti sao khng chn E (Mechanical Turk): o S dng Amazon Mechanical Turk (Public crow..."
  },
  {
    "id": 194,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is using an Amazon S3 bucket to collect data that will be used for ML workflows. The company needs to use AWS Glue DataBrew to clean and normalize the data. Which solution will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Create a DataBrew dataset by using the S3 path. Clean and normalize the data by using a DataBrew profile job."
      },
      {
        "id": "B",
        "text": "Create a DataBrew dataset by using the S3 path. Clean and normalize the data by using a DataBrew recipe job."
      },
      {
        "id": "C",
        "text": "Create a DataBrew dataset by using a Java Database Connectivity (JDBC) driver to connect to the S3 bucket. Clean and normalize the data by using a DataBrew profile job."
      },
      {
        "id": "D",
        "text": "Create a DataBrew dataset by using a Java Database Connectivity (JDBC) driver to connect to the S3 bucket. Clean and normalize the data by using a DataBrew recipe job."
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (S3 path + Recipe job): o DataBrew Dataset (S3 Path): AWS Glue DataBrew h tr kt ni trc tip (native) vi Amazon S3. Bn ch cn tr n ng dn S3 (S3 path)  to Dataset m khng cn qua bt k driver trung gian no. o Recipe Job: Trong DataBrew, tp hp cc bc bin i d liu (nh lm sch, chun ha, i tn ct, x l null...) c gi l mt \"Recipe\".  p dng cc bc ny ln ton b tp d liu ln v xut kt qu ra, bn phi chy mt Recipe Job. y chnh xc l cng c  thc hin yu cu \"clean and normalize the data\". Ti sao khng chn A (Profile job): o DataBrew Profile Job dng  chy thng k m t (descriptive statistics) v d liu (nh phn phi gi tr,  tng quan, cht lng d liu). N ch to ra mt bn bo co (Profile) v d liu ch khng thc hin vic bin i hay to ra dataset mi  c lm sch. Ti sao khng chn C v D (JDBC driver): o JDBC (Java Database Connectivity) l giao thc dng  kt ni vi cc c s d liu quan h (nh RDS, Redshift, SQL Server). Amazon S3 l dch v lu tr i tng (Object Storage), khng phi Database, nn khng s dng JDBC driver  kt ni trong DataBrew. DataBrew kt ni S3 trc tip."
  },
  {
    "id": 195,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is developing a new ML model that uses the XGBoost algorithm. The company will train the model on data that is stored in an Amazon S3 bucket. The data is in a nested JSON format. An ML engineer needs to convert the JSON files into a tabular format. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Create an AWS Glue PySpark job that uses the Relationalize transform to convert the files."
      },
      {
        "id": "B",
        "text": "Write custom Scala code to convert the files. Use Amazon EMR Serverless to run the Scala code."
      },
      {
        "id": "C",
        "text": "Create an AWS Lambda function that uses a Python runtime and invokes the reduce() function to convert the files. Invoke the Lambda function."
      },
      {
        "id": "D",
        "text": "Create an Amazon Athena database that is based on the JSON files. Use the Athena flatten function to convert the data."
      }
    ],
    "answer": "A",
    "explanation": "Ti sao chn A (AWS Glue Relationalize): o AWS Glue l dch v ETL serverless (khng cn qun l my ch), gip gim ti a gnh nng vn hnh (Least operational overhead). o Relationalize Transform: y l mt hm chuyn i c sn v chuyn bit trong th vin AWS Glue. N c thit k  t ng lm phng (flatten) cc cu trc d liu lng nhau phc tp (nested JSON) thnh dng bng (tabular/relational) gm cc ct v hng. Bn khng cn phi vit code phc tp  duyt tng node trong JSON. o y l gii php \"chun sch gio khoa\" ca AWS cho vic x l d liu nested JSON trc khi a vo hun luyn m hnh (nh XGBoost yu cu d liu bng). Ti sao khng chn B (EMR Serverless + Custom Scala): o Mc d EMR Serverless gim bt vic qun l server, nhng vic phi vit code Scala ty chnh  phn tch c php v lm phng file JSON lng nhau l mt cng vic tn km thi gian v cng sc pht trin (High development overhead) so vi vic gi mt hm c sn trong Glue. Ti sao khng chn C (Lambda + Python reduce): o S dng hm reduce()  lm phng JSON th cng l vic \"pht minh li bnh xe\" (reinventing the wheel). N phc tp, d li v kh bo tr. Ngoi ra, Lambda c gii hn v thi gian chy (15 pht) v b nh, c th khng x l tt nu file d liu ln. Ti sao khng chn D (Athena flatten): o Amazon Athena ch yu dng  truy vn (Query). Mc d SQL c th h tr lm phng mng (UNNEST/FLATTEN), nhng vic nh ngha schema cho JSON phc tp v vit cu lnh SQL  bin i ton b dataset thnh bng phng thng rm r hn so vi Glue. Athena ph hp  xem d liu hn l bin i v lu tr (ETL) cho training pipeline."
  },
  {
    "id": 196,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to create an internal-only chat interface to help employees handle customer queries. Currently, the employees need to refer to a massive knowledge base of internal documents to address customer issues. The new solution must be serverless. Which combination of steps will meet these requirements?",
    "options": [
      {
        "id": "A",
        "text": "Set up Amazon Bedrock with the Anthropic Claude foundation model."
      },
      {
        "id": "B",
        "text": "Set up Amazon SageMaker JumpStart with the Llama foundation model."
      },
      {
        "id": "C",
        "text": "Use Amazon EC2 instances with Amazon API Gateway to invoke the model API."
      },
      {
        "id": "D",
        "text": "Use AWS Lambda functions with Amazon API Gateway to invoke the model API."
      },
      {
        "id": "E",
        "text": "Use an Amazon S3 bucket to store vector database dumps and embeddings. * F. Use Amazon RDS for MySQL to store vector database dumps and embeddings."
      }
    ],
    "answer": "A",
    "explanation": "Cu hi yu cu mt gii php hon ton Serverless (khng my ch) cho ng dng Chatbot RAG. Di y l l do chn b ba ny: Ti sao chn A (Amazon Bedrock): o Amazon Bedrock l dch v Serverless hon ton (Fully Managed), cung cp quyn truy cp vo cc m hnh nn tng (Foundation Models) nh Claude thng qua API. Bn khng cn qun l my ch hay endpoint nh SageMaker (tr khi dng Serverless Inference, nhng Bedrock l la chn native hn cho GenAI API). Ti sao chn D (Lambda + API Gateway): o y l cp i kinh in ca kin trc Serverless Compute trn AWS. API Gateway cung cp im cui REST/WebSocket  chat client kt ni, v AWS Lambda thc thi logic nghip v (gi Bedrock API, x l context) m khng cn chy server EC2 lin tc. Ti sao chn E (Amazon S3): o Amazon S3 l dch v lu tr i tng Serverless. Trong kin trc RAG (Retrieval-Augmented Generation), S3 thng c dng lm \"Data Source\"  cha cc ti liu ni b (Knowledge Base). (Lu : Mc d cn mt Vector Database  search, nhng S3 l ni lu tr d liu gc bn vng v serverless nht trong cc la chn lu tr c a ra). Ti sao khng chn B (SageMaker JumpStart): Mc d JumpStart gip trin khai model nhanh, nhng n thng trin khai ln SageMaker Real-time Endpoints (da trn EC2 instances), ngha l bn phi tr tin theo gi cho instance chy, khng phi l Serverless thc th. Ti sao khng chn C (EC2): EC2 l IaaS (Infrastructure as a Service), yu cu qun l server, v li, scaling th cng. N vi phm yu cu \"Serverless\". Ti sao khng chn F (RDS MySQL): Amazon RDS truyn thng yu cu provision instance. D c Aurora Serverless, nhng trong ng cnh RAG, S3 (cho raw docs) hoc OpenSearch Serverless (cho vector) thng c u tin hn RDS MySQL tiu chun."
  },
  {
    "id": 197,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer needs to deploy a trained model that is based on a genetic algorithm. The algorithm solves a complex problem and can take several minutes to generate predictions. When the model is deployed, the model needs to access large amounts of data to process requests. The requests can involve as much as 100 MB of data. Which deployment solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Deploy the model to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer."
      },
      {
        "id": "B",
        "text": "Deploy the model to an Amazon SageMaker real-time endpoint."
      },
      {
        "id": "C",
        "text": "Deploy the model to an Amazon SageMaker Asynchronous Inference endpoint."
      },
      {
        "id": "D",
        "text": "Package the model as a container. Deploy the model to Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances."
      }
    ],
    "answer": "C",
    "explanation": "Ti sao chn C (SageMaker Asynchronous Inference): o X l Payload Ln (100 MB): SageMaker Asynchronous Inference c thit k c bit  x l cc payload input c kch thc ln (ln n 1 GB) bng cch upload d liu ln Amazon S3 trc khi x l, thay v gi trc tip qua HTTP body nh Real-time endpoint (thng gii hn  mc 6MB). o Thi gian x l di (Long Prediction Times): Thut ton di truyn trong  bi mt \"vi pht\"  chy. Real-time Endpoint c gii hn timeout (mc nh 60s), nn kt ni s b ngt trc khi model chy xong. Async Inference cho php thi gian x l ln ti 1 gi, ph hp hon ho cho cc tc v tn thi gian. o Ti u vn hnh (Least Operational Overhead): y l dch v c qun l hon ton (fully managed), c sn tnh nng hng i (queuing) v autoscaling (thm ch scale v 0 khi khng dng), gip bn khng phi t xy dng h thng queue hay qun l server EC2 th cng. Ti sao khng chn A (EC2) v D (ECS): C hai gii php ny u l \"Self-managed\" (T qun l). Bn s phi t cu hnh Load Balancer, t vit code  x l hng i (queue) cho cc request lu, v t lo phn scaling h tng. iu ny tn nhiu cng sc vn hnh hn nhiu so vi gii php c sn ca SageMaker. Ti sao khng chn B (Real-time Endpoint): Real-time endpoint khng h tr payload 100MB v khng ph hp cho cc request ko di vi pht (s gy timeout li client)."
  },
  {
    "id": 198,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "An ML engineer wants to use a set of survey responses as training data for an ML classifier. All the survey responses are either \"yes\" or \"no.\" The ML engineer needs to convert the responses into a feature that will produce better model training results. The ML engineer must not increase the dimensionality of the dataset. Which methods will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": "A",
        "text": "Binary encoding"
      },
      {
        "id": "B",
        "text": "Label encoding"
      },
      {
        "id": "C",
        "text": "One-hot encoding"
      },
      {
        "id": "D",
        "text": "Statistical imputation"
      },
      {
        "id": "E",
        "text": "Tokenization"
      }
    ],
    "answer": "A",
    "explanation": "Cu hi t ra hai yu cu chnh: chuyn i d liu dng vn bn (\"yes\"/\"no\") sang dng s  m hnh hiu c, v quan trng nht l khng c lm tng s chiu (dimensionality) ca tp d liu. Ti sao chn B (Label encoding): Label encoding gn mt s nguyn duy nht cho mi nhn. Vi d liu nh phn \"yes\"/\"no\", n s chuyn thnh 0 v 1 (v d: No=0, Yes=1). Qu trnh ny thay th 1 ct vn bn bng 1 ct s, do  s chiu ca d liu c gi nguyn (1 column -> 1 column). y l cch n gin v hiu qu nht cho bin nh phn. Ti sao chn A (Binary encoding): Mc d vi cc bin c nhiu gi tr (high cardinality), Binary encoding thng to ra nhiu ct (logarit c s 2 ca s lng nhn), nhng vi trng hp c bit ch c 2 gi tr (\"yes\"/\"no\"), Binary encoding hot ng tng t nh Label encoding. N chuyn nhn thnh chui nh phn (0 hoc 1) v lu trong mt ct duy nht. Do , n cng tha mn iu kin khng tng s chiu. Ti sao khng chn C (One-hot encoding): One-hot encoding s to ra mt ct mi cho mi gi tr duy nht. Vi \"yes\" v \"no\", n s to ra 2 ct mi (v d: is_yes v is_no). Vic bin 1 ct thnh 2 ct vi phm yu cu \"must not increase the dimensionality\". Ti sao khng chn D (Statistical imputation): y l k thut  in cc gi tr b thiu (missing values), khng phi l k thut m ha (encoding) d liu vn bn sang s. Ti sao khng chn E (Tokenization): Tokenization l bc tch vn bn thnh cc t/token (thng dng trong NLP). Bn thn n cha to ra feature s (cn qua bc vectorization nh TF-IDF hay Embedding), v cc bc sau  thng lm tng s chiu d liu ln rt nhiu."
  },
  {
    "id": 199,
    "type": "matching",
    "required_answers": 1,
    "question": "Question 139",
    "options": [],
    "answer": "D",
    "explanation": "Sequence-to-Sequence (seq2seq): Thut ton ny chuyn dng cho cc bi ton u vo l mt chui v u ra l mt chui, in hnh nh dch my (Machine Translation) v tm tt vn bn (Text Summarization). Semantic segmentation: y l thut ton th gic my tnh phn loi tng pixel (pixel-level classification) trong nh. N rt quan trng cho xe t li  phn bit chnh xc u l ng, va h, hay ngi i b  cp  chi tit nht. Random Cut Forest (RCF): L thut ton hc khng gim st (unsupervised learning) chuyn dng  pht hin bt thng (Anomaly Detection). N rt hiu qu trong vic tm ra cc im d liu d bit (abnormal/outliers) trong tp d liu."
  },
  {
    "id": 200,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "A company is planning to use an Amazon SageMaker prebuilt algorithm to create a recommendation model. The algorithm must be able to make predictions on high- dimensional sparse data. Which SageMaker algorithm should the company choose for the recommendation model?",
    "options": [
      {
        "id": "A",
        "text": "K-nearest neighbors (k-NN)"
      },
      {
        "id": "B",
        "text": "Factorization Machines"
      },
      {
        "id": "C",
        "text": "Principal component analysis (PCA)"
      },
      {
        "id": "D",
        "text": "Sequence-to-Sequence (seq2seq)"
      }
    ],
    "answer": "B",
    "explanation": "Ti sao chn B (Factorization Machines): o High-dimensional sparse data: Thut ton Factorization Machines (FM) trong Amazon SageMaker c thit k c bit  x l cc b d liu tha (sparse) v nhiu chiu (high-dimensional). o Recommendation Model: y l thut ton tiu chun cho cc h thng gi  (Recommender Systems), v d nh d on ngi dng c click vo qung co hay khng (Click-through rate prediction) hoc d on xp hng item, ni m ma trn tng tc gia ngi dng v sn phm thng rt tha tht (rt nhiu s 0). Ti sao khng chn A (K-nearest neighbors - k-NN): o Mc d k-NN c th dng cho gi  (da trn s tng ng), nhng n thng gp kh khn v hiu nng (curse of dimensionality) khi lm vic vi d liu qu nhiu chiu v tha tht so vi Factorization Machines. Trong SageMaker, k-NN thng c dng cho classification hoc regression da trn ch mc (index-based). Ti sao khng chn C (Principal component analysis - PCA): o PCA l thut ton gim chiu d liu (Dimensionality Reduction), khng phi l thut ton  xy dng m hnh d on hay gi  trc tip. N thng c dng  bc tin x l (preprocessing). Ti sao khng chn D (Sequence-to-Sequence - seq2seq): o Seq2Seq l thut ton hc c gim st chuyn dng cho x l ngn ng t nhin (NLP) nh dch my, tm tt vn bn, ni u vo v u ra l cc chui (sequences). N khng ph hp cho d liu bng tha tht ca bi ton gi  thng thng."
  },
  {
    "id": 201,
    "type": "multiple_choice",
    "required_answers": 1,
    "question": "ssss A company has several teams that have developed separate prediction models on their own laptops. The teams developed the models by using Python with scikit-learn and TensorFlow frameworks. The company must rebuild the models and must integrate the models into an ML infrastructure that the company manages by using Amazon SageMaker. The company also must incorporate the models into a model registry. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": "A",
        "text": "Export the models from the laptops to an Amazon S3 bucket. Use an Amazon API Gateway REST API and AWS Lambda functions with SageMaker endpoints to access the models. Register the models in the SageMaker Model Registry."
      },
      {
        "id": "B",
        "text": "Import the models into the SageMaker Model Registry. Use SageMaker to run the imported models."
      },
      {
        "id": "C",
        "text": "Use code from the laptops to create containers for the models. Use the bring your own container (BYOC) functionality of SageMaker to import and use the models. Register the models in the SageMaker Model Registry."
      }
    ],
    "answer": "?",
    "explanation": ""
  }
]